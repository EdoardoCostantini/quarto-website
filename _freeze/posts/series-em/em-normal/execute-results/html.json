{
  "hash": "05bc05891030e9d98ba4b86882248801",
  "result": {
    "markdown": "---\ndraft: true\ntitle: Using the EM algorithm for missing data analysis\nauthor: Edoardo Costantini\ndate: '2021-11-19'\nslug: em-algorithm-missing-data\ncategories: [\"statistics\", \"regression\", \"missing values\"]\nbibliography: ../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nThe [Expectation-Maximization (EM)](https://en.wikipedia.org/wiki/Expectationâ€“maximization_algorithm) algorithm is an iterative procedure that can be used to obtain maximum likelihood estimates (MLE) for a variety of statistical models.\nIt can be used to estimate the parameters of factor analysis models or to estimate the covariance matrix of a collection of variables assumed to come from a normal distribution.\nIn this post, I want to briefly describe a code implementation of the EM algorithm used for this latter purpose.\nFor a formal presentation of the algorithm, I recommend reading Schafer [-@schafer:1997, ch 5.3.3] or Little and Rubin [-@littleRubin:2002, p. 168].\n\n## Review likelihood concepts\n\nBefore getting to the EM algorithm is important to review a few concepts related to maximum likelihood estimation.\nThe EM algorithm aims to obtain the MLE estimates of some model parameters.\nSo first, it is important to make sure you understand what maximum likelihood estimation is.\nFirst of all, let's clarify what we mean when we say likelihood.\n\nConsider the following complete data (from @littleRubin:2002 example 7.7, p. 152):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load Little Rubin data -------------------------------------------------------\n\n# Create data\nY <- matrix(c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10,\n                  26, 29, 56, 31, 52, 55, 71 ,31, 54, 47,40,66,68,\n                  6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,\n                  60,52, 20, 47, 33, 22,6,44,22,26,34,12,12,\n                  78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,\n                  115.9,83.8,113.3,109.4), ncol = 5)\n                  \n# Store useful information\n  n <- nrow(Y)\n  p <- ncol(Y)\n```\n:::\n\nEvery continuous distribution has a probability density function.\n\nIf we assume that this data comes from a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) (MVN) we are saying that every row was sampled from this normal multivariate normal distribution.\nIn math, we would write:\n\n\\[\nY \\sim N(\\mathbf{\\mu}, \\mathbf{\\Sigma})\n\\]\n\nIn such a scenario, the MLE estimate will then try to find the population values of $\\mu$ and $\\Sigma$ that maximize the chance of observing the data we have observed ($Y$).\nFor a given row of $Y$, we can compute the relative probability of observing it density by plugging the value in the probability density function of the MVN distribution.\n\n\\[\ndet(2 \\pi \\mathbf{\\Sigma})^{-\\frac{1}{2}} exp \\left(-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu) \\right)\n\\]\n\n# The EM algorithm\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load Little Rubin data -------------------------------------------------------\n\n# Create data\nY <- matrix(c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10,\n                  26, 29, 56, 31, 52, 55, 71 ,31, 54, 47,40,66,68,\n                  6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,\n                  60,52, 20, 47, 33, 22,6,44,22,26,34,12,12,\n                  78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,\n                  115.9,83.8,113.3,109.4), ncol = 5)\n                  \n# Store useful information\n  n <- nrow(Y)\n  p <- ncol(Y)\n```\n:::\n\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}