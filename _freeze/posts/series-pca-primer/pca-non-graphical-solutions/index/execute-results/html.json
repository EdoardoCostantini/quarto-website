{
  "hash": "32854ba9d34f6162f36d9516a4f4b9d4",
  "result": {
    "markdown": "---\ntitle: Deciding the Number of PCs with Non-Graphical Solutions to the Scree Test\nauthor: Edoardo Costantini\ndate: '2022-05-16'\nslug: pca-non-graphical\ncategories: [\"PCA\", \"Tutorials\"]\n---\n\n\n# Introduction\n\nDifferent solutions:\n\n- Kaiser Rule (aka Optimal Coordinate) $n_{oc}$.\nIn its simplest form, the Kaiser's rule retains only the PCs with variances exceeding 1.\nIf a PC has less variance than 1, it means that it explains less total variance than a single variable in the data, which makes it useless.\n\n- Acceleration Factor.\nFor every $j$-th eigenvalue, the acceleration factor $a$ is calculated as the change in the slope between the line connecting the $eig_j$ and $eig_{j-1}$, and the line connecting $eig_j$ and $eig_{j+1}$\n$$\na_{j} = (eig_{j+1} - eig_{j}) - (eig_{j} - eig_{j-1})\n$$\nOnce the largest $a_j$ is found, the number of components is set to $j-1$.\n\n# Learn by coding\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres <- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues <- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Non-graphical solutions\nngs <- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man <- sum(eigenvalues > 1)\n\n# Accelration factor\na <- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] <- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man <- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        manual nFactor\nnaf          2       2\nnkaiser      2       2\n```\n:::\n:::\n\n\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres <- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues <- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n\n# Non-graphical solutions\nngs <- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man <- sum(eigenvalues > 1)\n\n# Accelration factor\na <- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] <- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man <- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n```\n:::\n\n\n# Other resources\n\n- [Cross-entropy in RPubs](https://rpubs.com/juanhklopper/cross_entropy)\n- [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n- [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n- [ML Gloassary](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n- [Loss Functions in Machine Learning](https://medium.com/swlh/cross-entropy-loss-in-pytorch-c010faf97bab)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}