{
  "hash": "f737028f163149429cab38c1c240099a",
  "result": {
    "markdown": "---\ndraft: false\ntitle: Cross-entropy as a measure of predictive performance\nauthor: Edoardo Costantini\ndate: '2022-04-22'\nslug: covmatwt\ncategories: [\"Machine Learning\"]\nbibliography: ../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nCross-entropy (CE) quantifies the difference between two probability distributions.\nAs such, it comes in handy as a [loss function](https://en.wikipedia.org/wiki/Loss_function) in multi-class classification tasks (e.g., multinomial logistic regression).\nIt also provides an elegant solution for determining the difference between actual and predicted categorical data point values.\nIt can be used to determine the predictive performance of a classification model.\nThe value of the cross-entropy is higher when the predicted classes diverge more from the true labels.\n\n# Learn by coding\n\nIn a multiclass-classification task, we calculate a separate \"loss\" for each class for each observation and sum the result:\n\n$$\nCE = - \\sum^{N}_{i = 1} \\sum^{K}_{k = 1} p_{(i, k)}log(\\hat{p}_{(i, k)}) (\\#eq:CE)\n$$\n\nwhere\n\n- $N$ is the sample size.\n- $K$ is the number of categories of the variable we are trying to predict.\n- $p$ is a scalar taking value $0 = \\text{no}$ or $1 = \\text{yes}$ to indicate whether observation $i$ belongs to class $k$. This can also be thought of as the true probability of the observation belonging to that class.\n- $\\hat{p}$ is a scalar indicating the predicted probability of observation $i$ belonging to class $k$.\n- $log$ is the natural logarithm.\n\nLet's see an example in R.\nThe `iris` data records the petal and sepal dimensions for 150 and their species.\nConsider the task of predicting the flowers' species based on all the numeric predictors available.\nWe will fit a multinomial logistic regression on the data and compute the cross-entropy between the observed and predicted class membership.\n\nTo start, we should prepare the R environment by loading a few packages we will use:\n\n- `nnet` to estimate the multinomial logistic model;\n- `MLmetric` to check someone else's implementation of the cross-entropy computation.\n- `FactoMineR` to create a [disjunctive table](https://www.xlstat.com/en/solutions/features/complete-disjuncive-tables-creating-dummy-variables) from an R factor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare environment ----------------------------------------------------------\n\n# Packages\nlibrary(nnet)\nlibrary(MLmetrics)  # for LogLoss() function\nlibrary(FactoMineR) # for tab.disjonctif() function\n\n# Default rounding for this sessino\noptions(\"digits\" = 5)\n```\n:::\n\nThen, we should estimate the multinomial logistic model of interest.\nWe will use this model to create predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit mulinomial logistic model ------------------------------------------------\n\n# Fit model\nglm_mln <- multinom(Species ~ Sepal.Length, data = iris)\n```\n:::\n\nWe can now create two R matrices `p` and `p_hat` storing all the scalars $p_{ik}$ and $\\hat{p}_{ik}$ we need to compute \\@ref(eq:CE).\n\n- First, we want to store all the $p_{ik}$ in one matrix.\nTo do so, we can create a disjunctive table based on the `species` factor.\nThis is an $N \\times K$ matrix storing 0s and 1s to indicate which class every observation belongs to.\n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # Obtain p and p_har -----------------------------------------------------------\n  \n  # store true labels in a matrix p\n  p <- FactoMineR::tab.disjonctif(iris$Species)\n  \n  # check it\n  head(p)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n    setosa versicolor virginica\n  1      1          0         0\n  2      1          0         0\n  3      1          0         0\n  4      1          0         0\n  5      1          0         0\n  6      1          0         0\n  ```\n  :::\n  :::\n\n\n- Second, we want to obtain the predicted class probabilities for every observation:\n\n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # obtain predictions\n  p_hat <- predict(glm_mln, type = \"probs\")\n    \n  # check it\n  head(p_hat)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n     setosa versicolor virginica\n  1 0.80657   0.176155 0.0172792\n  2 0.91844   0.076558 0.0050018\n  3 0.96787   0.030792 0.0013399\n  4 0.98005   0.019262 0.0006841\n  5 0.87281   0.117765 0.0094276\n  6 0.47769   0.442466 0.0798435\n  ```\n  :::\n  :::\n\n\nWe can now write a loop to perform the computation in \\@ref(eq:CE) for every $i$ and $k$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute CE with a loop -------------------------------------------------------\n\n# Define parameters\nN <- nrow(iris) # sample size\nK <- nlevels(iris$Species) # number of classes\n\n# Create storing object for CE\nCE <- 0\n\n# Compute CE with a loop\nfor (i in 1:N){\n  for (k in 1:K){\n    CE <- CE - p[i, k] * log(p_hat[i, k])\n  }\n}\n\n# Print the value of CE\nCE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 91.034\n```\n:::\n:::\n\n\nWe can also work with the matrices `p` and `p_hat` directly to avoid using a loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute CE using the matrices directly ---------------------------------------\nce <- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 91.034\n```\n:::\n:::\n\nThis approach works for a binary prediction just as well.\nWe only need to pay attention to storing the true and predicted probabilities in matrix form.\nFor example, consider the task of predicting the transmission type (automatic or not) for the cars recorded in the `mtcars` dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Binary cross entropy ---------------------------------------------------------\n\n# Fit model\nglm_log <- glm(am ~ hp + wt,\n               family = binomial(link = 'logit'),\n               data = mtcars)\n\n# store true labels in a matrix p\np <- FactoMineR::tab.disjonctif(as.factor(mtcars$am))\n\n# obtain predicted probabilites in matrix form\npred_probs <- predict(glm_log, type = \"response\")\np_hat <- cbind(k_0 = 1 - pred_probs,\n               k_1 = pred_probs)\nclass(p_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n\nThe objects `p` and `p_hat` are all the information we need to compute the cross-entropy for this binary prediction task:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check the first few rows of p\nhead(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 1\n1 0 1\n2 0 1\n3 0 1\n4 1 0\n5 1 0\n6 1 0\n```\n:::\n\n```{.r .cell-code}\n# check the first few rows of p_hat\nhead(p_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       k_0       k_1\nMazda RX4         0.157664 0.8423355\nMazda RX4 Wag     0.595217 0.4047825\nDatsun 710        0.029759 0.9702408\nHornet 4 Drive    0.958272 0.0417280\nHornet Sportabout 0.930612 0.0693881\nValiant           0.995012 0.0049882\n```\n:::\n:::\n\n\nWe can use these new objects to obtain the binary CE with the same computation we used for the multiclass CE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute CE using the matrices directly\nce <- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.0296\n```\n:::\n:::\n\n\nIt is not uncommon to divide the value of the cross-entropy by the number of units on which the computation is performed, effectively producing an average loss across the units.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Express as average\nce / nrow(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.15717\n```\n:::\n:::\n\n\nJust to be sure, we can use the `LogLoss()` function from the `MLmetrics` package to compute the same binary CE.\nHowever, this function requires the true and predicted probabilities to be stored as vectors instead of matrices.\nSo first we need to obtain the vector versions of `p` and `p_hat`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute binary CE with MLmetrics implementation ------------------------------\n\n# Obtain vector of true probabilities\np_vec <- mtcars$am\n\n# Obtain vector of predicted probabilities\np_hat_vec <- predict(glm_log, type = \"response\")\n```\n:::\n\n\nand then we can simply provide these objects to the `LogLoss()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute and print binary CE with MLmetrics implementation\nMLmetrics::LogLoss(y_pred = p_hat_vec,\n                   y_true = p_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.15717\n```\n:::\n:::\n\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare environment ----------------------------------------------------------\n\n# Packages\nlibrary(nnet)\nlibrary(MLmetrics)  # for LogLoss() function\nlibrary(FactoMineR) # for tab.disjonctif() function\n\n# Default rounding for this sessino\noptions(\"digits\" = 5)\n\n# Fit mulinomial logistic model ------------------------------------------------\n\n# Fit model\nglm_mln <- multinom(Species ~ Sepal.Length, data = iris)\n\n# Obtain p and p_har -----------------------------------------------------------\n\n# store true labels in a matrix p\np <- FactoMineR::tab.disjonctif(iris$Species)\n\n# check it\nhead(p)\n\n# obtain predictions\np_hat <- predict(glm_mln, type = \"probs\")\n  \n# check it\nhead(p_hat)\n\n# Compute CE with a loop -------------------------------------------------------\n\n# Define parameters\nN <- nrow(iris) # sample size\nK <- nlevels(iris$Species) # number of classes\n\n# Create storing object for CE\nCE <- 0\n\n# Compute CE with a loop\nfor (i in 1:N){\n  for (k in 1:K){\n    CE <- CE - p[i, k] * log(p_hat[i, k])\n  }\n}\n\n# Print the value of CE\nCE\n\n# Compute CE using the matrices directly ---------------------------------------\nce <- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n# Binary cross entropy ---------------------------------------------------------\n\n# Fit model\nglm_log <- glm(am ~ hp + wt,\n               family = binomial(link = 'logit'),\n               data = mtcars)\n\n# store true labels in a matrix p\np <- FactoMineR::tab.disjonctif(as.factor(mtcars$am))\n\n# obtain predicted probabilites in matrix form\npred_probs <- predict(glm_log, type = \"response\")\np_hat <- cbind(k_0 = 1 - pred_probs,\n               k_1 = pred_probs)\nclass(p_hat)\n# check the first few rows of p\nhead(p)\n\n# check the first few rows of p_hat\nhead(p_hat)\n\n# Compute CE using the matrices directly\nce <- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n# Express as average\nce / nrow(mtcars)\n\n# Compute binary CE with MLmetrics implementation ------------------------------\n\n# Obtain vector of true probabilities\np_vec <- mtcars$am\n\n# Obtain vector of predicted probabilities\np_hat_vec <- predict(glm_log, type = \"response\")\n\n# Compute and print binary CE with MLmetrics implementation\nMLmetrics::LogLoss(y_pred = p_hat_vec,\n                   y_true = p_vec)\n```\n:::\n\n\n# Other resources\n\n- [Cross-entropy in RPubs](https://rpubs.com/juanhklopper/cross_entropy)\n- [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n- [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n- [ML Gloassary](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n- [Loss Functions in Machine Learning](https://medium.com/swlh/cross-entropy-loss-in-pytorch-c010faf97bab)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}