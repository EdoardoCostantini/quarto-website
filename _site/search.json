[
  {
    "objectID": "series-pca.html",
    "href": "series-pca.html",
    "title": "A primer on PCA",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-sm-course.html",
    "href": "series-sm-course.html",
    "title": "Course: Statistics and Methodology",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a Ph.D. candidate at Tilburg University working on multiple imputation algorithms for survey data."
  },
  {
    "objectID": "series-sampling.html",
    "href": "series-sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\nApr 22, 2022\n\n\nCross-entropy as a measure of predictive performance\n\n\nMachine Learning\n\n\n\n\nMar 14, 2022\n\n\nEstimating the weighted covariance matrix in R\n\n\nStatistics\n\n\n\n\nFeb 28, 2022\n\n\nEstimating ridge regression in R\n\n\nPenalty\n\n\n\n\nNov 17, 2021\n\n\nThe sweep operator\n\n\nThe EM algorithm,Matrix algebra,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html",
    "href": "posts/series-pca-primer/pcovr.html",
    "title": "Principal covariates regression in R",
    "section": "",
    "text": "Principal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables. Principal covariates regression (PCovR) is an alternative approach that modifies the optimization criteria behind PCA to include a supervision aspect . PCovR looks for a low-dimensional representation of \\(X\\) that accounts for the maximum amount of variation in both \\(X\\) and \\(y\\). Compared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of \\(X\\) variables and that correlate well with a set of \\(Y\\) variables. For more information, I recommend reading Vervloet et al. (2015) and De Jong and Kiers (1992). In this post, you can find my R code notes on this method. In these notes, I show the computations used by the PCovR R-package to perform the method.\nTo understand how PCovR differs from classical PCR we need to complicate the notation. Consider the following decomposition of the data:\n\\[\n\\begin{align}\n    \\mathbf{T} &= \\mathbf{X} \\mathbf{W} \\\\\n    \\mathbf{X} &= \\mathbf{T} \\mathbf{P}_X + \\mathbf{E}_X  \\\\\n    y      &= \\mathbf{T} \\mathbf{P}_y + \\mathbf{e}_y\n\\end{align}\n\\]\nwhere \\(\\mathbf{T}\\) is the matrix of PCs defined above, \\(\\mathbf{W}\\) is a \\(p \\times q\\) matrix of component weights defining what linear combination of the columns of \\(\\mathbf{X}\\) is used to compute the components, and \\(\\mathbf{P}_X\\) and \\(\\mathbf{P}_y\\) are the \\(q \\times p\\) and \\(q \\times 1\\) loading matrices relating the variables in \\(\\mathbf{X}\\) and \\(y\\) to the component scores in \\(\\mathbf{T}\\), respectively. \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\) are the reconstruction errors, the information lost by using \\(\\mathbf{T}\\) as summary of \\(\\mathbf{X}\\).\nClassical PCA can be formulated as the task of finding the \\(\\mathbf{W}\\) and \\(\\mathbf{P}_X\\) that minimize the reconstruction error \\(\\mathbf{E}_X\\):\n\\[\\begin{equation}\n    (\\mathbf{W}, \\mathbf{P}) = \\underset{\\mathbf{W}, \\mathbf{P}_X}{\\operatorname{argmin}} \\lVert \\mathbf{X} - \\mathbf{XWP}' \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\). PCovR can be formulated as the task of minimizing a weighted combination of both \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\):\n\\[\n\\begin{equation}\\label{eq:PCovR}\n    (\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y) = \\underset{\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y}{\\operatorname{argmin  }} \\alpha \\lVert (\\mathbf{X} - \\mathbf{XWP}_X') \\rVert^2 + (1 - \\alpha) \\lVert (y - \\mathbf{XWP}_y') \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\).\nThe parameter \\(\\alpha\\) defines which reconstruction error is being prioritized. When \\(\\alpha = 1\\), the emphasis is exclusively placed on reconstructing \\(\\mathbf{X}\\), leading PCovR to PCR. When \\(\\alpha = 0.5\\), the importance of \\(\\mathbf{X}\\) and \\(y\\) is equally weighted, a case that resembles Partial least square, which we discuss below. In practice, its value can be found by cross-validation or according to a sequential procedure based on maximum likelihood principles (Vervloet et al. 2013). In particular, \\[\n\\begin{equation}\\label{eq:aml}\n    \\alpha_{ML} = \\frac{\\lVert \\mathbf{X} \\lVert^2}{\\lVert \\mathbf{X} \\lVert^2  + \\lVert y \\lVert^2 \\frac{\\hat{\\sigma}_{\\mathbf{E}_X}^2}{\\hat{\\sigma}_{e_y}^2}}\n\\end{equation}\n\\]\nwhere \\(\\hat{\\sigma}_{\\mathbf{E}_X}^2\\) can be obtained as the unexplained variance by components computed according to classical PCA and \\(\\hat{\\sigma}_{e_y}^2\\) can be estimated as the unexplained variance by the linear model regressing \\(y\\) on \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "href": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "title": "Principal covariates regression in R",
    "section": "Reverting to PCA",
    "text": "Reverting to PCA\nI mentioned before that when \\(\\alpha = 1\\), PCovR reduces to PCR. Let’s see that in action. First, we set the desired value for \\(\\alpha\\):\n\n# Reverting to PCA -------------------------------------------------------------\n\n# Use alpha 1\nalpha <- 1\n\nthen, we can perform PCovR\n\n# Estimate PCovR\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,\n    r = npcs # fixed number of components\n)\n\nand classical PCA according to the Guerra-Urzola et al. (2021)\n\n# Classical PCA\nuSVt <- svd(X)\nU <- uSVt$u\nD <- diag(uSVt$d)\nV <- uSVt$v\nI <- nrow(X)                              # Define the number of rows\nP_hat <- (I - 1)^{-1 / 2} * V %*% D       # Component loadings\nW_hat <- (I - 1)^{1 / 2} * V %*% solve(D) # Component weights\nT_hat <- (I - 1)^{1 / 2} * U              # Component scores\nT_hat <- X %*% W_hat\n\nWe can now compare the results again\n\n# The scores obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$Te, T_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The loadings obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(t(package$Px), P_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The weights obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$W, W_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# P is now proportional to W\nTuckerCoef(package$W, t(package$Px))$tucker_value\n\n[1] 1"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "href": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "title": "Principal covariates regression in R",
    "section": "Maximum Likelihood estimation of \\(\\alpha\\)",
    "text": "Maximum Likelihood estimation of \\(\\alpha\\)\nThe value of \\(\\alpha\\) is not usually chosen arbitrarily. One could cross-validate it or compute it with a closed-form solution that relies on the Maximum likelihood approach (Vervloet et al. 2016). Here, I show how to use this latter approach. First, let’s simply fit PCovR by using the PCovR::pcovr() function and setting the model selection argument set to “seq”. As explained in the help-file, this “implies a sequential procedure in which the weighting value is determined on the basis of maximum likelihood principles”, exactly what we want.\n\n# Maximum likelihood tuning of alpha -------------------------------------------\n\n# Fit PCovR\npackage <- pcovr(\n    X = X_raw,\n    Y = y_raw,\n    rot = \"none\",\n    R = npcs, # fixed number of components\n    modsel = \"seq\" # fastest option\n)\n\nThen, we can compute the maximum likelihood value of \\(\\alpha\\) by first computing the error terms and taking their ratio.\n\n# Compute error ratio components\nlm_mod <- lm(y ~ -1 + X)\nery <- 1 - summary(lm_mod)$r.squared\n\nRmin <- npcs\nRmax <- npcs\nsing <- svd(X)\nvec <- Rmin:Rmax\nvec <- c(vec[1] - 1, vec, vec[length(vec)] + 1)\nVAF <- c(0, cumsum(sing$d^2) / sum(sing$d^2))\nVAF <- VAF[vec + 1]\nscr <- array(NA, c(1, length(vec)))\nfor (u in 2:(length(vec) - 1)) {\n    scr[, u] <- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])\n}\nerx <- 1 - VAF[which.max(scr)]\n\nWe could have computed the error ratio with the PCovR::err() R function:\n\n# Compute error ratio with function\nerr <- ErrorRatio(\n    X = X,\n    Y = y,\n    Rmin = npcs,\n    Rmax = npcs\n)\n\n# Is it the same?\nerr - erx/ery\n\n[1] 1.110223e-15\n\n\nWith this value, it is now easy to compute \\(\\alpha\\):\n\n# Find alpha ML\nalpha_ML <- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)\n\n# Compare to one found by package\npackage$a - alpha_ML\n\n[1] 0"
  }
]