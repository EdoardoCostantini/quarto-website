{"title":"PCA with metrics, dimensionality reduction through PCA and MCA","markdown":{"yaml":{"draft":true,"title":"PCA with metrics, dimensionality reduction through PCA and MCA","author":"Edoardo Costantini","date":"2022-05-17","slug":"pca-mix","categories":["Categorical data"],"bibliography":"../../resources/bibshelf.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n**Principal Component Analysis** (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{Z}$, where $r < p$.\nThe columns of $\\mathbf{Z}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{Z} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of coefficients, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{Z}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{Z}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nThe PCs scores are given by the $n \\times r$ matrix $\\mathbf{UD}$, and the weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}$.\n\n**Multiple Correspondence Analysis** (MCA) is generally regarded as an equivalent tool that applies to discrete data.\nChavent et al. [-@chaventEtAl:2014] have shown how using weights on rows and columns of the input data matrix can define a general PCA framework that includes standard PCA and MCA as special cases.\nThis approach is often referred to as **PCA with metrics**, as metrics are used to introduce the weights.\nIn this post, I want to show how PCA and MCA are related through this framework.\n\n## Generalised Singular Value Decomposition\n\nConsider an $n \\times p$ matrix of input variables $\\mathbf{Z}$ with row metric $\\mathbf{N}$ and column metric $\\mathbf{M}$.\nThe Generalized Singular Value Decomposition of $\\mathbf{Z}$ can be written as:\n$$\n\\mathbf{Z} = \\mathbf{U \\Lambda V}^T\n$$\nwhere:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{ZMZ}^T\\mathbf{N}$ and $\\mathbf{Z}^T\\mathbf{NZM}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{ZMZ}^T\\mathbf{N}$ such that $\\mathbf{U}^T\\mathbf{MU=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{Z}^T\\mathbf{NZM}$ such that $\\mathbf{V}^T\\mathbf{MV=I}$;\n\nThe GSVD of $\\mathbf{Z}$ can be obtained by taking \n\n- first taking the standard SVD of the transformed matrix $\\tilde{\\mathbf{Z}} = \\mathbf{N}^{1/2}\\mathbf{Z}\\mathbf{M}^{1/2}$ which gives:\n$$\n\\tilde{\\mathbf{Z}} = \\tilde{\\mathbf{U}}\\tilde{\\mathbf{\\Lambda}}\\tilde{\\mathbf{V}}^T\n$$\n- and then transforming each element back to the original scale\n$$\n\\mathbf{\\Lambda} = \\tilde{\\mathbf{\\Lambda}}\n$$\n$$\n\\mathbf{U} = \\mathbf{N}^{-1/2}\\tilde{\\mathbf{U}}\n$$\n$$\n\\mathbf{V} = \\mathbf{M}^{-1/2}\\tilde{\\mathbf{V}}\n$$\n\n\n## Relationship of GSVD to standard SVD\n\nIt's easy to see how this GSVD differs from the standard formulation of SVD simply by the presence of the metrics $\\mathbf{M}$ and $\\mathbf{M}$.\nAs you can see [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition), in the standard formulation of SVD:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{ZZ}^T$ and $\\mathbf{Z}^T\\mathbf{Z}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{ZZ}^T$ such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{Z}^T\\mathbf{NZM}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\n## PCA and MCA as special cases of GSVD\n\nThe solutions for both PCA and MCA can be obtained as special cases of the GSVD approach described by setting $\\mathbf{Z}$ equal to a preprocessed version of the original data $\\mathbf{X}$ and using $\\mathbf{N}$ and $\\mathbf{M}$ to appropriately weight the rows and columns.\n\n### PCA\n\nThe input data for standard PCA is the $n \\times p$ matrix $\\mathbf{X}$ of $n$ rows (observations) described by $p$ numerical variables.\nThe columns of this matrix are usually centered and standardized.\nThe GSVD can be used to find the solution to PCA by setting $\\mathbf{Z}$ equal to the centered and standardized version of $\\mathbf{X}$ and weighting:\n\n- its rows by $1/n$, which is obtained by setting $\\mathbf{N} = \\frac{1}{n}I_n$\n- its columns by $1$, which is obtained by setting $\\mathbf{M} = I_p$.\nThis metric indicates that the distance between two observations is the standard euclidean distance between two rows of $\\mathbf{Z}$\n\nBy setting these values for the metrics, it is easy to see how the GSVD of $\\mathbf{X}$ reduces to the standard SVD of $\\mathbf{Z}$.\n\n### MCA\n\nFor an $n \\times p$ data matrix $\\mathbf{X}$ with $n$ observations (rows) and $p$ discrete predictors (columns).\nEach $j = 1, \\dots, p$ discrete variable has $k_j$ possible values.\nThe sum of the $p$ $k_j$ values is $k$. \n$\\mathbf{X}$ is preprocessed by coding each level of the discrete items as binary variables describing whether each observation takes a specific value for every discrete variable.\nThis results in an $n \\times k$ [complete disjunctive table](https://www.xlstat.com/en/solutions/features/complete-disjuncive-tables-creating-dummy-variables) $\\mathbf{G}$, sometimes also referred to as an indicator matrix.\n\nMCA is usually obtained by applying Correspondence Analysis to $\\mathbf{G}$, which means applying standard PCA to the matrices of the row profiles and the column profiles.\nIn particular, for the goal of obtaining a lower-dimensional representation of $\\mathbf{X}$ we are interested in the standard PCA of the row profiles.\nWithin the framework of PCA with metrics, MCA can be obtained by first setting:\n\n- $\\mathbf{Z}$ to the centered $\\mathbf{G}$\n- $\\mathbf{N} = \\frac{1}{n}I_n$\n- $\\mathbf{M} = \\text{diag}(\\frac{n}{n_s}, s = 1, \\dots, k)$\n\nThe coordinates of the observations (the principal component scores) can be obtained by applying the GSVD of $\\mathbf{Z}$ with the given metrics.\n\n# Learn by coding\n\n```{r mca, warning = FALSE, message = FALSE}\n\n### Object: Performing Multiple Correspondance Analysis in R \n### Source: Practical Guide to Principal Component Methods \n###         (Chapter 5 Code)\n\n# Load Packages\n  library(\"FactoMineR\") # for analysis\n  library(\"factoextra\") # for visualizarion1\n#   library(\"httpg\")\n\n# Data Prep ---------------------------------------------------------------\n\n  data(\"poison\")\n  head(poison[, 1:7], 3) # survey style data\n  \n# Subset active individuals and variables\n  poison.active <- poison[1:55, 5:15] \n  head(poison.active[, 1:6], 3)\n  \n# Summaries\n  str(poison.active)\n  for (i in 1:4) {\n    plot(poison.active[,i], main=colnames(poison.active)[i],\n         ylab = \"Count\", col=\"steelblue\", las = 2)\n  }\n  \n# The analysis ------------------------------------------------------------\n  \n  res.mca <- FactoMineR::MCA(X = poison.active,\n                             ncp = 5,\n                             graph = TRUE)\n\n# Visualization -----------------------------------------------------------\n\n  # Eigenvalues / Variances\n  eig.val <- get_eigenvalue(res.mca)\n  eig.val # proportion of variances retained by dimensions\n  \n  # Percentages of Inertia explained by MCA\n  fviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0,45))\n  \n  # Biplot\n  fviz_mca_biplot(res.mca,\n                  repel = TRUE, # avoid text overlapping\n                  ggtheme = theme_minimal())\n    # Rows (individuals) are represented by blue points;\n    # Columns (variable categories) by red triangles.\n  \n  # Graph of variables\n  var <- get_mca_var(res.mca) \n  var\n  # Coordinates \n  head(var$coord)\n  # Cos2: quality on the factore map \n  head(var$cos2)\n  # Contributions to the principal components\n  head(var$contrib)\n  \n  # Graph of individuals\n  ind <- get_mca_ind(res.mca) # extract the results for individuals\n  ind\n  # Coordinates of column points\n  head(ind$coord)\n  # Quality of representation\n  head(ind$cos2)\n  # Contributions\n  head(ind$contrib)\n  # BIplot for individuals only (no vars)\n  fviz_mca_ind(res.mca, col.ind = \"cos2\",\n               gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               repel = TRUE, ggtheme = theme_minimal())\n  fviz_mca_ind(res.mca, \n               label = \"none\",\n               habillage = \"Vomiting\", # color by groups defined by variable\n               palette = c(\"#00AFBB\", \"#E7B800\"),\n               addEllipses = TRUE, ellipse.type = \"confidence\",\n               repel = TRUE, ggtheme = theme_minimal())\n  # More than 1 grouping variable\n  fviz_ellipses(res.mca, c(\"Vomiting\", \"Fever\"),\n                geom = \"point\")\n  # Bar plot for Cos2 of individuals \n  fviz_cos2(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n  # Contribution of individuals to the dimensions \n  fviz_contrib(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n\n# Supplementary elements --------------------------------------------------\n\n#   res.mca <- MCA(poison, \n#                  ind.sup = 53:55)\n#   ind <- get_mca_ind(res.mca)\n#   ind$coord\n#   res.mca$ind.sup # Supplementary individuals \n  \n# Doing it by hand --------------------------------------------------------\n  \n  # Data\n  data(wine)\n  MCA.dt <- wine[, sapply(wine, is.factor)]\n  sapply(MCA.dt, nlevels)\n  \n  # Goal: Find the coordinates of \n  npcs <- 4\n  res.mca <- FactoMineR::MCA(X = MCA.dt, ncp = npcs, graph = FALSE)\n  res.mca$ind$coord # Coordinates of the individuals on the dimensions\n  res.mca$svd$V\n  \n# Following AudigierEtAl2016_MICAT ----------------------------------------\n  \n  I <- nrow(MCA.dt)     # numebr of individuals\n  K <- ncol(MCA.dt)     # number of categorical predictors\n  qk <- sapply(MCA.dt,  # number of levels per categorical variable \n               nlevels) \n  J <- sum(qk)          # total number of categories\n  \n  # Disjunctive table\n  Z <- tab.disjonctif(MCA.dt)\n  # Notice the relationship between the disjunctive table and a \n  #Â contingency table \n  N <- t(Z[, 1:3]) %*% Z[, 4:7]\n  N - table(MCA.dt)\n  \n  # Distance Metric Matrix\n  pxkqk <- colSums(Z)/I # props ids taking category value on variable\n  D_Sigma <- diag(pxkqk)\n  \n  # Weight Matrix\n  W_mat <- diag(rep(1, I))/I\n  \n  # M matrix (center matrix)\n  M <- matrix((rep(colMeans(Z), nrow(Z))), \n              nrow = nrow(Z), \n              byrow = TRUE)\n  \n  # SVD of triplet (Z-M, D_Sigma, W_mat)\n  SVD.trip <- svd.triplet(X = Z - M,\n                          row.w = diag(W_mat),\n                          col.w = diag(1/K*solve(D_Sigma)),\n                          ncp = npcs)\n  \n  # Manual SVD triplet\n  SVD.man <- svd(sqrt(W_mat) %*% (Z - M) %*% sqrt(1/K*solve(D_Sigma)))\n  \n  # Convert back to correct scales (according to Chaven 2017 p. 3)\n  V.man <- (solve(sqrt(1/K*solve(D_Sigma))) %*% SVD.man$v)[, 1:npcs]\n  U.man <- (solve(sqrt(W_mat)) %*% SVD.man$u)[, 1:npcs]\n  L.man <- SVD.man$d[1:npcs]\n\n  # Compare SVD triplet and manual SVD of weighted matrix\n  round(abs(SVD.trip$V) - abs(V.man), 5)\n  round(abs(SVD.trip$U) - abs(U.man), 5)\n  round(abs(SVD.trip$vs[1:npcs]) - abs(L.man), 5)\n  \n  # Reconstruction Formula\n  d_hat <- SVD.trip$vs[1:npcs] # matrix of the singular values \n                            # (Squared would be eigenvalues of Z)\n  u_hat <- SVD.trip$U # Left singular vectors matrix\n  v_hat <- SVD.trip$V # Right singular vectors matrix\n  \n  z_hat <- u_hat %*% diag(d_hat) %*% t(v_hat) + M\n  colSums(z_hat)\n  colSums(Z)\n  \n  # Compare SVD matrices\n  # Matrix of singular values\n  res.mca$svd$vs[1:npcs] -\n    d_hat[1:npcs]\n\n  res.mca$svd$vs[1:npcs] -\n    L.man\n  \n  # Left Singular Vectors Matrix\n  round(\n    res.mca$svd$U - u_hat,\n    3\n  )\n\n  round(\n    res.mca$svd$U - U.man,\n    3\n  )\n\n  # Right Singular Vectors Matrix\n  round(\n    res.mca$svd$V -\n      v_hat, \n    3)\n  # Correlation between columns\n  # And look into the PCAmixdata package\n  round(cor(v_hat, res.mca$svd$V), 1)\n  \n  # Coordinates on Dimensions are recovered\n  round(\n    res.mca$ind$coord -\n      u_hat %*% diag(d_hat),\n    3\n  )\n\n# Following JosseHusson2016 -----------------------------------------------\n\n  I <- nrow(MCA.dt)\n  J <- ncol(MCA.dt)\n  X <- tab.disjonctif(MCA.dt)\n  rowMarg <- rowSums(X) # = J\n  colMarg <- colSums(X) # = number of ids in a category\n  D_Sigma <- diag(colMarg)\n  D <- 1/I * diag(rep(1, I)) # rowMasses\n  SVD.trip <- svd.triplet(X = diag(rep(1, I)) %*% X %*% solve(D_Sigma),\n                          row.w = diag( D ),\n                          col.w = diag( 1/(I*J)*D_Sigma ),\n                          ncp=2\n                          )\n  svd(I * X %*% solve(D_Sigma))\n  \n  SVD.trip$vs\n  round(SVD.trip$vs[2:3] - res.mca$svd$vs[1:2], 3)\n  \n  SVD.trip$U\n  res.mca$svd$U\n#   round(SVD.trip$U[, 2:3] - res.mca$svd$U, 3)\n  res.mca$svd$U\n  \n#   SVD.trip$V\n#   round(SVD.trip$V[, 2:3] - res.mca$svd$V, 3)\n#   res.mca$svd$V\n\n```\n\n```{r pca-mix}\n# Prepare environment ----------------------------------------------------------\n\nlibrary(psych)\nlibrary(PCAmixdata)\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n\ndata(tea)\n\nhead(tea)\n\nlapply(tea[, c(\"where\", \"how\", \"SPC\")], nlevels)\n\nsapply(tea, nlevels)\n\nx <- tea[, c(\"where\", \"how\", \"Tea\")]\n\nCTD <- tab.disjonctif(x)\npk <- colMeans(CTD)\n\nCTD[1, ] / pk\nCTD[4, ] / pk\n1/.64\n1/.56666667\n\nCTD_t <- t(apply(CTD, 1, function(r) {t(r)/pk} ))\ncolMeans(CTD_t)\n\n\nN <- tab.disjonctif(x)\n1/nrow(tea)\n\n# Correspondance analysis based on contingency table ---------------------------\nx <- tea[, c(\"SPC\", \"where\")]\nn <- nrow(x)\nr <- nlevels(x[, 1]) \nC <- nlevels(x[, 2]) \nN <- table(x)\n\n# Contingency table\nN\n\n# Indicator matrix\nZ <- tab.disjonctif(x)\nZ1 <- Z[, 1:r]\nZ2 <- Z[, -c(1:r)]\nN - t(Z1) %*% Z2\n\n# Correspondance matrix\nP <- 1/n * N\n\n# From Greenacre1984\nN\n\n# Column and row sums\nr_bold <- rowSums(N)\nc_bold <- colSums(N)\n\ndrop(N %*% rep(1, ncol(N))) - r_bold\ndrop(t(N) %*% rep(1, nrow(N))) - c_bold\n\nD_r <- diag(r_bold)\nD_c <- diag(c_bold)\n\n# Matrices of profiles\n\nR <- solve(D_r) %*% P\nC <- solve(D_c) %*% t(P)\n\n# Centroids\nr <- t(C) %*% c_bold\nc <- t(R) %*% r_bold\n\n# Generalized SVD of P - r_bold t(c_bold)\n\nP - r_bold %*% t(c_bold)\nA <- svd(P - r %*% t(c))$u\nB <- svd(P - r %*% t(c))$v\n\nt(A) %*% solve(D_r) %*% A\nt(B) %*% solve(D_c) %*% B\n\n# From Jolliffe p. 37\n# r_bold <- rep(1, r)\n# c_bold <- rep(1, C)\n# D_r <- diag(r_bold)\n# D_c <- diag(c_bold)\n# Omega <- solve(D_r)\n# Psi <- solve(D_c)\n# X <- P - r_bold %*% t(c_bold)\n\n# V <- svd(X)$u\n# M <- diag(svd(X)$d)\n# B <- svd(X)$v\n\n# round(t(V) %*% Omega %*% V, 3)\n# round(t(B) %*% Psi %*% B, 3)\n\n# X_til <- sqrt(Omega) %*% X %*% sqrt(Psi)\n\n# W <- svd(X_til)$u\n# K <- diag(svd(X_til)$d)\n# C <- svd(X_til)$v\n\n# solve(sqrt(Omega)) %*% W - V\n# solve(sqrt(Omega)) %*% C - B\n\n# W %*% K\n\n# # Row profiles\n# D_r\n\n# MCA based on Audigier et al 2017 p. 505 (p. 5 of pdf) ------------------------\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define row weights\n    I <- nrow(x)\n    R <- diag(1 / I, I)\n\n    # Define Z (the disjunctive table)\n    Z <- tab.disjonctif(x)\n\n    # Define column weights\n    K <- ncol(x)\n    plxk <- colMeans(Z)\n    D_sigma <- diag(plxk)\n    1 / K * solve(D_sigma)\n\n    # Define M\n    M <- matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n\n    # Centered matrix?\n    Z - M\n\n# MCA based on Chavent Et Al 2017 et al 2017 p. 505 (p. 5 of pdf) -----------1  ---\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define Z (the disjunctive table)\n    G <- tab.disjonctif(x)\n\n    # Define row weights\n    n <- nrow(x)\n    N <- diag(1 / n, n)\n\n    # Define column weights\n    M <- diag(n / colSums(G))\n    solve(D_sigma)\n\n    # Create Z, the centered G?\n    plxk <- colMeans(G)\n    G - matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n    Z <- t(t(G) - plxk)\n\n    # SVD of Z\n    u_til <- svd(Z)$u\n    lambda_til <- svd(Z)$d\n\n    # Principal Compoent Scores (factor coordinates of the rows)\n    u_til[, 1:3] %*% diag(lambda_til[1:3])\n```\n\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"pca-mix.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"custom.scss","title-block-banner":true,"draft":true,"title":"PCA with metrics, dimensionality reduction through PCA and MCA","author":"Edoardo Costantini","date":"2022-05-17","slug":"pca-mix","categories":["Categorical data"],"bibliography":["../../resources/bibshelf.bib"]},"extensions":{"book":{"multiFile":true}}}}}