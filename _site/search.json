[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n2022\nEdoardo Costantini, Kyle M. Lang, Tim Reeskens, and Klaas Sijtsma. (2022) \"High-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.\" tbd\n        \n        Github\n     \n        \n        dataset\n     \n        \n        Preprint\n     \n        \n        Published\n    \nEdoardo Costantini, Kyle M. Lang, Tim Reeskens, and Klaas Sijtsma. (2022) \"Solving the many variables problem in MICE with principal component regression.\" tbd\n        \n        Github\n     \n        \n        dataset\n     \n        \n        shiny\n     \n        \n        Preprint\n     \n        \n        Published\n    \nWorking on it\n2023\nEdoardo Costantini, Kyle M. Lang, Tim Reeskens, and Klaas Sijtsma. (2023) \"Supervised principal component regression imputation in MICE.\" tbd\n        \n        Github\n     \n        \n        shiny"
  },
  {
    "objectID": "posts/series-sampling/norta.html",
    "href": "posts/series-sampling/norta.html",
    "title": "Normal to anything (NORTA) sampling",
    "section": "",
    "text": "The Normal to anything (or NORTA) is a sampling approach that allows to generate multivariate data with a known correlation structure and arbitrary marginal distributions. For example, you could generate 2 variables with a given correlation structure and poissan marginal distirbutinos. The only requirement is that the target marginal distributions need to have an (inverse) cumulative distribution function."
  },
  {
    "objectID": "posts/series-sampling/norta.html#sample-from-multivariate-normal-distribution",
    "href": "posts/series-sampling/norta.html#sample-from-multivariate-normal-distribution",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Sample from multivariate normal distribution",
    "text": "Sample from multivariate normal distribution\nFirst we sample 1000 observations from a multivariate normal distirbution with four correlated variables (\\(\\rho = .7\\)).\n\n# 1. Sample from multivariate normal distribution -----------------------------\n\n# Set the seed\nset.seed(20210422)\n\n# Fix parameters\nn <- 1e3 # smaple size\np <- 2  # number of variables\nmu <- rep(0, p) # vector of means\nSigma <- matrix(.7, nrow = p, ncol = p); diag(Sigma) <- 1 # correlation matrix\n\n# Sample Multivairate Normal data\nX <- mvrnorm(n = n, mu = mu, Sigma = Sigma)\n\n# Plot the multivariate distribution (scatterplot)\nX_scatter <- ggplot(data.frame(X), aes(x = X1, y = X2)) +\n      geom_point()\n\n# Add marginals of X\nggMarginal(X_scatter, type = \"histogram\")"
  },
  {
    "objectID": "posts/series-sampling/norta.html#transform-normal-marginals-to-uniform",
    "href": "posts/series-sampling/norta.html#transform-normal-marginals-to-uniform",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Transform normal marginals to uniform",
    "text": "Transform normal marginals to uniform\nWe can now transform the marginal distributions fo the \\(x\\)s to any target paramteric distribution. For example, consider transforming the marginals to poissan distributions. First, we compute the values of the normal cumulative distribution function (pnorm()) and then we compute the quantiles corresponding to the resulting cumulative probabilities based on the target marginal distribution using the qpois() function.\n\n# 2. Transform marginals to a uniform distribution -----------------------------\n\n# Transform to uniform distribution (apply normal CDF to X)\nU <- pnorm(X) \n\n# Make scatterplot\nU_scatter <- ggplot(data.frame(U), aes(x = X1, y = X2)) +\n      geom_point()\n\n# Add marginals of U\nggMarginal(U_scatter, type = \"histogram\")"
  },
  {
    "objectID": "posts/series-sampling/norta.html#transform-uniform-marginals-to-anything",
    "href": "posts/series-sampling/norta.html#transform-uniform-marginals-to-anything",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Transform uniform marginals to anything",
    "text": "Transform uniform marginals to anything\n\n# 3. Transform marginals to anything with a (inverse) CDF ----------------------\n\n# Transform to poissan (apply target inverse-CDF to X)\nX_pois <- qpois(U, lambda = 2)\n\n# And visualize\nX_pois_scatter <- ggplot(data.frame(X_pois), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_pois_scatter, type = \"histogram\") \n\n\n\n\nAs another example, consider transforming the marginals to uniform distributions:\n\n# Transform to a beta distribution\nX_beta <- qbeta(U, shape1 = .5, shape2 = .5)\n\n# And visualize\nX_beta_scatter <- ggplot(data.frame(X_beta), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_beta_scatter, type = \"histogram\") \n\n\n\n\nWe can use this procedure to transform the marginal distribution to any target distribution."
  },
  {
    "objectID": "posts/series-sampling/norta.html#check-correlation-between-the-variables-is-still-the-same",
    "href": "posts/series-sampling/norta.html#check-correlation-between-the-variables-is-still-the-same",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Check correlation between the variables is still the same",
    "text": "Check correlation between the variables is still the same\nWe can see that the relatinoship between \\(X1\\) and \\(X2\\) is still the same.\n\n# Check that the multivariate distribution is still the same\npar(mfrow = c(1, 3))\n\n# Plot linear relationship between normal x1 and x2 (original data)\nplot(X[, 1], X[, 2], \n    main = paste0(\"Normal marginals, cor = \", round(cor(X)[1, 2], 3)), \n    xlab = \"x1\", ylab = \"x2\"\n    )\n\n# Plot linear relationship between normal x1 and x2 (original data)\nplot(X_pois[, 1], X_pois[, 2], \n    main = paste0(\"Poissan marginals, cor = \", round(cor(X_pois)[1, 2], 3)), \n    xlab = \"x1\", ylab = \"x2\"\n    )\nabline(lm(X_pois[, 1] ~ X_pois[, 2]), col = \"blue\", lwd = 3)\n\n# Plot linear relationship between normal x1 and x2 (original data)\nplot(X_beta[, 1], X_beta[, 2], \n    main = paste0(\"Beta marginals, cor = \", round(cor(X_beta)[1, 2], 3)), \n    xlab = \"x1\", ylab = \"x2\"\n    )\nabline(lm(X_beta[, 1] ~ X_beta[, 2]), col = \"blue\", lwd = 3)"
  },
  {
    "objectID": "posts/series-sampling/norta.html#online-resources",
    "href": "posts/series-sampling/norta.html#online-resources",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Online resources",
    "text": "Online resources\nSAS blogs: introduction to copulas"
  },
  {
    "objectID": "posts/series-em/sweep.html",
    "href": "posts/series-em/sweep.html",
    "title": "The sweep operator",
    "section": "",
    "text": "The sweep operator is a matrix transformation commonly used to estimate regression models. It performs elementary row operations on a \\(p \\times p\\) matrix which happens to be particularly useful to estimate multivariate linear models. Little and Rubin (2002, p148) defined it as follows:\n\nThe sweep operator is defined for symmetric matrices as follows. A \\(p \\times p\\) symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric \\(p \\times p\\) matrix H with elements defined as follows: \\[\nh_{kk} = -1/g_{kk}\n\\] \\[\nh_{jk} = h_{kj} = \\frac{g_{jk}}{g_{kk}}, j \\neq k\n\\] \\[\nh_{jl} = g_{jl} - \\frac{g_{jk}g_{kl}}{g_{kk}}, j \\neq k, l \\neq k\n\\]\n\nThe notation indicating this transformation is usually a variation of \\(\\text{SWEEP}[k]G\\), which can be read as sweeping matrix \\(G\\) on column (and row) \\(k\\). It is important to know that:\n\nAny symmetric matrix \\(G\\) can be swept over \\(l\\) multiple positions. The notation \\(\\text{SWEEP}[k_1, k_2, ..., k_l]G\\) indicates successive applications of \\(\\text{SWEEP}[k]G\\) with \\(k = k_1, \\dots, k_l\\).\nThe sweep operator is commutative. Sweeps on multiple positions do not need to be carried out in any particular order:\n\n\\[\n\\text{SWEEP}[k_2]\\text{SWEEP}[k_1]G = \\text{SWEEP}[k_1]\\text{SWEEP}[k_2]G\n\\]\n\nThe \\(l\\) sweeping positions do not need to be consecutive. For example, \\(k_1\\) could indicate the third column and \\(k_2\\) could indicate the sixth column.\n\nIn this post, I want to show how the sweep operator can be used to estimate the parameters of any linear regressions model. If you are interested in the mathematical details, I recommend reading the full sweep operator description in Goodnight (goodnight:1979?), Schafer (1997), or Little and Rubin (2002, p148).\nGoodnight (goodnight:1979?) is a particularly helpful paper as it describes an easy implementation of the sweep operator. Following Goodnight, given an originally symmetric positive definite matrix G, \\(\\text{SWEEP}[k]G\\) modifies a matrix G as follows:\n\nStep 1: Let \\(D = g_{kk}\\)\nStep 2: Divide row \\(k\\) by \\(D\\).\nStep 3: For every other row \\(i \\neq k\\), let \\(B = g_{ik}\\). Subtract \\(B \\times \\text{row } k\\) from row \\(i\\). Set \\(g_{ik} = -B/D\\).\nStep 4: Set \\(g_{kk} = 1/D\\)."
  },
  {
    "objectID": "posts/series-em/sweep.html#coding-a-sweep-function-in-r",
    "href": "posts/series-em/sweep.html#coding-a-sweep-function-in-r",
    "title": "The sweep operator",
    "section": "Coding a sweep function in R",
    "text": "Coding a sweep function in R\nLet’s start by coding a simple function that performs the operations described by Goodnight (goodnight:1979?). We want a function that takes as inputs a symmetric matrix (argument G) and a vector of positions to sweep over (argument K). The function below takes these two inputs and performs the four sweep steps for every element of K.\n\n# Write an R function implementing SWEEP(k)[G] according to Goodnight ----------\n\nsweepGoodnight <- function (G, K){\n\n  for(k in K){\n    # Step 1: Let D = g_kk\n    D <- G[k, k]\n\n    # Step 2: Divide row k by D.\n    G[k, ] <- G[k, ] / D\n\n    # Step 3:\n    # - For every other row i != k, let B = g_ik\n    # - Subtract B \\times row k from row i.\n    # - set g_ik = -B/D.\n    for(i in 1:nrow(G)){\n      if(i != k){\n        B <- G[i, k]\n        G[i, ] <- G[i, ] - B * G[k, ]\n        G[i, k] <- -1 * B / D\n      }\n    }\n    # Step 4: Set g_kk = 1/D\n    G[k, k] = 1/D\n  }\n\n  # Output\n  return(G)\n}\n\nLet’s check that this function returns what we want by comparing it with a function implemented by someone else.\n\n# Compare sweepGoodnight with other implementations ----------------------------\n\n# Install the `fastmatrix` package (run if you don't have it yet)\n# install.packages(\"fastmatrix\")\n\n# Load fastmatrix\nlibrary(fastmatrix)\n\n# Define an example dataset\nX <- matrix(c(1, 1, 1, 1,\n              1, 2, 1, 3,\n              1, 3, 1, 3,\n              1, 1,-1, 2,\n              1, 2,-1, 2,\n              1, 3,-1, 1), ncol = 4, byrow = TRUE)\n\n# Define the G matrix\nG <- crossprod(X)\n\n# Define a vector of positions to sweep over\nK <- 1:3\n\n# Perform SWEEP[K]G with fastmatrix sweep.operator\nH_fm <- sweep.operator(G, k = K)\n\n# Perform SWEEP[K]G with our sweepGoodnight implementation\nH_sg <- sweepGoodnight(G, K = K)\n\n# Compare the two\nall.equal(H_fm, H_sg)\n\n[1] TRUE\n\n\nThe functions fastmatrix::sweep.operator() and sweepGoodnight() return the same H matrix by sweeping matrix G over the positions defined in K."
  },
  {
    "objectID": "posts/series-em/sweep.html#using-the-sweep-operator-to-estimate-regression-models",
    "href": "posts/series-em/sweep.html#using-the-sweep-operator-to-estimate-regression-models",
    "title": "The sweep operator",
    "section": "Using the sweep operator to estimate regression models",
    "text": "Using the sweep operator to estimate regression models\nTo understand how the sweep operator relates to the estimation of multivariate linear models, we will work with a data set used by Little and Rubin (2002, p152).\n\n# Load Little Rubin data -------------------------------------------------------\n\n# Create data\n  X <- as.data.frame(\n          matrix(\n                  data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26,\n                           29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68,\n                           6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,\n                           60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12,\n                           78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7,\n                           72.5, 93.1, 115.9, 83.8, 113.3, 109.4),\n                  ncol = 5\n          )\n  )\n\n# Store useful information\n  n <- nrow(X)\n  p <- ncol(X)\n\nLet’s take a quick look at the first rows of the data to get an idea of what we are working with.\n\n# Glance at the first 6 rows of the data\n  head(X)\n\n  V1 V2 V3 V4    V5\n1  7 26  6 60  78.5\n2  1 29 15 52  74.3\n3 11 56  8 20 104.3\n4 11 31  8 47  87.6\n5  7 52  6 33  95.9\n6 11 55  9 22 109.2\n\n\n\nCompute the augmented covariance matrix\nTo obtain the estimates of the regression coefficients of a multivariate linear model, we need to sweep the augmented covariance matrix of the data (\\(\\Theta\\)) over the positions of the predictors. This is a \\((p+1) \\times (p+1)\\) matrix storing the covariance matrix and the means of the dataset. It usually looks like this:\n\\[\n\\Theta =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\sigma^2_1 & ... & \\sigma_{1p}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\sigma_{1p} & ... & \\sigma^2_{p}\n\\end{bmatrix}\n\\]\nwith \\(\\mu_1, \\dots, \\mu_p\\), \\(\\sigma^2_1, \\dots, \\sigma^2_p\\), and \\(\\sigma_{jk}\\) being the means, variances, and covariances of the variables in our dataset, respectively.\nIn R, we can obtain this matrix in just a few steps starting from our dataset X:\n\nAugment the original data with a column of 1s on the left.\nWe can use the cbind() function to append a column of 1s to the left of X. Keep in mind that we need to perform matrix operations with the resulting object. Therefore, we need to make sure we are working with an R object of the class matrix instead of data.frame.\n\n# Obtain the augmented covariance matrix ---------------------------------------\n\n# Augment X\n  X_aug <- cbind(int = 1, as.matrix(X))\n\n# Glance at the first 6 rows of X_aug\n  head(X_aug)\n\n     int V1 V2 V3 V4    V5\n[1,]   1  7 26  6 60  78.5\n[2,]   1  1 29 15 52  74.3\n[3,]   1 11 56  8 20 104.3\n[4,]   1 11 31  8 47  87.6\n[5,]   1  7 52  6 33  95.9\n[6,]   1 11 55  9 22 109.2\n\n\nCompute the augmented matrix of sufficient statistics \\(T\\).\n\\(T\\) is the matrix having as elements the sum of the cross-products of the columns of X_aug.\n\\[\nT =\n\\begin{bmatrix}\nn & \\sum{x_1} & ... & \\sum{x_p}\\\\\n\\sum{x_1} & \\sum{x_1^2} & ... & \\sum{x_1 x_p}\\\\\n... & ... & ... & ...\\\\\n\\sum{x_p} & \\sum{x_1 x_p} & ... & \\sum{x_p^2}\n\\end{bmatrix}\n\\]\nSince the first column of X_aug is a column of 1s, the first element of T is the number of rows in the data, the first column and rows store the sum of scores on each variable (sufficient statistics for the mean), and the other elements store the sum of products between the columns of X (sufficient statistics for the covariance matrix of X).\nIn R, we can compute it easily with the cross-product function:\n\n# Compute the matrix of sufficient statistics (T matrix)\n  Tmat <- crossprod(X_aug)\n\nTransform T to G\n\\(G\\) is simply \\(T / n\\)\n\\[\nG =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\frac{\\sum{x_1^2}}{n} & ... & \\frac{\\sum{x_1 x_p}}{n}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\frac{\\sum{x_1 x_p}}{n} & ... & \\frac{\\sum{x_p^2}}{n}\n\\end{bmatrix}\n\\]\n\n# Compute G\n  G <- Tmat / n\n\nCompute \\(\\Theta\\) by sweeping G over the first row and column.\nLet’s use our sweepGoodnight() function to perform SWEEP[1]G and obtain \\(\\Theta\\)\n\\[\n\\Theta =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\sigma^2_1 & ... & \\sigma_{1p}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\sigma_{1p} & ... & \\sigma^2_{p}\n\\end{bmatrix}\n\\]\nIn R:\n\n# Sweep G over the first position\n  Theta <- sweepGoodnight(G, 1)\n\n# Check how it looks\n  Theta\n\n           int         V1         V2         V3          V4         V5\nint   1.000000   7.461538   48.15385  11.769231   30.000000   95.42308\nV1   -7.461538  31.940828   19.31361 -28.662722  -22.307692   59.68935\nV2  -48.153846  19.313609  223.51479 -12.810651 -233.923077  176.38107\nV3  -11.769231 -28.662722  -12.81065  37.869822    2.923077  -47.55621\nV4  -30.000000 -22.307692 -233.92308   2.923077  258.615385 -190.90000\nV5  -95.423077  59.689349  176.38107 -47.556213 -190.900000  208.90485\n\n# Check Theta is storing the means in the first row and column\n  colMeans(X)\n\n       V1        V2        V3        V4        V5 \n 7.461538 48.153846 11.769231 30.000000 95.423077 \n\n# Check Theta is storing the ML covariance matrix everywhere else\n  cov(X) * (n-1) / n\n\n          V1         V2         V3          V4         V5\nV1  31.94083   19.31361 -28.662722  -22.307692   59.68935\nV2  19.31361  223.51479 -12.810651 -233.923077  176.38107\nV3 -28.66272  -12.81065  37.869822    2.923077  -47.55621\nV4 -22.30769 -233.92308   2.923077  258.615385 -190.90000\nV5  59.68935  176.38107 -47.556213 -190.900000  208.90485\n\n\nPay attention to a couple of things:\n\nThe covariance matrix stored in \\(\\Theta\\) is the Maximum Likelihood version (denominator should be n instead of the default n-1)\nWe could have constructed the object Theta just by using colMeans(X) and cov(X) * (n-1) / n directly. However, it is important to note the relationship between Tmat, G, and Theta. In particular, pay attention to the fact that Theta is the result of sweeping G in the first position. When I started looking into this topic I did not understand this, and I kept sweeping Theta over the first position, resulting in a confusing double sweeping of the first column and row. I will get back to this point in a sec.\n\n\n\n\nEstimate multivariate linear models\nNow let’s see how we can use \\(\\Theta\\) to estimate any multivariate linear model involving the variables in our dataset. First, let’s see how we would obtain these linear models in R with standard procedures. Say we want to regress V1 and V3 on V2, V4, and V5 from the X dataset. We will start by creating a formula for an lm function to estimate the model we want.\n\n# Fit some multivariate linear models ------------------------------------------\n\n  # Define the dependent variables (dvs) of the multivairate linear models\n  dvs <- c(\"V1\", \"V3\")\n\n  # Define the predictors (ivs) of the multivairate linear models\n  ivs <- c(\"V2\", \"V4\", \"V5\")\n\n  # Create the formula (complicated but flexible way)\n  formula_mlm <- paste0(\"cbind(\",\n                       paste0(dvs, collapse = \", \"),\n                       \") ~ \",\n                       paste0(ivs, collapse = \" + \"))\n\n  # Check the formula\n  formula_mlm\n\n[1] \"cbind(V1, V3) ~ V2 + V4 + V5\"\n\n\nNext, we will fit the multivariate linear model with the lm() function:\n\n  # Fit the model with the lm function\n  mlm0 <- lm(formula_mlm, data = X)\n  coef(mlm0)\n\n                     V1          V3\n(Intercept) -45.7660931 135.1150663\nV2           -0.2747666  -0.6559719\nV4            0.1455375  -1.0485195\nV5            0.6507081  -0.6319507\n\n\nThese are our intercepts, and regression coefficients for the multivariate linear model. We can sweep \\(\\Theta\\) over the positions of the independent variables to obtain the the same intercept and regression coefficients. First, let’s define a vector of positions to sweep over based on the variable names we stored in ivs.\n\n# Fit some multivariate linear models using sweep ------------------------------\n\n  # Define positions to sweep over\n  sweep_over <- which(colnames(Theta) %in% ivs)\n\nThen, let’s simply sweep our \\(\\Theta\\) over these positions.\n\n  # Sweep theta\n  H <- sweepGoodnight(Theta, K = sweep_over)\n\n  # Check out the result\n  H\n\n            int          V1           V2          V3           V4           V5\nint  612.422481 -45.7660931 -5.874822779 135.1150663 -6.469828251 -1.408803042\nV1    45.766093   1.6538239  0.274766592  -1.6628629 -0.145537538 -0.650708148\nV2    -5.874823  -0.2747666  0.085293622  -0.6559719  0.073716182 -0.004651691\nV3  -135.115066  -1.6628629  0.655971950   2.4781175  1.048519534  0.631950668\nV4    -6.469828   0.1455375  0.073716182  -1.0485195  0.075591156  0.006836668\nV5    -1.408803   0.6507081 -0.004651691  -0.6319507  0.006836668  0.014961788\n\n\nOur regression coefficients are here in this new matrix. We just need to find them. We know that the dependent variables are V1 and V3, and that the independent variables are V2, V4, and V5. Let’s index the rows of H with the names of the ivs (and the name of the intercept row), and the columns of H with the names of the dvs.\n\n  # Extract the regression coefficients from H\n  H[c(\"int\", ivs), dvs]\n\n             V1          V3\nint -45.7660931 135.1150663\nV2   -0.2747666  -0.6559719\nV4    0.1455375  -1.0485195\nV5    0.6507081  -0.6319507\n\n  # Compare with coefficients from lm function\n  coef(mlm0)\n\n                     V1          V3\n(Intercept) -45.7660931 135.1150663\nV2           -0.2747666  -0.6559719\nV4            0.1455375  -1.0485195\nV5            0.6507081  -0.6319507\n\n\nNote that, we are sweeping \\(\\Theta\\) only over the predictors, but we also get the estimate of the intercept. Remember that \\(\\Theta\\) is the result of sweeping G over the first position, which is the position where the intercept estimate appears. You could obtain the same result by directly sweeping G over position 1, and the position of the predictors. In code:\n\n  # Sweep G\n  sweepGoodnight(G, c(1, sweep_over))[c(\"int\", ivs), dvs]\n\n             V1          V3\nint -45.7660931 135.1150663\nV2   -0.2747666  -0.6559719\nV4    0.1455375  -1.0485195\nV5    0.6507081  -0.6319507\n\n\nTherefore, you can think of finding the coefficients of a multivariate linear model using the sweep operator as:\n\nSWEEP(1, \\(k_1, \\dots, k_l\\))[G] or as,\nSWEEP(\\(k_1, \\dots, k_l\\))[SWEEP(1)[G]] or as,\nSWEEP(\\(k_1, \\dots, k_l\\))[\\(\\Theta\\)]\n\nwith \\(k_1, \\dots, k_l\\) being the positions of the \\(K\\) predictors in matrix \\(G\\).\nFinally, just play around with what variables you consider as dvs and ivs. You will discover the magic of the sweep operator.\n\n# Play around with variable roles ------------------------------------------\n\n  # Define different dependent variables (dvs) for the multivairate linear models\n  dvs <- c(\"V1\", \"V2\", \"V5\")\n\n  # Define different predictors (ivs) for the multivairate linear models\n  ivs <- c(\"V3\", \"V4\")\n\n  # Create the formula (complicated but flexible way)\n  formula_mlm <- paste0(\"cbind(\",\n                       paste0(dvs, collapse = \", \"),\n                       \") ~ \",\n                       paste0(ivs, collapse = \" + \"))\n\n  # Fit the model with the MLM\n  mlm1 <- lm(formula_mlm, data = X)\n  coef(mlm1)\n\n                     V1         V2          V5\n(Intercept) 18.63186149 78.3607367 131.2824064\nV3          -0.75087203 -0.2686979  -1.1998512\nV4          -0.07777123 -0.9014841  -0.7246001\n\n  # Define positions to sweep over\n  sweep_over <- which(colnames(Theta) %in% ivs)\n\n  # Sweep Theta over new positions\n  sweepGoodnight(Theta, K = sweep_over)[c(\"int\", ivs), dvs]\n\n             V1         V2          V5\nint 18.63186149 78.3607367 131.2824064\nV3  -0.75087203 -0.2686979  -1.1998512\nV4  -0.07777123 -0.9014841  -0.7246001"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html",
    "href": "posts/series-pca-primer/pls.html",
    "title": "Implementing a PLS alogirthm in R",
    "section": "",
    "text": "Many data analysts face the problem of analyzing data sets with many, often highly correlated, variables. Partial Least Square Regression (PLSR) is a regression method that uses linear combinations of the original predictors to reduce their dimensionality. As principal component regression, PLS uses derived inputs, however, it differs from PCR by how the linear combinations are constructed.\nGiven a set of predictors \\(X_{n \\times p}\\) and a vector of dependent variable scores \\(y_{n \\times 1}\\), the least-square solution for the multiple linear regression\n\\[\ny = X \\beta + \\epsilon\\text{, with } \\epsilon \\sim N(0, \\sigma^2)\n\\]\nis\n\\[\n\\beta = (X'X)^{-1}X'y\n\\]\nwhere \\(X'\\) is the transpose of the data matrix \\(X\\), and \\(()^{-1}\\) is the matrix inverse. When collinearity is present in \\(X\\) or \\(p > n\\), then \\(X'X\\) is singular and its inverse cannot be computed. Derived input regression methods like PCR and PLSR bypass this problem by taking linear combinations of the columns of the original \\(X\\) and regressing \\(Y\\) on just a few of these linear combinations. The peculiarity of PLSR is that it includes information on both \\(X\\) and \\(Y\\) in the definition of the linear combinations. In this post, we look at two algorithms to estimate PLSR to get a better understanding of the method.\n\n\nHere, I describe informally the algorithm steps:\n\nPreprocessing the data - the columns of the input matrix \\(X\\) are standardized to have mean 0 and variance 1.\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed: \\(\\rho_{1j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the first partial-least-square direction \\(z_1\\) - The cross-products \\(\\rho_{1j}\\) are used as weights to obtain a linear combination of the columns: \\(z_1 = \\sum \\rho_{1j} x_{j}\\). This implies that the contribution of each column to \\(z_1\\) is weighted by their univariate relationship with the dependent variable \\(y\\).\nRegression of \\(y\\) on \\(z_1\\) - The outcome variable \\(y\\) is regressed on this first direction \\(z_1\\) to obtain \\(\\hat{\\theta}_1\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_1\\).\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed again: \\(\\rho_{2j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the second partial-least-square direction \\(z_2\\) - The cross-products \\(\\rho_{2j}\\) are used as weights to obtain a linear combination of the columns: \\(z_2 = \\sum \\rho_{2j} x_{j}\\). Notice that the columns \\(x_j\\) we are using now are orthogonal to the previous partial least square direction \\(z_1\\).\nRegression of \\(y\\) on \\(z_2\\) - The outcome variable \\(y\\) is regressed on the second direction \\(z_2\\) to obtain \\(\\hat{\\theta}_2\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_2\\).\n\nThe procedure continues until \\(M\\) partial least square directions have been computed. The result is a set of independent directions that have both high variance and high correlation with the dependent variable, in contrast to PCR which finds a set of independent directions that have high variance.\nNow we report pseudo-code for the implementation of the PLS algorithm (inspired by Report Algorithm 3.3 p.103 as in (Hastie, Tibshirani, and Wainwright 2015)). We will use it to write the R code in the next session.\n\nStandardized each \\(x_j\\) to have mean 0 and variance 1\nSet:\n\n\\(\\hat{y}^{(0)} = \\bar{y}1\\)\n\\(x_{j}^{(0)} = x_{j}\\)\n\nFor \\(m = 1, 2, \\dots, M\\)\n\n\\(z_m = \\sum_{j = 1}^{p} \\rho_{mj}x_{j}^{(m-1)}\\)\n\\(\\hat{\\theta}_m = \\frac{z_m'y}{z_m' z_m}\\)\n\\(\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_m z_m\\)\nfor \\(j = 1, \\dots, p\\) orthogonalize \\(x_{j}\\) with respect to \\(z_m\\): \\(x_{j}^{(m)} = x_{j}^{(m-1)} - \\frac{z_m' x_{j}^{(m)}}{z_m' z_m}z_m\\)\n\nOutput the sequence of fitted vectors \\(\\hat{y}^{m}\\)"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#coding-the-pls-algorithm-manually",
    "href": "posts/series-pca-primer/pls.html#coding-the-pls-algorithm-manually",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Coding the PLS algorithm manually",
    "text": "Coding the PLS algorithm manually\nLet’s go through the steps described in the pseudo code above. First we want to standardize the predictors.\n\n# 1. Standardize the data ------------------------------------------------------\n\n    # Scale the predictors\n    Xstd <- scale(X)\n\n    # Means of X are now 0\n    cbind(\n        X = head(colMeans(X)), \n        Xstd = round(head(colMeans(Xstd)), 5)\n        )\n\n                         X Xstd\nconfused         1.8196721    0\nright words      1.7950820    0\nsensations       0.5983607    0\ndescribe         2.2213115    0\nanalyze problems 2.5081967    0\nupset            1.6065574    0\n\n    # SD of X are now 1\n    cbind(\n        X = head(apply(X, 2, sd)),\n        Xstd = round(head(apply(Xstd, 2, sd)), 5)\n        )\n\n                        X Xstd\nconfused         1.178431    1\nright words      1.278807    1\nsensations       1.073034    1\ndescribe         1.216364    1\nanalyze problems 1.077539    1\nupset            1.295621    1\n\n\nThen we want to set the intial values\n\n# 2. Set initial vlaues --------------------------------------------------------\n\n    # 2a: Set the initial prediction for y_hat to the mean of y\n    # Create an empty data.frame to store the initial value and future predictions\n    yhat_m <- matrix(rep(NA, n * (M + 1)), nrow = n)\n\n    # Replace the initial prediction with the mean of y\n    yhat_m[, 1] <- mean(y)\n\n    # 2b: Set every xj0 to xj\n    # Create an empty list of X values\n    Xm <- lapply(1:(M + 1), matrix, nrow = n, ncol = p)\n\n    # Place X as initial value for Xm\n    Xm[[1]] <- as.matrix(Xstd)\n\nFinally, we can move to the estimation step. First we create the number\n\n# 3. Estimation ----------------------------------------------------------------\n\n    # Preparing objects\n    z <- matrix(NA, nrow = n, ncol = (M + 1)) # container for directions\n    W <- matrix(NA, nrow = p, ncol = (M + 1)) # container for directions\n    theta_hat <- rep(NA, (M + 1)) # container for thetas\n\nThen we move to the proper estimation.\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:(M + 1)) {\n        # 3a: Compute zm\n        W[, m] <- t(Xm[[m - 1]]) %*% y   # inner products / covariances\n        W / n\n        z[, m] <- Xm[[m - 1]] %*% W[, m] # compute the direction zm\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] <- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        yhat_m[, m] <- yhat_m[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            Xm[[m]][, j] <- orthogonalize(Xm[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Fit PLSR model w/ pls package\n    pls_fit_pls <- pls::plsr(\n        y ~ as.matrix(X),\n        ncomp = M,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n\n    # Fit PCR model w/ plsdof package\n    pls_fit_plsdof <- plsdof::pls.model(as.matrix(Xstd), y)\n    pls_fit_plsdof$coefficients[, 3+1]\n\n [1] -0.77322985 -0.55969544  0.44823243  0.42519983 -0.82981915  0.27264689\n [7] -0.88380881  0.07420445 -0.24761114  0.10450986 -0.06604870  0.52244833\n[13] -1.67949848 -0.88035012 -0.90311608  0.28814506  0.11064435  0.51922786\n[19]  0.94014053 -0.62037921\n\n    # Compare predictions using up to a given m\n    m <- 3\n    head(\n        data.frame(\n            pls = round(as.data.frame(fitted(pls_fit_pls)), 3)[, m],\n            plsdof = round(pls_fit_plsdof$Yhat, 3)[, m + 1],\n            man = round(yhat_m, 3)[, m + 1]\n    ))\n\n     pls plsdof    man\n1 36.809 36.819 36.819\n2 29.231 29.243 29.243\n3 31.197 31.336 31.336\n4 30.902 30.767 30.767\n5 35.186 34.869 34.869\n6 25.087 24.876 24.876"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#types-of-dependent-variables",
    "href": "posts/series-pca-primer/pls.html#types-of-dependent-variables",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Types of dependent variables",
    "text": "Types of dependent variables\n\n# Types of DVs -----------------------------------------------------------------\n\n    data(oliveoil)\n    sens.pcr <- pls::pcr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n    sens.pls <- pls::plsr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n\n    oliveoil$sensory\n\n   yellow green brown glossy transp syrup\nG1   21.4  73.4  10.1   79.7   75.2  50.3\nG2   23.4  66.3   9.8   77.8   68.7  51.7\nG3   32.7  53.5   8.7   82.3   83.2  45.4\nG4   30.2  58.3  12.2   81.1   77.1  47.8\nG5   51.8  32.5   8.0   72.4   65.3  46.5\nI1   40.7  42.9  20.1   67.7   63.5  52.2\nI2   53.8  30.4  11.5   77.8   77.3  45.2\nI3   26.4  66.5  14.2   78.7   74.6  51.8\nI4   65.7  12.1  10.3   81.6   79.6  48.3\nI5   45.0  31.9  28.4   75.7   72.9  52.8\nS1   70.9  12.2  10.8   87.7   88.1  44.5\nS2   73.5   9.7   8.3   89.9   89.7  42.3\nS3   68.1  12.0  10.8   78.4   75.1  46.4\nS4   67.6  13.9  11.9   84.6   83.8  48.5\nS5   71.4  10.6  10.8   88.1   88.5  46.7\nS6   71.4  10.0  11.4   89.5   88.5  47.2"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#prediction-of-new-data",
    "href": "posts/series-pca-primer/pls.html#prediction-of-new-data",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Prediction of new data",
    "text": "Prediction of new data\nHow do we predict new data? Let’s start by generating data form scratch\n\n# Prediction -------------------------------------------------------------------\n\n    n <- 50 # number of observations\n    p <- 15 # number of variables\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- 100 + rnorm(n)\n    M <- 10 # number of \"components\"\n\n    ntest <- 200 #\n    Xtest <- matrix(rnorm(ntest * p), ncol = p) # test data\n    ytest <- rnorm(ntest) # test data\n\n    # Fit alternative PLSs\n    out_pls <- pls::plsr(\n        y ~ X,\n        ncomp = M,\n        scale = TRUE,\n        center = TRUE,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n    out_plsdof <- plsdof::pls.model(X, y, compute.DoF = TRUE, Xtest = Xtest, ytest = NULL)\n\n    # Obtain predictions on new data\n    head(\n        round(\n            cbind(\n                PLS = predict(out_pls, newdata = Xtest)[, , M],\n                PLSdof = out_plsdof$prediction[, M + 1]\n            ), 5\n        )\n    )\n\n           PLS    PLSdof\n[1,]  99.91546  99.91546\n[2,] 100.52518 100.52518\n[3,] 100.61482 100.61482\n[4,]  99.81698  99.81698\n[5,] 100.38015 100.38015\n[6,] 100.76092 100.76092\n\n    # Make sure scale of prediction is correct\n    out_pls_cF <- plsr(\n      y ~ X,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    Xs <- scale(X, center = TRUE, scale = FALSE)\n    ys <- scale(y, center = FALSE, scale = FALSE)\n\n    out_pls_cF <- plsr(\n      ys ~ Xs,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    head(\n        round(\n        cbind(\n            PLS = predict(out_pls, newdata = Xtest)[, , M],\n            PLS_cf = mean(y) + predict(out_pls_cF, newdata = Xtest)[, , M],\n            PLSdof = out_plsdof$prediction[, M]\n        ), 5\n        )\n    )\n\n           PLS    PLS_cf    PLSdof\n[1,]  99.91546  99.83684  99.91662\n[2,] 100.52518 100.44656 100.52460\n[3,] 100.61482 100.53620 100.61432\n[4,]  99.81698  99.73836  99.81537\n[5,] 100.38015 100.30153 100.37996\n[6,] 100.76092 100.68230 100.76126"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#degrees-of-freedom-of-the-residuals",
    "href": "posts/series-pca-primer/pls.html#degrees-of-freedom-of-the-residuals",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Degrees of freedom of the residuals",
    "text": "Degrees of freedom of the residuals\n\n# PLS degrees of freedom -------------------------------------------------------\n\n    library(plsdof)\n    set.seed(1234)\n\n    # Generate data data\n    n <- 100 # number of observations\n    p <- 15 # number of variables\n    m <- 15\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- rnorm(n)\n\n    # Fit model with package\n    outpls <- pls.model(X, y, compute.DoF = TRUE)\n    outpls$DoF\n    outpls.internal <- linear.pls.fit(X, y, m, DoF.max = min(n - 1, p + 1))\n\n    # Fit model with person PLS function\n    outpls_man <- pls.manual(ivs = X, dv = y, m = m)\n\n    # Fit model with plsr function from pls\n    pls_fit_pls <- plsr(\n        y ~ X,\n        ncomp = m,\n        scale = FALSE,\n        center = FALSE,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n\n    # Y hats\n    round(outpls_man$Yhat - outpls$Yhat, 5)\n\n    # T scores\n    j <- 1\n    cbind(\n        PLSR = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2)))[, j],\n        PLSTT = outpls.internal$TT[, j],\n        manualTs = outpls_man$Ts[, j],\n        manualTsn = outpls_man$Tsn[, j]\n    )\n\n    # Degrees of freedom PLSR\n    predi <- sapply(1:m, function(j) {\n        predict(pls_fit_pls, ncomp = j)\n    })\n    DoF_plsr <- dofPLS(\n        X,\n        y,\n        TT = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2))),\n        Yhat = predi,\n        DoF.max = m + 1\n    )\n\n    # Degrees of freedom manual\n    DoF_manual <- dofPLS(\n        X,\n        y,\n        TT = outpls_man$Tsn,\n        Yhat = outpls_man$Yhat[, 2:(m + 1)],\n        DoF.max = m + 1\n    )\n\n    # Single DoF\n    DoF_single <- c(1, sapply(1:m, function(i) {\n        dofPLS_single(\n            X,\n            y,\n            TT = outpls_man$Tsn,\n            Yhat = outpls_man$Yhat[, (i + 1)],\n            q = i\n        )\n    }))\n\n    # Compare degrees of freedom\n    cbind(\n        PLSR = DoF_plsr,\n        PLSdof = outpls$DoF,\n        PLS_manual = DoF_manual,\n        diff = round(outpls$DoF - DoF_manual, 5),\n        DoF_single = DoF_single\n    )"
  },
  {
    "objectID": "posts/series-pca-primer/pca-non-graphical-solutions.html",
    "href": "posts/series-pca-primer/pca-non-graphical-solutions.html",
    "title": "Deciding the Number of PCs with Non-Graphical Solutions to the Scree Test",
    "section": "",
    "text": "Learn by coding\n\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres <- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues <- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n\n\n\n# Non-graphical solutions\nngs <- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man <- sum(eigenvalues > 1)\n\n# Accelration factor\na <- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] <- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man <- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n\n        manual nFactor\nnaf          2       2\nnkaiser      2       2\n\n\n\n\nTL;DR, just give me the code!\n\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres <- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues <- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n\n# Non-graphical solutions\nngs <- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man <- sum(eigenvalues > 1)\n\n# Accelration factor\na <- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] <- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man <- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n\n\n\nOther resources\n\nCross-entropy in RPubs\nA Gentle Introduction to Cross-Entropy for Machine Learning\nUnderstanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names\nML Gloassary\nLoss Functions in Machine Learning"
  },
  {
    "objectID": "posts/series-pca-primer/pca-svd.html",
    "href": "posts/series-pca-primer/pca-svd.html",
    "title": "Principal Component Analysis and SVD",
    "section": "",
    "text": "Learn by coding\n\nsome <- 1\n\n\n\nTL;DR, just give me the code!\n\nsome <- 1\n\n\n\n\n\n\n\n\n\nReferences\n\nJolliffe, Ian T. 2002. Principal Component Analysis. Springer."
  },
  {
    "objectID": "posts/series-sm-course/boxplots.html",
    "href": "posts/series-sm-course/boxplots.html",
    "title": "Understanding boxplots",
    "section": "",
    "text": "Boxplots are descriptive tools to visualize the distribution of variables with a focus on their measures of spread and center. A boxplots report in the same figure the median, the 1st and 3rd quartiles, and indicate possible outliers.\nImagine wanting to plot the distribution of age in a court of students enrolled in a master program at a university. The age of the students is likely to be normally distributed around a mean of 26.\n\n# Set up ----------------------------------------------------------------------\n\n# Set seed\nset.seed(20220906)\n\n# Generate some age variable for a university master programme\nage <- round(rnorm(1e3, mean = 26, sd = 2), 0)\n\nThen, we can create the boxplot of this age variable in R by using the boxplot() function.\n\n# Look at the boxplot ---------------------------------------------------------\nboxplot(age)\n\n\n\n\n\n\nThe variable age is centered around 26 and 50% of the distribution is located between 25 (1st quartile) and 27 (3rd quartile). There are 6 values that represent possible outliers (the circles outside the whiskers)."
  },
  {
    "objectID": "posts/series-sm-course/boxplots.html#tldr-just-give-me-the-code",
    "href": "posts/series-sm-course/boxplots.html#tldr-just-give-me-the-code",
    "title": "Understanding boxplots",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Set up ----------------------------------------------------------------------\n\n# Set seed\nset.seed(20220906)\n\n# Generate some age variable for a university master programme\nage <- round(rnorm(1e3, mean = 26, sd = 2), 0)\n\n# Look at the boxplot ---------------------------------------------------------\nboxplot(age)\n\n# Boxplot with explanation\nC <- 1.5 # range multiplier\nboxplot(age, range = C)\n\n# Add arrows pointings to statistics\narrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,\n       x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,\n       length = 0.1)\n\n# Add labels of statistics\ntext(x = rep(.66, 5),\n     y = boxplot.stats(age, coef = C)$stats,\n     labels = c(\"lower whisker\",\n                \"1st quartile\",\n                \"median\",\n                \"3rd quartile\",\n                \"upper whisker\"),\n     adj = 1)\n\n# Add y axis labels\naxis(side = 2, at = boxplot.stats(age, coef = C)$stats[c(1, 3, 4)], labels = TRUE)\n\n# Compute boxplot statistics manually ------------------------------------------\n\n# Compute the median\nmed <- median(age)\n\n# Compute 1st and 3rd quartiles\nqnt <- quantile(age, probs = c(.25, .75))\n\n# Compute interquartile range\nIQR <- diff(qnt)[[1]]\n\n# Compute fences/whisker bounds\nC <- 1.5 # range multiplier\nfences <- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)\n\n# Put together the boxplot stats\nbxstats <- sort(c(med = med, qnt, f = fences))\n\n# Compute boxplot statistics with R function\nbxstats_auto <- boxplot.stats(age, coef = C)$stats\n\n# Compare results obtain manually and with the R function\ndata.frame(manual = bxstats, R.function = bxstats_auto)\n\n# Visualize the effect of different C -----------------------------------------\n\n# Allow two plots one next to the other\npar(mfrow = c(1, 2))\n\n# Plot C = 1.5 and 3\nlapply(c(1.5, 3.0), FUN = function (x){\n  C <- x\n  boxplot(age, range = C, main = paste0(\"C = \", C))\n\n  # Add arrows pointings to statistics\n  arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,\n         x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,\n         length = 0.1)\n  # Add labels of statistics\n  text(x = rep(.66, 5),\n       y = boxplot.stats(age, coef = C)$stats,\n       labels = c(paste(ifelse(C == 1.5, \"inner\", \"outer\"), \"fence \\n lower bound\"),\n                  \"1st quartile\",\n                  \"median\",\n                  \"3rd quartile\",\n                  paste(ifelse(C == 1.5, \"inner\", \"outer\"), \"fence \\n upper bound\")),\n       adj = 1)\n})"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html",
    "href": "posts/series-sm-course/se-resid.html",
    "title": "Understanding the residual standard error",
    "section": "",
    "text": "The residual standard error is a measure of fit for linear regression models. Conceptually, it can be thought of as the variability of the prediction error for a linear model. It is usually calculated as:\n\\[\nSE_{resid} = \\sqrt{ \\frac{ \\sum^{n}_{i = 1}(y_i - \\hat{y}_i)^2 }{df_{resid}} }\n\\]\nwhere:\n\n\\(n\\) is the sample size\n\\(k\\) is the number of parameters to estimate in the model\n\\(-1\\) is the degree of freedom lost to estimate the intercept\n\\(\\hat{y}_i\\) is the fitted \\(y\\) value for the \\(i\\)-th individual\n\\(df_{resid}\\) is the degrees of freedom of the residuals (\\(n - k - 1\\))\n\nThe smaller the residual standard error, the better the model fits the data."
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#learn-by-coding",
    "href": "posts/series-sm-course/se-resid.html#learn-by-coding",
    "title": "Understanding the residual standard error",
    "section": "Learn by coding",
    "text": "Learn by coding\nWe can compute the residual standard error manually after estimating a linear model in R. To get a better grasp of the residual standard error, let’s start by regressing the miles per gallon (mpg) on the number of cylinders (cyl), horsepower (hp), and weight (wt) of cars from the standard mtcars R dataset.\n\n# Fit a linear model -----------------------------------------------------------\n\n  lm_fit <- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\nWe can compute the residual standard error following the formula described above:\n\n# Compute the residual standard error manually ---------------------------------\n\n  # Define elements of the formula\n  n <- nrow(mtcars) # sample size\n  k <- 3            # number of parameters (regression coefficients)\n  yhat <- fitted(lm_fit) # fitted y values\n  y <- mtcars$mpg\n\n  # Compute rse\n  rse <- sqrt(sum((y - yhat)^2) / (n - k - 1))\n\n  # Print rse\n  rse\n\n[1] 2.511548\n\n\nWe can also extract it directly from any lm object:\n\n# residual standard error from lm output ---------------------------------------\n\n  # Use the sigma function to extract it from an lm object\n  sigma(lm_fit)\n\n[1] 2.511548\n\n  # Compare with the manual computation\n  sigma(lm_fit) - rse\n\n[1] 0"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#tldr-just-give-me-the-code",
    "href": "posts/series-sm-course/se-resid.html#tldr-just-give-me-the-code",
    "title": "Understanding the residual standard error",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Fit a linear model -----------------------------------------------------------\n\n  lm_fit <- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\n# Compute the residual standard error manually ---------------------------------\n\n  # Define elements of the formula\n  n <- nrow(mtcars) # sample size\n  k <- 3            # number of parameters (regression coefficients)\n  yhat <- fitted(lm_fit) # fitted y values\n  y <- mtcars$mpg\n\n  # Compute rse\n  rse <- sqrt(sum((y - yhat)^2) / (n - k - 1))\n\n  # Print rse\n  rse\n\n# residual standard error from lm output ---------------------------------------\n\n  # Use the sigma function to extract it from an lm object\n  sigma(lm_fit)\n\n  # Compare with the manual computation\n  sigma(lm_fit) - rse"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#other-resources",
    "href": "posts/series-sm-course/se-resid.html#other-resources",
    "title": "Understanding the residual standard error",
    "section": "Other resources",
    "text": "Other resources\n\nStatology: How to Interpret Residual Standard Error\nStatology: How to Calculate Residual Standard Error in R"
  },
  {
    "objectID": "posts/series-sm-course/qunatiles.html",
    "href": "posts/series-sm-course/qunatiles.html",
    "title": "Understanding quantiles",
    "section": "",
    "text": "TL;DR, just give me the code!\n\n# Set a seed\n\nset.seed(20220906)\n\n# Sample from a normal distribution\n\nage <- rnorm(1e5, mean = 27, sd = 2)\n\n# Define the 1st and 2nd quartile\n\nquartiles <- quantile(age, probs = c(.25, .75))\n\n# Plot density distribution\n\nplot(density(age),\n     main = \"Quartiles for the probability distribution of age\",\n     xlab = NA)\n\n# Costum x ticks\naxis(side = 1, at = c(27, round(quartiles, 1)), labels = TRUE)\n\n# Add points for quantiles\npoints(c(quartiles, median(age)),\n        y = rep(0, length(quartiles)+1))\npoints(c(quartiles, median(age)),\n       y = dnorm(c(quartiles, median(age)), mean = mean(age), sd = sd(age)))\n\n# Add segments to devide plot\nsegments(x0 = quartiles[1], y0 = 0,\n         x1 = quartiles[1], y1 = dnorm(quartiles[1],\n                                       mean = mean(age), sd = sd(age)))\nsegments(x0 = median(age), y0 = 0,\n         x1 = median(age), y1 = max(dnorm(age,\n                                       mean = mean(age), sd = sd(age))))\nsegments(x0 = quartiles[2], y0 = 0,\n         x1 = quartiles[2], y1 = dnorm(quartiles[2],\n                                       mean = mean(age), sd = sd(age)))\n\n# Add quartile labels\ntext(x = quartiles[1],\n     y = -.005,\n     \"1st quartile\")\ntext(x = quartiles[2],\n     y = -.005,\n     \"3rd quartile\")\n\n# Add percentage under the curve labels\ntext(x = c(24, 30, 26.3, 27.7),\n     y = c(.03, .03, .06, .06),\n     \"25 %\")"
  },
  {
    "objectID": "series-pca.html",
    "href": "series-pca.html",
    "title": "A primer on PCA",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis and SVD\n\n\nPCA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-sm-course.html",
    "href": "series-sm-course.html",
    "title": "Course: Statistics and Methodology",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a PhD researcher at Tilburg University working on multiple imputation algorithms for survey data."
  },
  {
    "objectID": "series-sampling.html",
    "href": "series-sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis and SVD\n\n\nPCA\n\n\n\n\nApr 22, 2022\n\n\nCross-entropy as a measure of predictive performance\n\n\nMachine Learning\n\n\n\n\nMar 14, 2022\n\n\nEstimating the weighted covariance matrix in R\n\n\nStatistics\n\n\n\n\nFeb 28, 2022\n\n\nEstimating ridge regression in R\n\n\nPenalty\n\n\n\n\nNov 17, 2021\n\n\nThe sweep operator\n\n\nThe EM algorithm,Matrix algebra,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-knowledge/ridge.html",
    "href": "posts/series-knowledge/ridge.html",
    "title": "Estimating ridge regression in R",
    "section": "",
    "text": "When there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance. This problem can be alleviated by imposing a size constraint (or penalty) on the coefficients. Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients values minimize a penalized residual sum of squares:\n\\[\n\\hat{\\beta}^{\\text{ridge}} = \\text{argmin}_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\right\\}\n\\]\nThe ridge solutions are not equivariant under scaling of the inputs. Therefore, it is recommended to standardize the inputs before solving the minimization problem.\nNotice that the intercept \\(\\beta_0\\) has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for \\(Y\\). Furthermore, by centering the predictors, we can separate the solution to the minimazion problem into two parts:\n\nIntercept \\[\n\\hat{\\beta}_0 = \\bar{y}=\\frac{1}{N}\\sum_{i = 1}^{N} y_i\n\\]\nPenalised regression coefficients \\[\n\\hat{\\beta}^{\\text{ridge}}=(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^Ty\n\\] which is the regular way of estimating regression coefficients with a penalty term (\\(\\lambda\\)) added on the diagonal (\\(\\mathbf{I}\\)) of the cross-product matrix (\\(\\mathbf{X}^T\\mathbf{X}\\)) to make it invertible (\\((...)^{-1}\\))."
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "href": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "title": "Estimating ridge regression in R",
    "section": "Fitting ridge regression manually",
    "text": "Fitting ridge regression manually\nFirst, let’s make sure the predictors are centered on the mean and scaled to have a variance of 1.\n\n# Fitting ridge regression manually --------------------------------------------\n\n# Scale the data (standardize)\nX_scale <- scale(X, center = TRUE, scale = TRUE)\n\nThen, we want to fit the ridge regression manually by separating the intercept and the regression coefficients estimation (two-step approach):\n\nEstimate the intercept (\\(\\hat{\\beta}_0\\))\n::: {.cell}\n# Estimate the intercept\nb0_hat_r <- mean(y)\n:::\nEstimate the ridge regression coefficients (\\(\\hat{\\beta}^{\\text{ridge}}\\)).\n\n\nCompute the cross-product matrix of the predictors.\nThis is the same step we would take if we wanted to compute the OLS estimates.\n::: {.cell}\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale) %*% X_scale\n:::\nDefine a value of \\(\\lambda\\).\nThis value is usually chosen by cross-validation from a grid of possible values. However, here we are only interested in how \\(\\lambda\\) is used in the computation, so we can simply give it a fixed value.\n::: {.cell}\n# Define a lambda value\nlambda <- .1\n:::\nCompute \\(\\hat{\\beta}^{\\text{ridge}}\\).\n::: {.cell}\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r <- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y\n:::\nwhere diag(p) is the identity matrix \\(\\mathbf{I}\\).\n\nFinally, let’s print the results:\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r)),\n  3\n)\n\n    twostep\nb0   20.091\nb1   -0.194\nb2    1.366\nb3   -1.373\nb4    0.438\nb5   -3.389\nb6    1.361\nb7    0.162\nb8    1.243\nb9    0.496\nb10  -0.460\n\n\nIt is important to note the effect of centering and scaling. When fitting ridge regression, many sources recommend centering the data. This allows separating the estimation of the intercept from the estimation of the regression coefficients. As a result, only the regression coefficients are penalised. To understand the effect of centering, consider what happens in regular OLS estimation when predictors are centered:\n\n# Centering in regular OLS -----------------------------------------------------\n\n# Create a version of X that is centered\nX_center <- scale(X, center = TRUE, scale = FALSE)\n\n# Fit an regular linear model\nlm_ols <- lm(y ~ X_center)\n\n# Check that b0 is equal to the mean of y\ncoef(lm_ols)[\"(Intercept)\"] - mean(y)\n\n  (Intercept) \n-3.552714e-15 \n\n\nFurthermore, let’s see what would have happened if we had penalised the intercept as well.\n\n# Consequence of penalising the intercept --------------------------------------\n\n# Add a vector of 1s to penalise the intercept\nX_scale_w1 <- cbind(1, X_scale)\n\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale_w1) %*% X_scale_w1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r_w1 <- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r),\n             onestep = c(b0 = bs_hat_r_w1[1],\n                         b = bs_hat_r_w1[-1])),\n  3\n)\n\n    twostep onestep\nb0   20.091  20.028\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460\n\n\nAs you see, the intercept would be shrunk toward zero, without any benefit. As a result, any prediction would also be offset by the same amount.\n\nAn alternative way to avoid penalising the intercept\nIt can be handy to obtain estimates of the regression coefficients and intercept in one step. We can use matrix algebra and R to simplify the two-step procedure to a single step. In particular, we can avoid the penalisation of the intercept by setting to 0 the first element of the “penalty” matrix lambda * diag(p + 1).\n\n# Alternative to avoid penalization of the intercept ---------------------------\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_w1)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\n\n# replace first element with 0\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r2 <- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)\n\n# Compare\nround(\n        data.frame(\n                twostep = c(b0 = b0_hat_r, b = bs_hat_r),\n                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])\n        ),\n        3\n)\n\n    twostep onestep\nb0   20.091  20.091\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460"
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "href": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "title": "Estimating ridge regression in R",
    "section": "Fit ridge regression with glmnet",
    "text": "Fit ridge regression with glmnet\nThe most popular R package to fit regularised regression is glmnet. Let’s see how we can replicate the results we obtained with the manual approach with glmnet. There are three important differences to consider:\n\nglmnet uses the biased sample variance estimate when scaling the predictors;\nglmnet returns the unstandardized regression coefficients;\nglmnet uses a different parametrization for \\(\\lambda\\).\n\nTo obtain the same results with the manual approach and glmnet we need to account for these.\n\nUse the biased estimation of variance\nFirst, let’s use the biased sample variance estimate in computing \\(\\hat{\\beta}^{\\text{ridge}}\\) with the manual approach:\n\n# Fitting ridge manually with biased variance estimation -----------------------\n\n# Standardize X\nX_scale <- sapply(1:p, function (j){\n  muj <- mean(X[, j])                  # mean\n  sj <- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd\n  (X[, j] - muj) / sj                  # center and scale\n})\n\n# Craete the desing matrix\nX_scale_dm <- cbind(1, X_scale)\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_dm)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r3 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)\n\n# Print results\nround(\n      data.frame(\n              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])\n      ),\n      3\n)\n\n    manual\nb0  20.091\nb1  -0.191\nb2   1.353\nb3  -1.354\nb4   0.430\nb5  -3.343\nb6   1.343\nb7   0.159\nb8   1.224\nb9   0.488\nb10 -0.449\n\n\n\n\nReturn the unstandardized coefficients\nNext, we need to revert these regression coefficients to their original scale. Since we are estimating the regression coefficients on the scaled data, they are computed on the standardized scale.\n\n# Return the  unstandardized coefficients --------------------------------------\n\n# Extract the original mean and standard deviations of all X variables\nmean_x <- colMeans(X)\nsd_x <- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version\n\n# Revert to original scale\nbs_hat_r4 <- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),\n               bs_hat_r3[-1] / sd_x)\n\n# Compare manual standardized and unstandardized results\nround(\n      data.frame(\n              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),\n              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])\n      ),\n      3\n)\n\n    standardized unstandardized\nb0        20.091         12.908\nb1        -0.191         -0.109\nb2         1.353          0.011\nb3        -1.354         -0.020\nb4         0.430          0.818\nb5        -3.343         -3.471\nb6         1.343          0.764\nb7         0.159          0.320\nb8         1.224          2.491\nb9         0.488          0.672\nb10       -0.449         -0.282\n\n\n\n\nAdjust the parametrization of \\(\\lambda\\) for glmnet\nNext, we need to understand the relationship between the \\(\\lambda\\) parametrization we used and the one used by glmnet. The following code shows that if we want to use a given value of lambda in glmnet we need to multiply it by the standard deviation of the dependent variable (sd_y) and divide it by the sample size (n).\n\n# Adjust the parametrization of lambda -----------------------------------------\n\n# Extract the original mean and standard deviations of y (for lambda parametrization)\nmean_y <- mean(y)\nsd_y <- sqrt(var(y) * (n - 1) / n)\n\n# Compute the value glmnet wants for your target lambda\nlambda_glmnet <- sd_y * lambda / n\n\n\n\nCompare manual and glmnet ridge regression output\nFinally, we can compare the results:\n\n# Fitting ridge regression with glmnet -----------------------------------------\n\n# Fit glmnet\nfit_glmnet_s <- glmnet(x = X,\n                       y = y,\n                       alpha = 0,\n                       lambda = lambda_glmnet, # correction for how penalty is used\n                       thresh = 1e-20)\n\nbs_glmnet <- coef(fit_glmnet_s)\n\n# Compare estimated coefficients\nround(\n      data.frame(\n        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),\n        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])\n      ),\n      3\n)\n\n       manual glmnet\nb0     12.908 12.908\nb.cyl  -0.109 -0.109\nb.disp  0.011  0.011\nb.hp   -0.020 -0.020\nb.drat  0.818  0.818\nb.wt   -3.471 -3.471\nb.qsec  0.764  0.764\nb.vs    0.320  0.320\nb.am    2.491  2.491\nb.gear  0.672  0.672\nb.carb -0.282 -0.282"
  }
]