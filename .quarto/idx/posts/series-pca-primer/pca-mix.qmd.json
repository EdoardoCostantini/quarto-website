{"title":"PCA with metrics, dimensionality reduction through PCA and MCA","markdown":{"yaml":{"draft":true,"title":"PCA with metrics, dimensionality reduction through PCA and MCA","author":"Edoardo Costantini","date":"2022-05-17","slug":"pca-mix","categories":["Categorical data"],"bibliography":"../../resources/bibshelf.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n**Principal Component Analysis** (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{Z}$, where $r < p$.\nThe columns of $\\mathbf{Z}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{Z} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of coefficients, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{Z}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{Z}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nThe PCs scores are given by the $n \\times r$ matrix $\\mathbf{UD}$, and the weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}$.\n\n**Multiple Correspondence Analysis** (MCA) is generally regarded as an equivalent tool that applies to discrete data.\nChavent et al. [-@chaventEtAl:2014] have shown how using weights on rows and columns of the input data matrix can define a general PCA framework that includes standard PCA and MCA as special cases.\nThis approach is often referred to as **PCA with metrics**, as metrics are used to introduce the weights.\nIn this post, I want to show how PCA and MCA are related through this framework.\n\n## Generalised Singular Value Decomposition\n\nConsider an $n \\times p$ matrix of input variables $\\mathbf{X}$ with row metric $\\mathbf{N}$ and column metric $\\mathbf{M}$.\nThe Generalized Singular Value Decomposition of $\\mathbf{X}$ can be written as:\n$$\n\\mathbf{X} = \\mathbf{U \\Lambda V}^T\n$$\nwhere:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XMX}^T\\mathbf{N}$ and $\\mathbf{X}^T\\mathbf{NXM}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XMX}^T\\mathbf{N}$ such that $\\mathbf{U}^T\\mathbf{MU=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{MV=I}$;\n\nThe GSVD of $\\mathbf{X}$ can be obtained by taking \n\n- first taking the standard SVD of the transformed matrix $\\tilde{\\mathbf{X}} = \\mathbf{N}^{1/2}\\mathbf{X}\\mathbf{M}^{1/2}$ which gives:\n$$\n\\tilde{\\mathbf{X}} = \\tilde{\\mathbf{U}}\\tilde{\\mathbf{\\Lambda}}\\tilde{\\mathbf{V}}^T\n$$\n- and then transforming each element back to the original scale\n$$\n\\mathbf{\\Lambda} = \\tilde{\\mathbf{\\Lambda}}\n$$\n$$\n\\mathbf{U} = \\mathbf{N}^{-1/2}\\tilde{\\mathbf{U}}\n$$\n$$\n\\mathbf{V} = \\mathbf{M}^{-1/2}\\tilde{\\mathbf{V}}\n$$\n\n\n## Relationship of GSVD to standard SVD\n\nIt's easy to see how this GSVD differs from the standard formulation of SVD simply by the presence of the metrics $\\mathbf{N}$ and $\\mathbf{M}$.\nAs you can see [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition), in the standard formulation of SVD:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XX}^T$ and $\\mathbf{X}^T\\mathbf{X}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XX}^T$ such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\n## PCA and MCA as special cases of GSVD\n\nThe solutions for both PCA and MCA can be obtained as special cases of the GSVD approach described by setting $\\mathbf{X}$ equal to a preprocessed version of the original data $\\mathbf{X}$ and using $\\mathbf{N}$ and $\\mathbf{M}$ to appropriately weight the rows and columns.\n\n### PCA\n\nThe input data for standard PCA is the $n \\times p$ matrix $\\mathbf{X}$ of $n$ rows (observations) described by $p$ numerical variables.\nThe columns of this matrix are usually centered and standardized.\nThe GSVD can be used to find the solution to PCA by setting $\\mathbf{X}$ equal to the centered and standardized version of $\\mathbf{X}$ and weighting:\n\n- its rows by $1/n$, which is obtained by setting $\\mathbf{N} = \\frac{1}{n}I_n$\n- its columns by $1$, which is obtained by setting $\\mathbf{M} = I_p$.\nThis metric indicates that the distance between two observations is the standard euclidean distance between two rows of $\\mathbf{X}$\n\nBy setting these values for the metrics, it is easy to see how the GSVD of $\\mathbf{X}$ reduces to the standard SVD of $\\mathbf{X}$.\n\n### MCA\n\nFor an $n \\times p$ data matrix $\\mathbf{X}$ with $n$ observations (rows) and $p$ discrete predictors (columns).\nEach $j = 1, \\dots, p$ discrete variable has $k_j$ possible values.\nThe sum of the $p$ $k_j$ values is $k$. \n$\\mathbf{X}$ is preprocessed by coding each level of the discrete items as binary variables describing whether each observation takes a specific value for every discrete variable.\nThis results in an $n \\times k$ [complete disjunctive table](https://www.xlstat.com/en/solutions/features/complete-disjuncive-tables-creating-dummy-variables) $\\mathbf{G}$, sometimes also referred to as an indicator matrix.\n\nMCA is usually obtained by applying Correspondence Analysis to $\\mathbf{G}$, which means applying standard PCA to the matrices of the row profiles and the column profiles.\nIn particular, for the goal of obtaining a lower-dimensional representation of $\\mathbf{X}$ we are interested in the standard PCA of the row profiles.\nWithin the framework of PCA with metrics, MCA can be obtained by first setting:\n\n- $\\mathbf{X}$ to the centered $\\mathbf{G}$\n- $\\mathbf{N} = \\frac{1}{n}I_n$\n- $\\mathbf{M} = \\text{diag}(\\frac{n}{n_s}, s = 1, \\dots, k)$\n\nThe coordinates of the observations (the principal component scores) can be obtained by applying the GSVD of $\\mathbf{X}$ with the given metrics.\n\n# Learn by coding\n\nTo understand the relationship between SVD, GSVD, PCA, and MCA we will need two packages:\n\n```{r}\n# Load Packages\nlibrary(\"FactoMineR\") # for analysis\nlibrary(\"factoextra\") # for visualizarion1\n```\n\n## MCA\n\n### A standard MCA analysis according to FactoMineR\n\nWe will start by performing the MCA analysis.\nLet's start by loading, preparing, and summarising the data:\n\n```{r}\n# Data Prep ---------------------------------------------------------------\n\n# Load data\ndata(\"poison\")\nhead(poison[, 1:7], 3) # survey style data\n\n# Subset active individuals and variables\npoison.active <- poison[1:55, 5:15]\nhead(poison.active[, 1:6], 3)\n\n# Summaries\nstr(poison.active)\nfor (i in 1:4) {\n    plot(poison.active[, i],\n        main = colnames(poison.active)[i],\n        ylab = \"Count\", col = \"steelblue\", las = 2\n    )\n}\n\n```\n\nWe can then use the `MCA()` function from the `FactorMineR` package to perform a standard MCA analysis of the data.\n\n```{r}\n# The analysis ------------------------------------------------------------\nres.mca <- FactoMineR::MCA(\n    X = poison.active,\n    ncp = 5,\n    graph = TRUE\n)\n\n```\n\nWe can now visualize different aspects of the analysis.\n\nWe extract the eigenvalues\n\n```{r}\n# Visualization -----------------------------------------------------------\n\n# Eigenvalues / Variances\neig.val <- get_eigenvalue(res.mca)\neig.val # proportion of variances retained by dimensions\n```\n\nPercentages of inertia explained by MCA\n\n```{r}\n# Percentages of Inertia explained by MCA\nfviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 45))\n```\n\nBiplots\n\n```{r}\n# Biplot\nfviz_mca_biplot(res.mca,\n    repel = TRUE, # avoid text overlapping\n    ggtheme = theme_minimal()\n)\n# Rows (individuals) are represented by blue points;\n# Columns (variable categories) by red triangles.\n```\n\nGraphs of variables\n\n```{r}\n\n# Graph of variables\nvar <- get_mca_var(res.mca)\nvar\n# Coordinates\nhead(var$coord)\n# Cos2: quality on the factore map\nhead(var$cos2)\n# Contributions to the principal components\nhead(var$contrib)\n\n```\n\nGraphs for individuals\n\n```{r}\n# Graph of individuals\nind <- get_mca_ind(res.mca) # extract the results for individuals\nind\n# Coordinates of column points\nhead(ind$coord)\n# Quality of representation\nhead(ind$cos2)\n# Contributions\nhead(ind$contrib)\n\n# BIplot for individuals only (no vars)\nfviz_mca_ind(res.mca,\n    col.ind = \"cos2\",\n    gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n    repel = TRUE, ggtheme = theme_minimal()\n)\nfviz_mca_ind(res.mca,\n    label = \"none\",\n    habillage = \"Vomiting\", # color by groups defined by variable\n    palette = c(\"#00AFBB\", \"#E7B800\"),\n    addEllipses = TRUE, ellipse.type = \"confidence\",\n    repel = TRUE, ggtheme = theme_minimal()\n)\n\n```\n\nEllipses\n\n```{r}\n\n# More than 1 grouping variable\nfviz_ellipses(res.mca, c(\"Vomiting\", \"Fever\"),\n    geom = \"point\"\n)\n\n```\n\nBar plot for Cos2 of individuals:\n\n```{r}\n# Bar plot for Cos2 of individuals\nfviz_cos2(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n```\n\nContribution of individuals to the dimensions \n\n```{r}\n# Contribution of individuals to the dimensions\nfviz_contrib(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n```\n\n### Perfomring MCA by hand\n\nLet's work with wine data:\n\n```{r}\n# Data\ndata(wine)\n\n```\n\nLet's keep only the variables that are facotrs\n\n```{r}\n# Keep factors\nMCA.dt <- wine[, sapply(wine, is.factor)]\n\n# Look at data\nMCA.dt\n\n```\n\nThe two variables involved have these number of levels:\n```{r}\n# Count levels\nsapply(MCA.dt, nlevels)\n\n```\n\nLet's now perform a standard MCA analysis with FactoMineR. Let's say we are aiming for extracting 4 components:\n\n```{r}\n# Define goal number of PCs\nnpcs <- 4\n\n# Perform MCA\nres.mca <- FactoMineR::MCA(\n    X = MCA.dt,\n    ncp = npcs,\n    graph = FALSE\n)\n\n```\n\nWe can now look at the coordinates of the individuals on the `npcs` dimensions:\n\n```{r}\n# Coordinates of the individuals on the dimensions\nres.mca$ind$coord\n\n```\n\nWe can also look at the right singular vectors of the data:\n\n```{r}\n# right singular vectors of data\nres.mca$svd$V\n\n```\n\nNow we can replicate the computations involved following a description of MCA I found in AudigierEtAl2016_MICAT.\nFirst, we need to extract a few objects we will need.\n\n```{r}\n# Create required processing objects\nI <- nrow(MCA.dt)             # numebr of individuals\nK <- ncol(MCA.dt)             # number of categorical predictors\nqk <- sapply(MCA.dt, nlevels) # number of levels per categorical variable\nJ <- sum(qk)                  # total number of categories\n\n```\n\nThen, we can create the disjonctive table:\n\n```{r}\n# Disjunctive table\nZ <- tab.disjonctif(MCA.dt)\n\n# And look at it\nZ\n\n```\n\nNotice the relationship of the disjunctive table with a standard contingency table:\n\n```{r}\n# Relationship between disjunctive table and contingency table\nN <- t(Z[, 1:3]) %*% Z[, 4:7]\nN - table(MCA.dt)\n\n```\n\nWe can compute the weighting matrices that are used to perform MCA.\nFirst, we want to compute the proportion of individuals taking a category value for each variable, which is known as a distance matrix:\n\n```{r}\n# Distance Metric Matrix\npxkqk <- colSums(Z) / I\nD_Sigma <- diag(pxkqk)\n\n```\n\nThen we want to compute the individual weight matrix:\n\n```{r}\n# Weight Matrix\nW_mat <- diag(rep(1, I))/I\n\n```\n\nand finally, we want to compute the center matrix:\n\n```{r}\n# M matrix (center matrix)\nM <- matrix((rep(colMeans(Z), nrow(Z))),\n    nrow = nrow(Z),\n    byrow = TRUE\n)\n\n```\n\nWe can now take the SVD of a centered version of the disjunctive table with an SVD triplet function form `FactoMineR`:\n\n```{r}\n# SVD of triplet (Z-M, D_Sigma, W_mat)\nSVD.trip <- FactoMineR::svd.triplet(\n    X = Z - M,\n    row.w = diag(W_mat),\n    col.w = diag(1 / K * solve(D_Sigma)),\n    ncp = npcs\n)\n\n```\n\nWe can also take the SVD of the same centered matrix using a thes standard `svd` R function:\n\n```{r}\n# Manual SVD triplet\nSVD.man <- svd(sqrt(W_mat) %*% (Z - M) %*% sqrt(1 / K * solve(D_Sigma)))\n\n```\n\nWhen computing the SVD in this manual way, we need to convert the scale according to Chaven 2017 p. 3\n\n```{r}\n# Convert back to correct scales (according to Chaven 2017 p. 3)\nV.man <- (solve(sqrt(1 / K * solve(D_Sigma))) %*% SVD.man$v)[, 1:npcs]\nU.man <- (solve(sqrt(W_mat)) %*% SVD.man$u)[, 1:npcs]\nL.man <- SVD.man$d[1:npcs]\n\n```\n\nDoing so, you will see that the computation obtained by `FactoMineR` approach is the same as the one done by hand\n\n```{r}\n# Compare SVD triplet and manual SVD of weighted matrix\nround(abs(SVD.trip$V) - abs(V.man), 5)\nround(abs(SVD.trip$U) - abs(U.man), 5)\nround(abs(SVD.trip$vs[1:npcs]) - abs(L.man), 5)\n  \n```\n\n```{r mca, warning = FALSE, message = FALSE}\n\n  # Reconstruction Formula\n  d_hat <- SVD.trip$vs[1:npcs] # matrix of the singular values \n                            # (Squared would be eigenvalues of Z)\n  u_hat <- SVD.trip$U # Left singular vectors matrix\n  v_hat <- SVD.trip$V # Right singular vectors matrix\n  \n  z_hat <- u_hat %*% diag(d_hat) %*% t(v_hat) + M\n  colSums(z_hat)\n  colSums(Z)\n  \n  # Compare SVD matrices\n  # Matrix of singular values\n  res.mca$svd$vs[1:npcs] -\n    d_hat[1:npcs]\n\n  res.mca$svd$vs[1:npcs] -\n    L.man\n  \n  # Left Singular Vectors Matrix\n  round(\n    res.mca$svd$U - u_hat,\n    3\n  )\n\n  round(\n    res.mca$svd$U - U.man,\n    3\n  )\n\n  # Right Singular Vectors Matrix\n  round(\n    res.mca$svd$V -\n      v_hat, \n    3)\n  # Correlation between columns\n  # And look into the PCAmixdata package\n  round(cor(v_hat, res.mca$svd$V), 1)\n  \n  # Coordinates on Dimensions are recovered\n  round(\n    res.mca$ind$coord -\n      u_hat %*% diag(d_hat),\n    3\n  )\n\n# Following JosseHusson2016 -----------------------------------------------\n\n  I <- nrow(MCA.dt)\n  J <- ncol(MCA.dt)\n  X <- tab.disjonctif(MCA.dt)\n  rowMarg <- rowSums(X) # = J\n  colMarg <- colSums(X) # = number of ids in a category\n  D_Sigma <- diag(colMarg)\n  D <- 1/I * diag(rep(1, I)) # rowMasses\n  SVD.trip <- svd.triplet(X = diag(rep(1, I)) %*% X %*% solve(D_Sigma),\n                          row.w = diag( D ),\n                          col.w = diag( 1/(I*J)*D_Sigma ),\n                          ncp=2\n                          )\n  svd(I * X %*% solve(D_Sigma))\n  \n  SVD.trip$vs\n  round(SVD.trip$vs[2:3] - res.mca$svd$vs[1:2], 3)\n  \n  SVD.trip$U\n  res.mca$svd$U\n#   round(SVD.trip$U[, 2:3] - res.mca$svd$U, 3)\n  res.mca$svd$U\n  \n#   SVD.trip$V\n#   round(SVD.trip$V[, 2:3] - res.mca$svd$V, 3)\n#   res.mca$svd$V\n\n```\n\n```{r pca-mix}\n# Prepare environment ----------------------------------------------------------\n\nlibrary(psych)\nlibrary(PCAmixdata)\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n\ndata(tea)\n\nhead(tea)\n\nlapply(tea[, c(\"where\", \"how\", \"SPC\")], nlevels)\n\nsapply(tea, nlevels)\n\nx <- tea[, c(\"where\", \"how\", \"Tea\")]\n\nCTD <- tab.disjonctif(x)\npk <- colMeans(CTD)\n\nCTD[1, ] / pk\nCTD[4, ] / pk\n1/.64\n1/.56666667\n\nCTD_t <- t(apply(CTD, 1, function(r) {t(r)/pk} ))\ncolMeans(CTD_t)\n\n\nN <- tab.disjonctif(x)\n1/nrow(tea)\n\n# Correspondance analysis based on contingency table ---------------------------\nx <- tea[, c(\"SPC\", \"where\")]\nn <- nrow(x)\nr <- nlevels(x[, 1]) \nC <- nlevels(x[, 2]) \nN <- table(x)\n\n# Contingency table\nN\n\n# Indicator matrix\nZ <- tab.disjonctif(x)\nZ1 <- Z[, 1:r]\nZ2 <- Z[, -c(1:r)]\nN - t(Z1) %*% Z2\n\n# Correspondance matrix\nP <- 1/n * N\n\n# From Greenacre1984\nN\n\n# Column and row sums\nr_bold <- rowSums(N)\nc_bold <- colSums(N)\n\ndrop(N %*% rep(1, ncol(N))) - r_bold\ndrop(t(N) %*% rep(1, nrow(N))) - c_bold\n\nD_r <- diag(r_bold)\nD_c <- diag(c_bold)\n\n# Matrices of profiles\n\nR <- solve(D_r) %*% P\nC <- solve(D_c) %*% t(P)\n\n# Centroids\nr <- t(C) %*% c_bold\nc <- t(R) %*% r_bold\n\n# Generalized SVD of P - r_bold t(c_bold)\n\nP - r_bold %*% t(c_bold)\nA <- svd(P - r %*% t(c))$u\nB <- svd(P - r %*% t(c))$v\n\nt(A) %*% solve(D_r) %*% A\nt(B) %*% solve(D_c) %*% B\n\n# From Jolliffe p. 37\n# r_bold <- rep(1, r)\n# c_bold <- rep(1, C)\n# D_r <- diag(r_bold)\n# D_c <- diag(c_bold)\n# Omega <- solve(D_r)\n# Psi <- solve(D_c)\n# X <- P - r_bold %*% t(c_bold)\n\n# V <- svd(X)$u\n# M <- diag(svd(X)$d)\n# B <- svd(X)$v\n\n# round(t(V) %*% Omega %*% V, 3)\n# round(t(B) %*% Psi %*% B, 3)\n\n# X_til <- sqrt(Omega) %*% X %*% sqrt(Psi)\n\n# W <- svd(X_til)$u\n# K <- diag(svd(X_til)$d)\n# C <- svd(X_til)$v\n\n# solve(sqrt(Omega)) %*% W - V\n# solve(sqrt(Omega)) %*% C - B\n\n# W %*% K\n\n# # Row profiles\n# D_r\n\n# MCA based on Audigier et al 2017 p. 505 (p. 5 of pdf) ------------------------\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define row weights\n    I <- nrow(x)\n    R <- diag(1 / I, I)\n\n    # Define Z (the disjunctive table)\n    Z <- tab.disjonctif(x)\n\n    # Define column weights\n    K <- ncol(x)\n    plxk <- colMeans(Z)\n    D_sigma <- diag(plxk)\n    1 / K * solve(D_sigma)\n\n    # Define M\n    M <- matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n\n    # Centered matrix?\n    Z - M\n\n# MCA based on Chavent Et Al 2017 et al 2017 p. 505 (p. 5 of pdf) -----------1  ---\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define Z (the disjunctive table)\n    G <- tab.disjonctif(x)\n\n    # Define row weights\n    n <- nrow(x)\n    N <- diag(1 / n, n)\n\n    # Define column weights\n    M <- diag(n / colSums(G))\n    solve(D_sigma)\n\n    # Create Z, the centered G?\n    plxk <- colMeans(G)\n    G - matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n    Z <- t(t(G) - plxk)\n\n    # SVD of Z\n    u_til <- svd(Z)$u\n    lambda_til <- svd(Z)$d\n\n    # Principal Compoent Scores (factor coordinates of the rows)\n    u_til[, 1:3] %*% diag(lambda_til[1:3])\n```\n\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n","srcMarkdownNoYaml":"\n\n# Introduction\n\n**Principal Component Analysis** (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{Z}$, where $r < p$.\nThe columns of $\\mathbf{Z}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{Z} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of coefficients, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{Z}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{Z}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nThe PCs scores are given by the $n \\times r$ matrix $\\mathbf{UD}$, and the weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}$.\n\n**Multiple Correspondence Analysis** (MCA) is generally regarded as an equivalent tool that applies to discrete data.\nChavent et al. [-@chaventEtAl:2014] have shown how using weights on rows and columns of the input data matrix can define a general PCA framework that includes standard PCA and MCA as special cases.\nThis approach is often referred to as **PCA with metrics**, as metrics are used to introduce the weights.\nIn this post, I want to show how PCA and MCA are related through this framework.\n\n## Generalised Singular Value Decomposition\n\nConsider an $n \\times p$ matrix of input variables $\\mathbf{X}$ with row metric $\\mathbf{N}$ and column metric $\\mathbf{M}$.\nThe Generalized Singular Value Decomposition of $\\mathbf{X}$ can be written as:\n$$\n\\mathbf{X} = \\mathbf{U \\Lambda V}^T\n$$\nwhere:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XMX}^T\\mathbf{N}$ and $\\mathbf{X}^T\\mathbf{NXM}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XMX}^T\\mathbf{N}$ such that $\\mathbf{U}^T\\mathbf{MU=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{MV=I}$;\n\nThe GSVD of $\\mathbf{X}$ can be obtained by taking \n\n- first taking the standard SVD of the transformed matrix $\\tilde{\\mathbf{X}} = \\mathbf{N}^{1/2}\\mathbf{X}\\mathbf{M}^{1/2}$ which gives:\n$$\n\\tilde{\\mathbf{X}} = \\tilde{\\mathbf{U}}\\tilde{\\mathbf{\\Lambda}}\\tilde{\\mathbf{V}}^T\n$$\n- and then transforming each element back to the original scale\n$$\n\\mathbf{\\Lambda} = \\tilde{\\mathbf{\\Lambda}}\n$$\n$$\n\\mathbf{U} = \\mathbf{N}^{-1/2}\\tilde{\\mathbf{U}}\n$$\n$$\n\\mathbf{V} = \\mathbf{M}^{-1/2}\\tilde{\\mathbf{V}}\n$$\n\n\n## Relationship of GSVD to standard SVD\n\nIt's easy to see how this GSVD differs from the standard formulation of SVD simply by the presence of the metrics $\\mathbf{N}$ and $\\mathbf{M}$.\nAs you can see [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition), in the standard formulation of SVD:\n\n- $\\mathbf{\\Lambda}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XX}^T$ and $\\mathbf{X}^T\\mathbf{X}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XX}^T$ such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\n## PCA and MCA as special cases of GSVD\n\nThe solutions for both PCA and MCA can be obtained as special cases of the GSVD approach described by setting $\\mathbf{X}$ equal to a preprocessed version of the original data $\\mathbf{X}$ and using $\\mathbf{N}$ and $\\mathbf{M}$ to appropriately weight the rows and columns.\n\n### PCA\n\nThe input data for standard PCA is the $n \\times p$ matrix $\\mathbf{X}$ of $n$ rows (observations) described by $p$ numerical variables.\nThe columns of this matrix are usually centered and standardized.\nThe GSVD can be used to find the solution to PCA by setting $\\mathbf{X}$ equal to the centered and standardized version of $\\mathbf{X}$ and weighting:\n\n- its rows by $1/n$, which is obtained by setting $\\mathbf{N} = \\frac{1}{n}I_n$\n- its columns by $1$, which is obtained by setting $\\mathbf{M} = I_p$.\nThis metric indicates that the distance between two observations is the standard euclidean distance between two rows of $\\mathbf{X}$\n\nBy setting these values for the metrics, it is easy to see how the GSVD of $\\mathbf{X}$ reduces to the standard SVD of $\\mathbf{X}$.\n\n### MCA\n\nFor an $n \\times p$ data matrix $\\mathbf{X}$ with $n$ observations (rows) and $p$ discrete predictors (columns).\nEach $j = 1, \\dots, p$ discrete variable has $k_j$ possible values.\nThe sum of the $p$ $k_j$ values is $k$. \n$\\mathbf{X}$ is preprocessed by coding each level of the discrete items as binary variables describing whether each observation takes a specific value for every discrete variable.\nThis results in an $n \\times k$ [complete disjunctive table](https://www.xlstat.com/en/solutions/features/complete-disjuncive-tables-creating-dummy-variables) $\\mathbf{G}$, sometimes also referred to as an indicator matrix.\n\nMCA is usually obtained by applying Correspondence Analysis to $\\mathbf{G}$, which means applying standard PCA to the matrices of the row profiles and the column profiles.\nIn particular, for the goal of obtaining a lower-dimensional representation of $\\mathbf{X}$ we are interested in the standard PCA of the row profiles.\nWithin the framework of PCA with metrics, MCA can be obtained by first setting:\n\n- $\\mathbf{X}$ to the centered $\\mathbf{G}$\n- $\\mathbf{N} = \\frac{1}{n}I_n$\n- $\\mathbf{M} = \\text{diag}(\\frac{n}{n_s}, s = 1, \\dots, k)$\n\nThe coordinates of the observations (the principal component scores) can be obtained by applying the GSVD of $\\mathbf{X}$ with the given metrics.\n\n# Learn by coding\n\nTo understand the relationship between SVD, GSVD, PCA, and MCA we will need two packages:\n\n```{r}\n# Load Packages\nlibrary(\"FactoMineR\") # for analysis\nlibrary(\"factoextra\") # for visualizarion1\n```\n\n## MCA\n\n### A standard MCA analysis according to FactoMineR\n\nWe will start by performing the MCA analysis.\nLet's start by loading, preparing, and summarising the data:\n\n```{r}\n# Data Prep ---------------------------------------------------------------\n\n# Load data\ndata(\"poison\")\nhead(poison[, 1:7], 3) # survey style data\n\n# Subset active individuals and variables\npoison.active <- poison[1:55, 5:15]\nhead(poison.active[, 1:6], 3)\n\n# Summaries\nstr(poison.active)\nfor (i in 1:4) {\n    plot(poison.active[, i],\n        main = colnames(poison.active)[i],\n        ylab = \"Count\", col = \"steelblue\", las = 2\n    )\n}\n\n```\n\nWe can then use the `MCA()` function from the `FactorMineR` package to perform a standard MCA analysis of the data.\n\n```{r}\n# The analysis ------------------------------------------------------------\nres.mca <- FactoMineR::MCA(\n    X = poison.active,\n    ncp = 5,\n    graph = TRUE\n)\n\n```\n\nWe can now visualize different aspects of the analysis.\n\nWe extract the eigenvalues\n\n```{r}\n# Visualization -----------------------------------------------------------\n\n# Eigenvalues / Variances\neig.val <- get_eigenvalue(res.mca)\neig.val # proportion of variances retained by dimensions\n```\n\nPercentages of inertia explained by MCA\n\n```{r}\n# Percentages of Inertia explained by MCA\nfviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 45))\n```\n\nBiplots\n\n```{r}\n# Biplot\nfviz_mca_biplot(res.mca,\n    repel = TRUE, # avoid text overlapping\n    ggtheme = theme_minimal()\n)\n# Rows (individuals) are represented by blue points;\n# Columns (variable categories) by red triangles.\n```\n\nGraphs of variables\n\n```{r}\n\n# Graph of variables\nvar <- get_mca_var(res.mca)\nvar\n# Coordinates\nhead(var$coord)\n# Cos2: quality on the factore map\nhead(var$cos2)\n# Contributions to the principal components\nhead(var$contrib)\n\n```\n\nGraphs for individuals\n\n```{r}\n# Graph of individuals\nind <- get_mca_ind(res.mca) # extract the results for individuals\nind\n# Coordinates of column points\nhead(ind$coord)\n# Quality of representation\nhead(ind$cos2)\n# Contributions\nhead(ind$contrib)\n\n# BIplot for individuals only (no vars)\nfviz_mca_ind(res.mca,\n    col.ind = \"cos2\",\n    gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n    repel = TRUE, ggtheme = theme_minimal()\n)\nfviz_mca_ind(res.mca,\n    label = \"none\",\n    habillage = \"Vomiting\", # color by groups defined by variable\n    palette = c(\"#00AFBB\", \"#E7B800\"),\n    addEllipses = TRUE, ellipse.type = \"confidence\",\n    repel = TRUE, ggtheme = theme_minimal()\n)\n\n```\n\nEllipses\n\n```{r}\n\n# More than 1 grouping variable\nfviz_ellipses(res.mca, c(\"Vomiting\", \"Fever\"),\n    geom = \"point\"\n)\n\n```\n\nBar plot for Cos2 of individuals:\n\n```{r}\n# Bar plot for Cos2 of individuals\nfviz_cos2(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n```\n\nContribution of individuals to the dimensions \n\n```{r}\n# Contribution of individuals to the dimensions\nfviz_contrib(res.mca, choice = \"ind\", axes = 1:2, top = 20)\n```\n\n### Perfomring MCA by hand\n\nLet's work with wine data:\n\n```{r}\n# Data\ndata(wine)\n\n```\n\nLet's keep only the variables that are facotrs\n\n```{r}\n# Keep factors\nMCA.dt <- wine[, sapply(wine, is.factor)]\n\n# Look at data\nMCA.dt\n\n```\n\nThe two variables involved have these number of levels:\n```{r}\n# Count levels\nsapply(MCA.dt, nlevels)\n\n```\n\nLet's now perform a standard MCA analysis with FactoMineR. Let's say we are aiming for extracting 4 components:\n\n```{r}\n# Define goal number of PCs\nnpcs <- 4\n\n# Perform MCA\nres.mca <- FactoMineR::MCA(\n    X = MCA.dt,\n    ncp = npcs,\n    graph = FALSE\n)\n\n```\n\nWe can now look at the coordinates of the individuals on the `npcs` dimensions:\n\n```{r}\n# Coordinates of the individuals on the dimensions\nres.mca$ind$coord\n\n```\n\nWe can also look at the right singular vectors of the data:\n\n```{r}\n# right singular vectors of data\nres.mca$svd$V\n\n```\n\nNow we can replicate the computations involved following a description of MCA I found in AudigierEtAl2016_MICAT.\nFirst, we need to extract a few objects we will need.\n\n```{r}\n# Create required processing objects\nI <- nrow(MCA.dt)             # numebr of individuals\nK <- ncol(MCA.dt)             # number of categorical predictors\nqk <- sapply(MCA.dt, nlevels) # number of levels per categorical variable\nJ <- sum(qk)                  # total number of categories\n\n```\n\nThen, we can create the disjonctive table:\n\n```{r}\n# Disjunctive table\nZ <- tab.disjonctif(MCA.dt)\n\n# And look at it\nZ\n\n```\n\nNotice the relationship of the disjunctive table with a standard contingency table:\n\n```{r}\n# Relationship between disjunctive table and contingency table\nN <- t(Z[, 1:3]) %*% Z[, 4:7]\nN - table(MCA.dt)\n\n```\n\nWe can compute the weighting matrices that are used to perform MCA.\nFirst, we want to compute the proportion of individuals taking a category value for each variable, which is known as a distance matrix:\n\n```{r}\n# Distance Metric Matrix\npxkqk <- colSums(Z) / I\nD_Sigma <- diag(pxkqk)\n\n```\n\nThen we want to compute the individual weight matrix:\n\n```{r}\n# Weight Matrix\nW_mat <- diag(rep(1, I))/I\n\n```\n\nand finally, we want to compute the center matrix:\n\n```{r}\n# M matrix (center matrix)\nM <- matrix((rep(colMeans(Z), nrow(Z))),\n    nrow = nrow(Z),\n    byrow = TRUE\n)\n\n```\n\nWe can now take the SVD of a centered version of the disjunctive table with an SVD triplet function form `FactoMineR`:\n\n```{r}\n# SVD of triplet (Z-M, D_Sigma, W_mat)\nSVD.trip <- FactoMineR::svd.triplet(\n    X = Z - M,\n    row.w = diag(W_mat),\n    col.w = diag(1 / K * solve(D_Sigma)),\n    ncp = npcs\n)\n\n```\n\nWe can also take the SVD of the same centered matrix using a thes standard `svd` R function:\n\n```{r}\n# Manual SVD triplet\nSVD.man <- svd(sqrt(W_mat) %*% (Z - M) %*% sqrt(1 / K * solve(D_Sigma)))\n\n```\n\nWhen computing the SVD in this manual way, we need to convert the scale according to Chaven 2017 p. 3\n\n```{r}\n# Convert back to correct scales (according to Chaven 2017 p. 3)\nV.man <- (solve(sqrt(1 / K * solve(D_Sigma))) %*% SVD.man$v)[, 1:npcs]\nU.man <- (solve(sqrt(W_mat)) %*% SVD.man$u)[, 1:npcs]\nL.man <- SVD.man$d[1:npcs]\n\n```\n\nDoing so, you will see that the computation obtained by `FactoMineR` approach is the same as the one done by hand\n\n```{r}\n# Compare SVD triplet and manual SVD of weighted matrix\nround(abs(SVD.trip$V) - abs(V.man), 5)\nround(abs(SVD.trip$U) - abs(U.man), 5)\nround(abs(SVD.trip$vs[1:npcs]) - abs(L.man), 5)\n  \n```\n\n```{r mca, warning = FALSE, message = FALSE}\n\n  # Reconstruction Formula\n  d_hat <- SVD.trip$vs[1:npcs] # matrix of the singular values \n                            # (Squared would be eigenvalues of Z)\n  u_hat <- SVD.trip$U # Left singular vectors matrix\n  v_hat <- SVD.trip$V # Right singular vectors matrix\n  \n  z_hat <- u_hat %*% diag(d_hat) %*% t(v_hat) + M\n  colSums(z_hat)\n  colSums(Z)\n  \n  # Compare SVD matrices\n  # Matrix of singular values\n  res.mca$svd$vs[1:npcs] -\n    d_hat[1:npcs]\n\n  res.mca$svd$vs[1:npcs] -\n    L.man\n  \n  # Left Singular Vectors Matrix\n  round(\n    res.mca$svd$U - u_hat,\n    3\n  )\n\n  round(\n    res.mca$svd$U - U.man,\n    3\n  )\n\n  # Right Singular Vectors Matrix\n  round(\n    res.mca$svd$V -\n      v_hat, \n    3)\n  # Correlation between columns\n  # And look into the PCAmixdata package\n  round(cor(v_hat, res.mca$svd$V), 1)\n  \n  # Coordinates on Dimensions are recovered\n  round(\n    res.mca$ind$coord -\n      u_hat %*% diag(d_hat),\n    3\n  )\n\n# Following JosseHusson2016 -----------------------------------------------\n\n  I <- nrow(MCA.dt)\n  J <- ncol(MCA.dt)\n  X <- tab.disjonctif(MCA.dt)\n  rowMarg <- rowSums(X) # = J\n  colMarg <- colSums(X) # = number of ids in a category\n  D_Sigma <- diag(colMarg)\n  D <- 1/I * diag(rep(1, I)) # rowMasses\n  SVD.trip <- svd.triplet(X = diag(rep(1, I)) %*% X %*% solve(D_Sigma),\n                          row.w = diag( D ),\n                          col.w = diag( 1/(I*J)*D_Sigma ),\n                          ncp=2\n                          )\n  svd(I * X %*% solve(D_Sigma))\n  \n  SVD.trip$vs\n  round(SVD.trip$vs[2:3] - res.mca$svd$vs[1:2], 3)\n  \n  SVD.trip$U\n  res.mca$svd$U\n#   round(SVD.trip$U[, 2:3] - res.mca$svd$U, 3)\n  res.mca$svd$U\n  \n#   SVD.trip$V\n#   round(SVD.trip$V[, 2:3] - res.mca$svd$V, 3)\n#   res.mca$svd$V\n\n```\n\n```{r pca-mix}\n# Prepare environment ----------------------------------------------------------\n\nlibrary(psych)\nlibrary(PCAmixdata)\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n\ndata(tea)\n\nhead(tea)\n\nlapply(tea[, c(\"where\", \"how\", \"SPC\")], nlevels)\n\nsapply(tea, nlevels)\n\nx <- tea[, c(\"where\", \"how\", \"Tea\")]\n\nCTD <- tab.disjonctif(x)\npk <- colMeans(CTD)\n\nCTD[1, ] / pk\nCTD[4, ] / pk\n1/.64\n1/.56666667\n\nCTD_t <- t(apply(CTD, 1, function(r) {t(r)/pk} ))\ncolMeans(CTD_t)\n\n\nN <- tab.disjonctif(x)\n1/nrow(tea)\n\n# Correspondance analysis based on contingency table ---------------------------\nx <- tea[, c(\"SPC\", \"where\")]\nn <- nrow(x)\nr <- nlevels(x[, 1]) \nC <- nlevels(x[, 2]) \nN <- table(x)\n\n# Contingency table\nN\n\n# Indicator matrix\nZ <- tab.disjonctif(x)\nZ1 <- Z[, 1:r]\nZ2 <- Z[, -c(1:r)]\nN - t(Z1) %*% Z2\n\n# Correspondance matrix\nP <- 1/n * N\n\n# From Greenacre1984\nN\n\n# Column and row sums\nr_bold <- rowSums(N)\nc_bold <- colSums(N)\n\ndrop(N %*% rep(1, ncol(N))) - r_bold\ndrop(t(N) %*% rep(1, nrow(N))) - c_bold\n\nD_r <- diag(r_bold)\nD_c <- diag(c_bold)\n\n# Matrices of profiles\n\nR <- solve(D_r) %*% P\nC <- solve(D_c) %*% t(P)\n\n# Centroids\nr <- t(C) %*% c_bold\nc <- t(R) %*% r_bold\n\n# Generalized SVD of P - r_bold t(c_bold)\n\nP - r_bold %*% t(c_bold)\nA <- svd(P - r %*% t(c))$u\nB <- svd(P - r %*% t(c))$v\n\nt(A) %*% solve(D_r) %*% A\nt(B) %*% solve(D_c) %*% B\n\n# From Jolliffe p. 37\n# r_bold <- rep(1, r)\n# c_bold <- rep(1, C)\n# D_r <- diag(r_bold)\n# D_c <- diag(c_bold)\n# Omega <- solve(D_r)\n# Psi <- solve(D_c)\n# X <- P - r_bold %*% t(c_bold)\n\n# V <- svd(X)$u\n# M <- diag(svd(X)$d)\n# B <- svd(X)$v\n\n# round(t(V) %*% Omega %*% V, 3)\n# round(t(B) %*% Psi %*% B, 3)\n\n# X_til <- sqrt(Omega) %*% X %*% sqrt(Psi)\n\n# W <- svd(X_til)$u\n# K <- diag(svd(X_til)$d)\n# C <- svd(X_til)$v\n\n# solve(sqrt(Omega)) %*% W - V\n# solve(sqrt(Omega)) %*% C - B\n\n# W %*% K\n\n# # Row profiles\n# D_r\n\n# MCA based on Audigier et al 2017 p. 505 (p. 5 of pdf) ------------------------\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define row weights\n    I <- nrow(x)\n    R <- diag(1 / I, I)\n\n    # Define Z (the disjunctive table)\n    Z <- tab.disjonctif(x)\n\n    # Define column weights\n    K <- ncol(x)\n    plxk <- colMeans(Z)\n    D_sigma <- diag(plxk)\n    1 / K * solve(D_sigma)\n\n    # Define M\n    M <- matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n\n    # Centered matrix?\n    Z - M\n\n# MCA based on Chavent Et Al 2017 et al 2017 p. 505 (p. 5 of pdf) -----------1  ---\n\n    # Work with categorical predictors from the tea dataset\n    x <- tea[, c(\"where\", \"how\", \"Tea\")]\n\n    # Define Z (the disjunctive table)\n    G <- tab.disjonctif(x)\n\n    # Define row weights\n    n <- nrow(x)\n    N <- diag(1 / n, n)\n\n    # Define column weights\n    M <- diag(n / colSums(G))\n    solve(D_sigma)\n\n    # Create Z, the centered G?\n    plxk <- colMeans(G)\n    G - matrix(rep(plxk, I), nrow = I, byrow = TRUE)\n    Z <- t(t(G) - plxk)\n\n    # SVD of Z\n    u_til <- svd(Z)$u\n    lambda_til <- svd(Z)$d\n\n    # Principal Compoent Scores (factor coordinates of the rows)\n    u_til[, 1:3] %*% diag(lambda_til[1:3])\n```\n\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"pca-mix.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"title-block-banner":true,"draft":true,"title":"PCA with metrics, dimensionality reduction through PCA and MCA","author":"Edoardo Costantini","date":"2022-05-17","slug":"pca-mix","categories":["Categorical data"],"bibliography":["../../resources/bibshelf.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}