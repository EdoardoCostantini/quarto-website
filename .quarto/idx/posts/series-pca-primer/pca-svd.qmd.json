{"title":"Principal Component Analysis, SVD, and EVD","markdown":{"yaml":{"title":"Principal Component Analysis, SVD, and EVD","author":"Edoardo Costantini","date":"2022-05-13","slug":"pca-svd-evd","categories":["PCA"],"bibliography":"../../resources/bibshelf.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nPrincipal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{T}$, where $r < p$.\n\nThe columns of $\\mathbf{T}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe follow the common practice of assuming that the columns of $\\mathbf{X}$ are mean-centered and scaled to have a variance of 1.\nThe first PC of $\\mathbf{X}$ is the linear combination of the columns of $\\mathbf{X}$ with the largest variance:\n$$\n    \\mathbf{t}_1 = \\lambda_{11} \\mathbf{x}_1 + \\lambda_{12} \\mathbf{x}_2 + \\dots + \\lambda_{1p} \\mathbf{x}_p = \\mathbf{X} \\mathbf{\\lambda}_1\n$$\nwith $\\mathbf{\\lambda}_1$ being the $1 \\times p$ vector of coefficients $\\lambda_{11}, \\dots, \\lambda_{1p}$.\nThe second principal component ($\\mathbf{t}_2$) is defined by finding the vector of coefficients $\\mathbf{\\lambda}_2$ giving the linear combination of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$ with maximal variance out of all the linear combinations that are uncorrelated with $\\mathbf{t}_1$.\nEvery subsequent column of $\\mathbf{T}$ can be understood in the same way.\nAs a result, the PCs are independent by definition and every subsequent PC has less variance than the preceding one.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{T} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of weights, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_q$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{T}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{T}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nwhere:\n\n- $\\mathbf{D}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XX}^T$ and $\\mathbf{X}^T\\mathbf{X}$;\n- $\\mathbf{U}$ is an $n \\times p$ orthogonal matrix such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times p$ orthogonal matrix of eigenvectors of $\\mathbf{X}^T\\mathbf{X}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\nThe scores on the first $r$ PCs are given by the $n \\times r$ matrix $\\mathbf{U}_{r}\\mathbf{D}_{r}$, where $\\mathbf{U}_{r}$ and $\\mathbf{D}_{r}$ stand for the reduced forms of $\\mathbf{U}$ and $\\mathbf{D}$ obtained by taking their first $r$ column and diagonal values.\nThe weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}_{r}$, so that:\n$$\n    \\mathbf{T} = \\mathbf{U}_{r}\\mathbf{D}_{r} = \\mathbf{X}\\mathbf{V}_{r}\n$$\n\nAnother way to achieve the same solution relies on the [eigenvalue decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) (EVD) of the cross-product matrix (or the correlation matrix).\nIn particular, the EVD of $\\mathbf{X}^{T} \\mathbf{X}$ is equal to $\\mathbf{V} \\mathbf{D}^{2} \\mathbf{V}^{T}$, where $\\mathbf{V}$ and $\\mathbf{D}$ are the same eigenvectors and eigenvalues, respectively, defined above.\n\n# Learn by coding\n\nLet us start by generating sampling 1000 (n) observations from a multivariate normal distribution.\nIn this example, we use a block structure in the correlation matrix to generate a collection of 4 items (p) that can be summarized well by two principal components (K).\n\n```{r data, warning = FALSE, message = FALSE}\n# Generate data ----------------------------------------------------------------\n\n# Fixed parameters\nn <- 1e3  # sample size\np <- 4    # final number of items\nK <- 2    # target number of components\nrho <- .8 # Fixed correlation between observed items\n\n# Define correlation matrix with blocks\nSigma_K <- lapply(1:K, function (x){\n    Sigma <- matrix(rho,\n                    nrow = p/K,\n                    ncol = p/K)\n            diag(Sigma) <- 1\n            Sigma\n})\nSigma <- Matrix::bdiag(Sigma_K)\n\n# Define vector of observed item means\nmu <- rep(5, p)\n\n# Sample data from multivaraite normal distribution\nset.seed(1235)\nX <- MASS::mvrnorm(n, mu, Sigma)\n\n# Scale it for future use in PCA\nX <- scale(X)\n\n```\n\n## PCA as SVD\n\nAs we discussed, PCA can be obtained as the SVD of $X$.\n\n```{r pca-svd, warning = FALSE, message = FALSE}\n# PCA as singular value decomposition of X -------------------------------------\n\n# Perform SVD\nuSVt  <- svd(X)\n\n# Extract objects\nU     <- uSVt$u\nSigma <- diag(uSVt$d)\nV     <- uSVt$v\n\n# Compute the PC scores\nT_svd <- U %*% Sigma\n\n# Compute the PC scores (equivalent way)\nT_svd <- X %*% V\n\n# Define eigenvalues\neigenv_svd <- uSVt$d^2\n\n# Compute cumulative proportion of explained variance\nCPVE_svd <- cumsum(prop.table(eigenv_svd))\n\n```\n\n## PCA as eigenvalue decomposition\n\nPCA can also be obtained as the eigenvalue decomposition of the cross-product matrix of $X$\n\n```{r pca-eigen-xtx, warning = FALSE, message = FALSE}\n# PCA as eigenvalue decomposition of XtX --------------------------------------------\n\n# Compute the cross-product matrix XtX\nXtX <- t(X) %*% X\n\n# Perform eigenvalue decomposition\neigenmat_XtX <- eigen(XtX)\n\n# Extract eigenvalues\neigenvalues_XtX <- eigenmat_XtX$values\n\n# Extract component loadings\neigenvectors_XtX <- eigenmat_XtX$vectors\n\n# Compute the PC scores\nT_eig_XtX <- X %*% eigenvectors_XtX\n\n# Compute cumulative proportion of explained variance\nCPVE_eig_XtX <- cumsum(prop.table(eigenvalues_XtX))\n\n```\n\nIt can also be computed by the eigenvalue decomposition of the correlation matrix of $X$.\n\n```{r pca-eigen-corx, warning = FALSE, message = FALSE}\n# PCA as eigenvalue decomposition of cor(X) -----------------------------------------\n\n# Compute the correlation matrix\nX_cormat <- cor(X, method = \"pearson\")\n\n# Perform eigenvalue decomposition\neigenmaT_eig_corx <- eigen(X_cormat)\n\n# Extract eigenvalues\neigenvalues_corx <- eigenmaT_eig_corx$values\n\n# Extract component loadings\neigenvectors_corx <- eigenmaT_eig_corx$vectors\n\n# Compute the PC scores\nT_eig_corx <- X %*% eigenvectors_corx\n\n# Compute cumulative proportion of explained variance\nCPVE_corx <- cumsum(prop.table(eigenvalues_corx))\n\n```\n\n## Compare\n\nTo check our results we can compare the PC scores, CPVEs, and loading matrices with the results of the standard R function to compute PCA (`prcomp`).\nLet's compute the PCs and all the relevant quantities with the `prcomp`:\n\n```{r compare, warning = FALSE, message = FALSE}\n# PCA with prcomp --------------------------------------------------------------\n\n# Compute PCA with prcomp\nPCX <- prcomp(X)\n\n# Extract component scores\nT_prcomp <- PCX$x\n\n# Extract eigenvalues\neigenvalues_prcomp <- PCX$sdev^2\n\n# Extract component loadings\nV_prcomp <- as.matrix(PCX$rotation)\n\n# Extract cumulative explained variance\nCPVE_PCX <- cumsum(prop.table(eigenvalues_prcomp))\n\n```\n\nTo compare the PC score and loading matrices we will use [Tucker congruence](https://en.wikipedia.org/wiki/Congruence_coefficient) coefficient.\nThis is a measure of similarity between matrices that ranges between -1 and +1.\nA congruence of 1 means the two matrices are identical.\nTo compare the vectors of the cumulative proportion of explained variance we will simply print the vectors one next to the other.\nIn the following, you can see that all of the ways to find solutions to the PCA problem are identical.\n\n### PC scores\n```{r compare-pcs, warning = FALSE, message = FALSE}\n# Compare solutions ------------------------------------------------------------\n\n# Load package for tucker congruence\nlibrary(RegularizedSCA)\n\n# > PC scores ------------------------------------------------------------------\n\n# prcomp results = SVD\nTuckerCoef(T_prcomp, T_svd)$tucker_value\n\n# prcomp results = eigenvalues_XtX\nTuckerCoef(T_prcomp, T_eig_XtX)$tucker_value\n\n# prcomp results = eigenvectors_corx\nTuckerCoef(T_prcomp, T_eig_corx)$tucker_value\n\n```\n\n### Component loadings\n```{r compare-load, warning = FALSE, message = FALSE}\n# > Component loadings ---------------------------------------------------------\n\n# prcomp results = SVD\nTuckerCoef(V_prcomp, V)$tucker_value\n\n# prcomp results = eigenvalues_XtX\nTuckerCoef(V_prcomp, eigenvectors_XtX)$tucker_value\n\n# prcomp results = eigenvectors_corx\nTuckerCoef(V_prcomp, eigenvectors_corx)$tucker_value\n\n```\n\n### Cumulative proportion of explained variance\n```{r compare-cpve, warning = FALSE, message = FALSE}\n# Cumulative proportion of explained variance\ndata.frame(\n    SVD = CPVE_svd,\n    eig_XtX = CPVE_eig_XtX,\n    eig_corx = CPVE_corx,\n    prcomp = CPVE_PCX\n)\n\n```\n\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n\n# References"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"pca-svd.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"title-block-banner":true,"title":"Principal Component Analysis, SVD, and EVD","author":"Edoardo Costantini","date":"2022-05-13","slug":"pca-svd-evd","categories":["PCA"],"bibliography":["../../resources/bibshelf.bib"]},"extensions":{"book":{"multiFile":true}}}}}