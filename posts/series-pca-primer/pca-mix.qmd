---
draft: true
title: PCA with metrics, dimensionality reduction through PCA and MCA
author: Edoardo Costantini
date: '2022-05-17'
slug: pca-mix
categories: ["Categorical data"]
bibliography: ../../resources/bibshelf.bib
---

# Introduction

**Principal Component Analysis** (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \times p$ data matrix $\mathbf{X}$ with minimal loss of information.
We refer to this low-dimensional representation as the $n \times r$ matrix $\mathbf{Z}$, where $r < p$.
The columns of $\mathbf{Z}$ are called principal components (PCs) of $\mathbf{X}$.
We can write the relationship between all the PCs and $\mathbf{X}$ in matrix notation:
\begin{equation} \label{eq:PCAmatnot}
    \mathbf{Z} = \mathbf{X} \mathbf{\Lambda}
\end{equation}
where $\mathbf{\Lambda}$ is a $p \times r$ matrix of coefficients, with columns $\mathbf{\lambda}_1, \dots, \mathbf{\lambda}_r$.
PCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.
The coefficient vectors $\mathbf{\lambda}_1, \dots, \mathbf{\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\mathbf{x}_1, \dots, \mathbf{x}_p$.
The projected values are the principal component scores $\mathbf{Z}$.

The goal of PCA is to find the values of $\mathbf{\Lambda}$ that maximize the variance of the columns of $\mathbf{Z}$.
One way to find the PCA solution for $\mathbf{\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\mathbf{X}$:

\begin{equation} \label{eq:SVD}
    \mathbf{X} = \mathbf{UDV}'
\end{equation}

The PCs scores are given by the $n \times r$ matrix $\mathbf{UD}$, and the weights $\mathbf{\Lambda}$ are given by the $p \times r$ matrix $\mathbf{V}$.

**Multiple Correspondence Analysis** (MCA) is generally regarded as an equivalent tool that applies to discrete data.
Chavent et al. [-@chaventEtAl:2014] have shown how using weights on rows and columns of the input data matrix can define a general PCA framework that includes standard PCA and MCA as special cases.
This approach is often referred to as **PCA with metrics**, as metrics are used to introduce the weights.
In this post, I want to show how PCA and MCA are related through this framework.

## Generalised Singular Value Decomposition

Consider an $n \times p$ matrix of input variables $\mathbf{X}$ with row metric $\mathbf{N}$ and column metric $\mathbf{M}$.
The Generalized Singular Value Decomposition of $\mathbf{X}$ can be written as:
$$
\mathbf{X} = \mathbf{U \Lambda V}^T
$$
where:

- $\mathbf{\Lambda}$ is the $r \times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\mathbf{XMX}^T\mathbf{N}$ and $\mathbf{X}^T\mathbf{NXM}$;
- $\mathbf{U}$ is the $n \times r$ matrix of the first $r$ eigenvectors of $\mathbf{XMX}^T\mathbf{N}$ such that $\mathbf{U}^T\mathbf{MU=I}$
- $\mathbf{V}$ is the $p \times r$ matrix of the first $r$ eigenvectors of $\mathbf{X}^T\mathbf{NXM}$ such that $\mathbf{V}^T\mathbf{MV=I}$;

The GSVD of $\mathbf{X}$ can be obtained by taking 

- first taking the standard SVD of the transformed matrix $\tilde{\mathbf{X}} = \mathbf{N}^{1/2}\mathbf{X}\mathbf{M}^{1/2}$ which gives:
$$
\tilde{\mathbf{X}} = \tilde{\mathbf{U}}\tilde{\mathbf{\Lambda}}\tilde{\mathbf{V}}^T
$$
- and then transforming each element back to the original scale
$$
\mathbf{\Lambda} = \tilde{\mathbf{\Lambda}}
$$
$$
\mathbf{U} = \mathbf{N}^{-1/2}\tilde{\mathbf{U}}
$$
$$
\mathbf{V} = \mathbf{M}^{-1/2}\tilde{\mathbf{V}}
$$


## Relationship of GSVD to standard SVD

It's easy to see how this GSVD differs from the standard formulation of SVD simply by the presence of the metrics $\mathbf{N}$ and $\mathbf{M}$.
As you can see [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition), in the standard formulation of SVD:

- $\mathbf{\Lambda}$ is the $r \times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\mathbf{XX}^T$ and $\mathbf{X}^T\mathbf{X}$;
- $\mathbf{U}$ is the $n \times r$ matrix of the first $r$ eigenvectors of $\mathbf{XX}^T$ such that $\mathbf{U}^T\mathbf{U=I}$
- $\mathbf{V}$ is the $p \times r$ matrix of the first $r$ eigenvectors of $\mathbf{X}^T\mathbf{NXM}$ such that $\mathbf{V}^T\mathbf{V=I}$;

## PCA and MCA as special cases of GSVD

The solutions for both PCA and MCA can be obtained as special cases of the GSVD approach described by setting $\mathbf{X}$ equal to a preprocessed version of the original data $\mathbf{X}$ and using $\mathbf{N}$ and $\mathbf{M}$ to appropriately weight the rows and columns.

### PCA

The input data for standard PCA is the $n \times p$ matrix $\mathbf{X}$ of $n$ rows (observations) described by $p$ numerical variables.
The columns of this matrix are usually centered and standardized.
The GSVD can be used to find the solution to PCA by setting $\mathbf{X}$ equal to the centered and standardized version of $\mathbf{X}$ and weighting:

- its rows by $1/n$, which is obtained by setting $\mathbf{N} = \frac{1}{n}I_n$
- its columns by $1$, which is obtained by setting $\mathbf{M} = I_p$.
This metric indicates that the distance between two observations is the standard euclidean distance between two rows of $\mathbf{X}$

By setting these values for the metrics, it is easy to see how the GSVD of $\mathbf{X}$ reduces to the standard SVD of $\mathbf{X}$.

### MCA

For an $n \times p$ data matrix $\mathbf{X}$ with $n$ observations (rows) and $p$ discrete predictors (columns).
Each $j = 1, \dots, p$ discrete variable has $k_j$ possible values.
The sum of the $p$ $k_j$ values is $k$. 
$\mathbf{X}$ is preprocessed by coding each level of the discrete items as binary variables describing whether each observation takes a specific value for every discrete variable.
This results in an $n \times k$ [complete disjunctive table](https://www.xlstat.com/en/solutions/features/complete-disjuncive-tables-creating-dummy-variables) $\mathbf{G}$, sometimes also referred to as an indicator matrix.

MCA is usually obtained by applying Correspondence Analysis to $\mathbf{G}$, which means applying standard PCA to the matrices of the row profiles and the column profiles.
In particular, for the goal of obtaining a lower-dimensional representation of $\mathbf{X}$ we are interested in the standard PCA of the row profiles.
Within the framework of PCA with metrics, MCA can be obtained by first setting:

- $\mathbf{X}$ to the centered $\mathbf{G}$
- $\mathbf{N} = \frac{1}{n}I_n$
- $\mathbf{M} = \text{diag}(\frac{n}{n_s}, s = 1, \dots, k)$

The coordinates of the observations (the principal component scores) can be obtained by applying the GSVD of $\mathbf{X}$ with the given metrics.

# Learn by coding

To understand the relationship between SVD, GSVD, PCA, and MCA we will need two packages:

```{r}
# Load Packages
library("FactoMineR") # for analysis
library("factoextra") # for visualizarion1
```

## MCA

### A standard MCA analysis according to FactoMineR

We will start by performing the MCA analysis.
Let's start by loading, preparing, and summarising the data:

```{r}
# Data Prep ---------------------------------------------------------------

# Load data
data("poison")
head(poison[, 1:7], 3) # survey style data

# Subset active individuals and variables
poison.active <- poison[1:55, 5:15]
head(poison.active[, 1:6], 3)

# Summaries
str(poison.active)
for (i in 1:4) {
    plot(poison.active[, i],
        main = colnames(poison.active)[i],
        ylab = "Count", col = "steelblue", las = 2
    )
}

```

We can then use the `MCA()` function from the `FactorMineR` package to perform a standard MCA analysis of the data.

```{r}
# The analysis ------------------------------------------------------------
res.mca <- FactoMineR::MCA(
    X = poison.active,
    ncp = 5,
    graph = TRUE
)

```

We can now visualize different aspects of the analysis.

We extract the eigenvalues

```{r}
# Visualization -----------------------------------------------------------

# Eigenvalues / Variances
eig.val <- get_eigenvalue(res.mca)
eig.val # proportion of variances retained by dimensions
```

Percentages of inertia explained by MCA

```{r}
# Percentages of Inertia explained by MCA
fviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 45))
```

Biplots

```{r}
# Biplot
fviz_mca_biplot(res.mca,
    repel = TRUE, # avoid text overlapping
    ggtheme = theme_minimal()
)
# Rows (individuals) are represented by blue points;
# Columns (variable categories) by red triangles.
```

Graphs of variables

```{r}

# Graph of variables
var <- get_mca_var(res.mca)
var
# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)

```

Graphs for individuals

```{r}
# Graph of individuals
ind <- get_mca_ind(res.mca) # extract the results for individuals
ind
# Coordinates of column points
head(ind$coord)
# Quality of representation
head(ind$cos2)
# Contributions
head(ind$contrib)

# BIplot for individuals only (no vars)
fviz_mca_ind(res.mca,
    col.ind = "cos2",
    gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
    repel = TRUE, ggtheme = theme_minimal()
)
fviz_mca_ind(res.mca,
    label = "none",
    habillage = "Vomiting", # color by groups defined by variable
    palette = c("#00AFBB", "#E7B800"),
    addEllipses = TRUE, ellipse.type = "confidence",
    repel = TRUE, ggtheme = theme_minimal()
)

```

Ellipses

```{r}

# More than 1 grouping variable
fviz_ellipses(res.mca, c("Vomiting", "Fever"),
    geom = "point"
)

```

Bar plot for Cos2 of individuals:

```{r}
# Bar plot for Cos2 of individuals
fviz_cos2(res.mca, choice = "ind", axes = 1:2, top = 20)
```

Contribution of individuals to the dimensions 

```{r}
# Contribution of individuals to the dimensions
fviz_contrib(res.mca, choice = "ind", axes = 1:2, top = 20)
```

### Perfomring MCA by hand

Let's work with wine data:

```{r}
# Data
data(wine)

```

Let's keep only the variables that are facotrs

```{r}
# Keep factors
MCA.dt <- wine[, sapply(wine, is.factor)]

# Look at data
MCA.dt

```

The two variables involved have these number of levels:
```{r}
# Count levels
sapply(MCA.dt, nlevels)

```

Let's now perform a standard MCA analysis with FactoMineR. Let's say we are aiming for extracting 4 components:

```{r}
# Define goal number of PCs
npcs <- 4

# Perform MCA
res.mca <- FactoMineR::MCA(
    X = MCA.dt,
    ncp = npcs,
    graph = FALSE
)

```

We can now look at the coordinates of the individuals on the `npcs` dimensions:

```{r}
# Coordinates of the individuals on the dimensions
res.mca$ind$coord

```

We can also look at the right singular vectors of the data:

```{r}
# right singular vectors of data
res.mca$svd$V

```

Now we can replicate the computations involved following a description of MCA I found in AudigierEtAl2016_MICAT.
First, we need to extract a few objects we will need.

```{r}
# Create required processing objects
I <- nrow(MCA.dt)             # numebr of individuals
K <- ncol(MCA.dt)             # number of categorical predictors
qk <- sapply(MCA.dt, nlevels) # number of levels per categorical variable
J <- sum(qk)                  # total number of categories

```

Then, we can create the disjonctive table:

```{r}
# Disjunctive table
Z <- tab.disjonctif(MCA.dt)

# And look at it
Z

```

Notice the relationship of the disjunctive table with a standard contingency table:

```{r}
# Relationship between disjunctive table and contingency table
N <- t(Z[, 1:3]) %*% Z[, 4:7]
N - table(MCA.dt)

```

We can compute the weighting matrices that are used to perform MCA.
First, we want to compute the proportion of individuals taking a category value for each variable, which is known as a distance matrix:

```{r}
# Distance Metric Matrix
pxkqk <- colSums(Z) / I
D_Sigma <- diag(pxkqk)

```

Then we want to compute the individual weight matrix:

```{r}
# Weight Matrix
W_mat <- diag(rep(1, I))/I

```

and finally, we want to compute the center matrix:

```{r}
# M matrix (center matrix)
M <- matrix((rep(colMeans(Z), nrow(Z))),
    nrow = nrow(Z),
    byrow = TRUE
)

```

We can now take the SVD of a centered version of the disjunctive table with an SVD triplet function form `FactoMineR`:

```{r}
# SVD of triplet (Z-M, D_Sigma, W_mat)
SVD.trip <- FactoMineR::svd.triplet(
    X = Z - M,
    row.w = diag(W_mat),
    col.w = diag(1 / K * solve(D_Sigma)),
    ncp = npcs
)

```

We can also take the SVD of the same centered matrix using a thes standard `svd` R function:

```{r}
# Manual SVD triplet
SVD.man <- svd(sqrt(W_mat) %*% (Z - M) %*% sqrt(1 / K * solve(D_Sigma)))

```

When computing the SVD in this manual way, we need to convert the scale according to Chaven 2017 p. 3

```{r}
# Convert back to correct scales (according to Chaven 2017 p. 3)
V.man <- (solve(sqrt(1 / K * solve(D_Sigma))) %*% SVD.man$v)[, 1:npcs]
U.man <- (solve(sqrt(W_mat)) %*% SVD.man$u)[, 1:npcs]
L.man <- SVD.man$d[1:npcs]

```

Doing so, you will see that the computation obtained by `FactoMineR` approach is the same as the one done by hand

```{r}
# Compare SVD triplet and manual SVD of weighted matrix
round(abs(SVD.trip$V) - abs(V.man), 5)
round(abs(SVD.trip$U) - abs(U.man), 5)
round(abs(SVD.trip$vs[1:npcs]) - abs(L.man), 5)
  
```

```{r mca, warning = FALSE, message = FALSE}

  # Reconstruction Formula
  d_hat <- SVD.trip$vs[1:npcs] # matrix of the singular values 
                            # (Squared would be eigenvalues of Z)
  u_hat <- SVD.trip$U # Left singular vectors matrix
  v_hat <- SVD.trip$V # Right singular vectors matrix
  
  z_hat <- u_hat %*% diag(d_hat) %*% t(v_hat) + M
  colSums(z_hat)
  colSums(Z)
  
  # Compare SVD matrices
  # Matrix of singular values
  res.mca$svd$vs[1:npcs] -
    d_hat[1:npcs]

  res.mca$svd$vs[1:npcs] -
    L.man
  
  # Left Singular Vectors Matrix
  round(
    res.mca$svd$U - u_hat,
    3
  )

  round(
    res.mca$svd$U - U.man,
    3
  )

  # Right Singular Vectors Matrix
  round(
    res.mca$svd$V -
      v_hat, 
    3)
  # Correlation between columns
  # And look into the PCAmixdata package
  round(cor(v_hat, res.mca$svd$V), 1)
  
  # Coordinates on Dimensions are recovered
  round(
    res.mca$ind$coord -
      u_hat %*% diag(d_hat),
    3
  )

# Following JosseHusson2016 -----------------------------------------------

  I <- nrow(MCA.dt)
  J <- ncol(MCA.dt)
  X <- tab.disjonctif(MCA.dt)
  rowMarg <- rowSums(X) # = J
  colMarg <- colSums(X) # = number of ids in a category
  D_Sigma <- diag(colMarg)
  D <- 1/I * diag(rep(1, I)) # rowMasses
  SVD.trip <- svd.triplet(X = diag(rep(1, I)) %*% X %*% solve(D_Sigma),
                          row.w = diag( D ),
                          col.w = diag( 1/(I*J)*D_Sigma ),
                          ncp=2
                          )
  svd(I * X %*% solve(D_Sigma))
  
  SVD.trip$vs
  round(SVD.trip$vs[2:3] - res.mca$svd$vs[1:2], 3)
  
  SVD.trip$U
  res.mca$svd$U
#   round(SVD.trip$U[, 2:3] - res.mca$svd$U, 3)
  res.mca$svd$U
  
#   SVD.trip$V
#   round(SVD.trip$V[, 2:3] - res.mca$svd$V, 3)
#   res.mca$svd$V

```

```{r pca-mix}
# Prepare environment ----------------------------------------------------------

library(psych)
library(PCAmixdata)

library("FactoMineR")
library("factoextra")

data(tea)

head(tea)

lapply(tea[, c("where", "how", "SPC")], nlevels)

sapply(tea, nlevels)

x <- tea[, c("where", "how", "Tea")]

CTD <- tab.disjonctif(x)
pk <- colMeans(CTD)

CTD[1, ] / pk
CTD[4, ] / pk
1/.64
1/.56666667

CTD_t <- t(apply(CTD, 1, function(r) {t(r)/pk} ))
colMeans(CTD_t)


N <- tab.disjonctif(x)
1/nrow(tea)

# Correspondance analysis based on contingency table ---------------------------
x <- tea[, c("SPC", "where")]
n <- nrow(x)
r <- nlevels(x[, 1]) 
C <- nlevels(x[, 2]) 
N <- table(x)

# Contingency table
N

# Indicator matrix
Z <- tab.disjonctif(x)
Z1 <- Z[, 1:r]
Z2 <- Z[, -c(1:r)]
N - t(Z1) %*% Z2

# Correspondance matrix
P <- 1/n * N

# From Greenacre1984
N

# Column and row sums
r_bold <- rowSums(N)
c_bold <- colSums(N)

drop(N %*% rep(1, ncol(N))) - r_bold
drop(t(N) %*% rep(1, nrow(N))) - c_bold

D_r <- diag(r_bold)
D_c <- diag(c_bold)

# Matrices of profiles

R <- solve(D_r) %*% P
C <- solve(D_c) %*% t(P)

# Centroids
r <- t(C) %*% c_bold
c <- t(R) %*% r_bold

# Generalized SVD of P - r_bold t(c_bold)

P - r_bold %*% t(c_bold)
A <- svd(P - r %*% t(c))$u
B <- svd(P - r %*% t(c))$v

t(A) %*% solve(D_r) %*% A
t(B) %*% solve(D_c) %*% B

# From Jolliffe p. 37
# r_bold <- rep(1, r)
# c_bold <- rep(1, C)
# D_r <- diag(r_bold)
# D_c <- diag(c_bold)
# Omega <- solve(D_r)
# Psi <- solve(D_c)
# X <- P - r_bold %*% t(c_bold)

# V <- svd(X)$u
# M <- diag(svd(X)$d)
# B <- svd(X)$v

# round(t(V) %*% Omega %*% V, 3)
# round(t(B) %*% Psi %*% B, 3)

# X_til <- sqrt(Omega) %*% X %*% sqrt(Psi)

# W <- svd(X_til)$u
# K <- diag(svd(X_til)$d)
# C <- svd(X_til)$v

# solve(sqrt(Omega)) %*% W - V
# solve(sqrt(Omega)) %*% C - B

# W %*% K

# # Row profiles
# D_r

# MCA based on Audigier et al 2017 p. 505 (p. 5 of pdf) ------------------------

    # Work with categorical predictors from the tea dataset
    x <- tea[, c("where", "how", "Tea")]

    # Define row weights
    I <- nrow(x)
    R <- diag(1 / I, I)

    # Define Z (the disjunctive table)
    Z <- tab.disjonctif(x)

    # Define column weights
    K <- ncol(x)
    plxk <- colMeans(Z)
    D_sigma <- diag(plxk)
    1 / K * solve(D_sigma)

    # Define M
    M <- matrix(rep(plxk, I), nrow = I, byrow = TRUE)

    # Centered matrix?
    Z - M

# MCA based on Chavent Et Al 2017 et al 2017 p. 505 (p. 5 of pdf) -----------1  ---

    # Work with categorical predictors from the tea dataset
    x <- tea[, c("where", "how", "Tea")]

    # Define Z (the disjunctive table)
    G <- tab.disjonctif(x)

    # Define row weights
    n <- nrow(x)
    N <- diag(1 / n, n)

    # Define column weights
    M <- diag(n / colSums(G))
    solve(D_sigma)

    # Create Z, the centered G?
    plxk <- colMeans(G)
    G - matrix(rep(plxk, I), nrow = I, byrow = TRUE)
    Z <- t(t(G) - plxk)

    # SVD of Z
    u_til <- svd(Z)$u
    lambda_til <- svd(Z)$d

    # Principal Compoent Scores (factor coordinates of the rows)
    u_til[, 1:3] %*% diag(lambda_til[1:3])
```


# TL;DR, just give me the code!
```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```
