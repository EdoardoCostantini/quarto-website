---
draft: true
title: Principal covariates regression in R
author: Edoardo Costantini
date: '2022-08-04'
slug: pcovr
categories: ["Supervised learning"]
bibliography: ../../resources/bibshelf.bib
---

# Introduction

Principal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables.
Compared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of $X$ variables and that correlated well with a set of $Y$ variables.
For more information, I recommend reading [@vervloetEtAl:2015] and [@de1992principal, @de1992principal].
In this post, you can find my R code notes on this method.
In these notes, I show the computations used by the `PCovR` R-package to perform the method.

# R code notes

This are some R notes:

```{r code-notes}
# Set up environment -----------------------------------------------------------

    # Load package that implements this method
    library("PCovR", verbose = FALSE, quietly = TRUE)

    # Load package for measure of similarity (TuckerCoef)
    library("RegularizedSCA", verbose = FALSE, quietly = TRUE)

    # Load example data from PCovR package
    data(alexithymia)

    # Explore its scale
    colMeans(alexithymia$X)
    colMeans(alexithymia$Y)

    apply(alexithymia$X, 2, var)
    apply(alexithymia$Y, 2, var)

    # Subset data
    X_raw <- alexithymia$X
    y_raw <- alexithymia$Y[, 1, drop = FALSE]

    # Define paramters that can be useful
    n <- nrow(X_raw)
    p <- ncol(X_raw)

    # Scale data
    X <- scale(X_raw)# * (n - 1) / n
    y <- scale(y_raw)# * (n - 1) / n

    # Define parameters
    alpha <- .5 # weighting parameter
    npcs <- 5

# Estimation -------------------------------------------------------------------

    # Estimate with PCovR function
    out <- PCovR::pcovr_est(
        X = X,
        Y = y,
        a = alpha,
        r = npcs # fixed number of components
    )

    # Estimate manually (Vervolet version)
    Hx <- X %*% solve(t(X) %*% X) %*% t(X)
    G_vv <- alpha * X %*% t(X) / sum(X^2) + (1 - alpha) * Hx %*% y %*% t(y) %*% Hx / sum(y^2)
    EG_vv <- eigen(G_vv) # eigen-decomposition of matrix
    T_vv <- EG_vv$vectors[, 1:npcs]

# Compare results --------------------------------------------------------------

    # T scores
    Ts <- list(
        PCovR = head(out$Te),
        PCovR_man = head(X %*% out$W),
        Vervolet = head(T_vv)
    )

    # Weights
    W <- list(
        PCovR = out$W,
        Vervolet = solve(t(X) %*% X) %*% t(X) %*% T_vv
    )

    # Px
    ( t(out$Te) %*% X )[, 1:5]
    ( t(out$W) %*% t(X) %*% X )[, 1:5]
    out$Px[, 1:5]

    # Py
    cbind(
        Py = out$Py,
        TtY = t(out$Te) %*% y,
        WtXtY = t(out$W) %*% t(X) %*% y
    )

    # B
    cbind(
        B = drop(out$B),
        WPY = drop(out$W %*% out$Py),
        WWtXtY = drop(out$W %*% t(out$W) %*% t(X) %*% y)
    )

# Maximum likelihood tuning of alpha -------------------------------------------

    # Fit PCovR
    pcovr_out <- pcovr(
        X = X_raw,
        Y = y_raw,
        rot = "none",
        R = npcs, # fixed number of components
        modsel = "seq" # fastest option
    )

    # Compute error ratio with function
    err <- ErrorRatio(
        X = X,
        Y = y,
        Rmin = npcs,
        Rmax = npcs
    )

    # Compute error ratio components
    lm_mod <- lm(y ~ -1 + X)
    ery <- 1 - summary(lm_mod)$r.squared

    Rmin <- npcs
    Rmax <- npcs
    sing <- svd(X)
    vec <- Rmin:Rmax
    vec <- c(vec[1] - 1, vec, vec[length(vec)] + 1)
    VAF <- c(0, cumsum(sing$d^2) / sum(sing$d^2))
    VAF <- VAF[vec + 1]
    scr <- array(NA, c(1, length(vec)))
    for (u in 2:(length(vec) - 1)) {
        scr[, u] <- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])
    }
    erx <- 1 - VAF[which.max(scr)]

    # Find alpha ML
    alpha_ML <- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)

    # Compare to one found by package
    pcovr_out$a - alpha_ML

# Reverting to PCA -------------------------------------------------------------

    # Use alpha 1
    alpha <- 1

    # Estimate PCovR quantities
    pcovr_out <- PCovR::pcovr_est(
        X = X,
        Y = y,
        a = alpha,
        r = npcs # fixed number of components
    )

    # Perform regular SVD of X
    uSVt <- svd(X)

    # Extract objects
    U <- uSVt$u
    D <- diag(uSVt$d)
    V <- uSVt$v

    # Define the number of rows
    I <- nrow(X)

    # Component loadings
    P_hat <- P_psych <- (I - 1)^{-1 / 2} * V %*% D

    # Component weights
    W_hat <- W_psych <- (I - 1)^{1 / 2} * V %*% solve(D)

    # Component scores
    T_hat <- T_psych <- (I - 1)^{1 / 2} * U
    T_hat <- T_psych_xw <- X %*% W_hat

    # The scores obtained with PCovR are the same as the ones obtained with PCA
    TuckerCoef(pcovr_out$Te, T_psych[, 1:npcs])$tucker_value

    # The loadings obtained with PCovR are the same as the ones obtained with PCA
    TuckerCoef(t(pcovr_out$Px), P_psych[, 1:npcs])$tucker_value

    # The weights obtained with PCovR are the same as the ones obtained with PCA
    TuckerCoef(pcovr_out$W, W_psych[, 1:npcs])$tucker_value

    # P is now proportional to W
    TuckerCoef(pcovr_out$W, t(pcovr_out$Px))$tucker_value

```

# TL;DR, just give me the code!
```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```

# References