{"title":"Principal Component Analysis and SVD","markdown":{"yaml":{"title":"Principal Component Analysis and SVD","author":"Edoardo Costantini","date":"2022-05-13","slug":"pcasvd","categories":["PCA"],"bibliography":"../../resources/bibshelf.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nPrincipal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{Z}$, where $r < p$.\n\nThe columns of $\\mathbf{Z}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe follow the common practice of assuming that the columns of $\\mathbf{X}$ are mean-centered and scaled to have a variance of 1.\nThe first PC of $\\mathbf{X}$ is the linear combination of the columns of $\\mathbf{X}$ with the largest variance:\n$$\n    \\mathbf{z}_1 = \\lambda_{11} \\mathbf{x}_1 + \\lambda_{12} \\mathbf{x}_2 + \\dots + \\lambda_{1p} \\mathbf{x}_p = \\mathbf{X} \\mathbf{\\lambda}_1\n$$\nwith $\\mathbf{\\lambda}_1$ being the $1 \\times p$ vector of coefficients $\\lambda_{11}, \\dots, \\lambda_{1p}$.\nThe second principal component ($\\mathbf{z}_2$) is defined by finding the vector of coefficients $\\mathbf{\\lambda}_2$ giving the linear combination of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$ with maximal variance out of all the linear combinations that are uncorrelated with $\\mathbf{z}_1$.\nEvery subsequent column of $\\mathbf{Z}$ can be understood in the same way.\nAs a result, the PCs are independent by definition and every subsequent PC has less variance than the preceding one.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{Z} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of weights, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_q$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{Z}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{Z}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nwhere:\n\n- $\\mathbf{D}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XX}^T$ and $\\mathbf{X}^T\\mathbf{X}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XX}^T$ such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\nThe PCs scores are given by the $n \\times r$ matrix $\\mathbf{UD}$, and the weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}$ [@jolliffe:2002 p45].\n\n## SVD solution to PCA\n\n## Eigen-decomposition solution to PCA\n\n# Learn by coding\n\n```{r data, warning = FALSE, message = FALSE}\n\nsome <- 1\n\n```\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n\n# References"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"pca-svd.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"title-block-banner":true,"title":"Principal Component Analysis and SVD","author":"Edoardo Costantini","date":"2022-05-13","slug":"pcasvd","categories":["PCA"],"bibliography":["../../resources/bibshelf.bib"]},"extensions":{"book":{"multiFile":true}}}}}