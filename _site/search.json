[
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\nApr 22, 2022\n\n\nCross-entropy as a measure of predictive performance\n\n\nMachine Learning\n\n\n\n\nMar 14, 2022\n\n\nEstimating the weighted covariance matrix in R\n\n\nStatistics\n\n\n\n\nFeb 28, 2022\n\n\nEstimating ridge regression in R\n\n\nPenalty\n\n\n\n\nNov 17, 2021\n\n\nThe sweep operator\n\n\nThe EM algorithm,Matrix algebra,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a Ph.D. candidate at Tilburg University working on multiple imputation algorithms for survey data."
  },
  {
    "objectID": "series-pca.html",
    "href": "series-pca.html",
    "title": "A primer on PCA",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html",
    "href": "posts/series-sm-course/se-resid.html",
    "title": "Understanding the residual standard error",
    "section": "",
    "text": "The residual standard error is a measure of fit for linear regression models. Conceptually, it can be thought of as the variability of the prediction error for a linear model. It is usually calculated as:\n\\[\nSE_{resid} = \\sqrt{ \\frac{ \\sum^{n}_{i = 1}(y_i - \\hat{y}_i)^2 }{df_{resid}} }\n\\]\nwhere:\n\n\\(n\\) is the sample size\n\\(k\\) is the number of parameters to estimate in the model\n\\(-1\\) is the degree of freedom lost to estimate the intercept\n\\(\\hat{y}_i\\) is the fitted \\(y\\) value for the \\(i\\)-th individual\n\\(df_{resid}\\) is the degrees of freedom of the residuals (\\(n - k - 1\\))\n\nThe smaller the residual standard error, the better the model fits the data."
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#introduction",
    "href": "posts/series-sm-course/se-resid.html#introduction",
    "title": "Understanding the residual standard error",
    "section": "",
    "text": "The residual standard error is a measure of fit for linear regression models. Conceptually, it can be thought of as the variability of the prediction error for a linear model. It is usually calculated as:\n\\[\nSE_{resid} = \\sqrt{ \\frac{ \\sum^{n}_{i = 1}(y_i - \\hat{y}_i)^2 }{df_{resid}} }\n\\]\nwhere:\n\n\\(n\\) is the sample size\n\\(k\\) is the number of parameters to estimate in the model\n\\(-1\\) is the degree of freedom lost to estimate the intercept\n\\(\\hat{y}_i\\) is the fitted \\(y\\) value for the \\(i\\)-th individual\n\\(df_{resid}\\) is the degrees of freedom of the residuals (\\(n - k - 1\\))\n\nThe smaller the residual standard error, the better the model fits the data."
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#learn-by-coding",
    "href": "posts/series-sm-course/se-resid.html#learn-by-coding",
    "title": "Understanding the residual standard error",
    "section": "Learn by coding",
    "text": "Learn by coding\nWe can compute the residual standard error manually after estimating a linear model in R. To get a better grasp of the residual standard error, let’s start by regressing the miles per gallon (mpg) on the number of cylinders (cyl), horsepower (hp), and weight (wt) of cars from the standard mtcars R dataset.\n\n# Fit a linear model -----------------------------------------------------------\n\n  lm_fit &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\nWe can compute the residual standard error following the formula described above:\n\n# Compute the residual standard error manually ---------------------------------\n\n  # Define elements of the formula\n  n &lt;- nrow(mtcars) # sample size\n  k &lt;- 3            # number of parameters (regression coefficients)\n  yhat &lt;- fitted(lm_fit) # fitted y values\n  y &lt;- mtcars$mpg\n\n  # Compute rse\n  rse &lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))\n\n  # Print rse\n  rse\n\n[1] 2.511548\n\n\nWe can also extract it directly from any lm object:\n\n# residual standard error from lm output ---------------------------------------\n\n  # Use the sigma function to extract it from an lm object\n  sigma(lm_fit)\n\n[1] 2.511548\n\n  # Compare with the manual computation\n  sigma(lm_fit) - rse\n\n[1] 0"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#tldr-just-give-me-the-code",
    "href": "posts/series-sm-course/se-resid.html#tldr-just-give-me-the-code",
    "title": "Understanding the residual standard error",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Fit a linear model -----------------------------------------------------------\n\n  lm_fit &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\n# Compute the residual standard error manually ---------------------------------\n\n  # Define elements of the formula\n  n &lt;- nrow(mtcars) # sample size\n  k &lt;- 3            # number of parameters (regression coefficients)\n  yhat &lt;- fitted(lm_fit) # fitted y values\n  y &lt;- mtcars$mpg\n\n  # Compute rse\n  rse &lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))\n\n  # Print rse\n  rse\n\n# residual standard error from lm output ---------------------------------------\n\n  # Use the sigma function to extract it from an lm object\n  sigma(lm_fit)\n\n  # Compare with the manual computation\n  sigma(lm_fit) - rse"
  },
  {
    "objectID": "posts/series-sm-course/se-resid.html#other-resources",
    "href": "posts/series-sm-course/se-resid.html#other-resources",
    "title": "Understanding the residual standard error",
    "section": "Other resources",
    "text": "Other resources\n\nStatology: How to Interpret Residual Standard Error\nStatology: How to Calculate Residual Standard Error in R"
  },
  {
    "objectID": "posts/series-knowledge/crossentropy.html",
    "href": "posts/series-knowledge/crossentropy.html",
    "title": "Cross-entropy as a measure of predictive performance",
    "section": "",
    "text": "Introduction\nCross-entropy (CE) quantifies the difference between two probability distributions. As such, it comes in handy as a loss function in multi-class classification tasks (e.g., multinomial logistic regression). It also provides an elegant solution for determining the difference between actual and predicted categorical data point values. It can be used to determine the predictive performance of a classification model. The value of the cross-entropy is higher when the predicted classes diverge more from the true labels.\n\n\nLearn by coding\nIn a multiclass-classification task, we calculate a separate “loss” for each class for each observation and sum the result:\n\\[\nCE = - \\sum^{N}_{i = 1} \\sum^{K}_{k = 1} p_{(i, k)}log(\\hat{p}_{(i, k)}) (\\#eq:CE)\n\\]\nwhere\n\n\\(N\\) is the sample size.\n\\(K\\) is the number of categories of the variable we are trying to predict.\n\\(p\\) is a scalar taking value \\(0 = \\text{no}\\) or \\(1 = \\text{yes}\\) to indicate whether observation \\(i\\) belongs to class \\(k\\). This can also be thought of as the true probability of the observation belonging to that class.\n\\(\\hat{p}\\) is a scalar indicating the predicted probability of observation \\(i\\) belonging to class \\(k\\).\n\\(log\\) is the natural logarithm.\n\nLet’s see an example in R. The iris data records the petal and sepal dimensions for 150 and their species. Consider the task of predicting the flowers’ species based on all the numeric predictors available. We will fit a multinomial logistic regression on the data and compute the cross-entropy between the observed and predicted class membership.\nTo start, we should prepare the R environment by loading a few packages we will use:\n\nnnet to estimate the multinomial logistic model;\nMLmetric to check someone else’s implementation of the cross-entropy computation.\nFactoMineR to create a disjunctive table from an R factor\n\n\n# Prepare environment ----------------------------------------------------------\n\n# Packages\nlibrary(nnet)\nlibrary(MLmetrics)  # for LogLoss() function\nlibrary(FactoMineR) # for tab.disjonctif() function\n\n# Default rounding for this sessino\noptions(\"digits\" = 5)\n\nThen, we should estimate the multinomial logistic model of interest. We will use this model to create predictions.\n\n# Fit mulinomial logistic model ------------------------------------------------\n\n# Fit model\nglm_mln &lt;- multinom(Species ~ Sepal.Length, data = iris)\n\nWe can now create two R matrices p and p_hat storing all the scalars \\(p_{ik}\\) and \\(\\hat{p}_{ik}\\) we need to compute @ref(eq:CE).\n\nFirst, we want to store all the \\(p_{ik}\\) in one matrix. To do so, we can create a disjunctive table based on the species factor. This is an \\(N \\times K\\) matrix storing 0s and 1s to indicate which class every observation belongs to.\n\n# Obtain p and p_har -----------------------------------------------------------\n\n# store true labels in a matrix p\np &lt;- FactoMineR::tab.disjonctif(iris$Species)\n\n# check it\nhead(p)\n\n  setosa versicolor virginica\n1      1          0         0\n2      1          0         0\n3      1          0         0\n4      1          0         0\n5      1          0         0\n6      1          0         0\n\n\nSecond, we want to obtain the predicted class probabilities for every observation:\n\n# obtain predictions\np_hat &lt;- predict(glm_mln, type = \"probs\")\n\n# check it\nhead(p_hat)\n\n   setosa versicolor virginica\n1 0.80657   0.176155 0.0172792\n2 0.91844   0.076558 0.0050018\n3 0.96787   0.030792 0.0013399\n4 0.98005   0.019262 0.0006841\n5 0.87281   0.117765 0.0094276\n6 0.47769   0.442466 0.0798435\n\n\n\nWe can now write a loop to perform the computation in @ref(eq:CE) for every \\(i\\) and \\(k\\).\n\n# Compute CE with a loop -------------------------------------------------------\n\n# Define parameters\nN &lt;- nrow(iris) # sample size\nK &lt;- nlevels(iris$Species) # number of classes\n\n# Create storing object for CE\nCE &lt;- 0\n\n# Compute CE with a loop\nfor (i in 1:N){\n  for (k in 1:K){\n    CE &lt;- CE - p[i, k] * log(p_hat[i, k])\n  }\n}\n\n# Print the value of CE\nCE\n\n[1] 91.034\n\n\nWe can also work with the matrices p and p_hat directly to avoid using a loop:\n\n# Compute CE using the matrices directly ---------------------------------------\nce &lt;- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n[1] 91.034\n\n\nThis approach works for a binary prediction just as well. We only need to pay attention to storing the true and predicted probabilities in matrix form. For example, consider the task of predicting the transmission type (automatic or not) for the cars recorded in the mtcars dataset.\n\n# Binary cross entropy ---------------------------------------------------------\n\n# Fit model\nglm_log &lt;- glm(am ~ hp + wt,\n               family = binomial(link = 'logit'),\n               data = mtcars)\n\n# store true labels in a matrix p\np &lt;- FactoMineR::tab.disjonctif(as.factor(mtcars$am))\n\n# obtain predicted probabilites in matrix form\npred_probs &lt;- predict(glm_log, type = \"response\")\np_hat &lt;- cbind(k_0 = 1 - pred_probs,\n               k_1 = pred_probs)\nclass(p_hat)\n\n[1] \"matrix\" \"array\" \n\n\nThe objects p and p_hat are all the information we need to compute the cross-entropy for this binary prediction task:\n\n# check the first few rows of p\nhead(p)\n\n  0 1\n1 0 1\n2 0 1\n3 0 1\n4 1 0\n5 1 0\n6 1 0\n\n# check the first few rows of p_hat\nhead(p_hat)\n\n                       k_0       k_1\nMazda RX4         0.157664 0.8423355\nMazda RX4 Wag     0.595217 0.4047825\nDatsun 710        0.029759 0.9702408\nHornet 4 Drive    0.958272 0.0417280\nHornet Sportabout 0.930612 0.0693881\nValiant           0.995012 0.0049882\n\n\nWe can use these new objects to obtain the binary CE with the same computation we used for the multiclass CE:\n\n# Compute CE using the matrices directly\nce &lt;- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n[1] 5.0296\n\n\nIt is not uncommon to divide the value of the cross-entropy by the number of units on which the computation is performed, effectively producing an average loss across the units.\n\n# Express as average\nce / nrow(mtcars)\n\n[1] 0.15717\n\n\nJust to be sure, we can use the LogLoss() function from the MLmetrics package to compute the same binary CE. However, this function requires the true and predicted probabilities to be stored as vectors instead of matrices. So first we need to obtain the vector versions of p and p_hat.\n\n# Compute binary CE with MLmetrics implementation ------------------------------\n\n# Obtain vector of true probabilities\np_vec &lt;- mtcars$am\n\n# Obtain vector of predicted probabilities\np_hat_vec &lt;- predict(glm_log, type = \"response\")\n\nand then we can simply provide these objects to the LogLoss() function:\n\n# Compute and print binary CE with MLmetrics implementation\nMLmetrics::LogLoss(y_pred = p_hat_vec,\n                   y_true = p_vec)\n\n[1] 0.15717\n\n\n\n\nTL;DR, just give me the code!\n\n# Prepare environment ----------------------------------------------------------\n\n# Packages\nlibrary(nnet)\nlibrary(MLmetrics)  # for LogLoss() function\nlibrary(FactoMineR) # for tab.disjonctif() function\n\n# Default rounding for this sessino\noptions(\"digits\" = 5)\n\n# Fit mulinomial logistic model ------------------------------------------------\n\n# Fit model\nglm_mln &lt;- multinom(Species ~ Sepal.Length, data = iris)\n\n# Obtain p and p_har -----------------------------------------------------------\n\n# store true labels in a matrix p\np &lt;- FactoMineR::tab.disjonctif(iris$Species)\n\n# check it\nhead(p)\n\n# obtain predictions\np_hat &lt;- predict(glm_mln, type = \"probs\")\n  \n# check it\nhead(p_hat)\n\n# Compute CE with a loop -------------------------------------------------------\n\n# Define parameters\nN &lt;- nrow(iris) # sample size\nK &lt;- nlevels(iris$Species) # number of classes\n\n# Create storing object for CE\nCE &lt;- 0\n\n# Compute CE with a loop\nfor (i in 1:N){\n  for (k in 1:K){\n    CE &lt;- CE - p[i, k] * log(p_hat[i, k])\n  }\n}\n\n# Print the value of CE\nCE\n\n# Compute CE using the matrices directly ---------------------------------------\nce &lt;- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n# Binary cross entropy ---------------------------------------------------------\n\n# Fit model\nglm_log &lt;- glm(am ~ hp + wt,\n               family = binomial(link = 'logit'),\n               data = mtcars)\n\n# store true labels in a matrix p\np &lt;- FactoMineR::tab.disjonctif(as.factor(mtcars$am))\n\n# obtain predicted probabilites in matrix form\npred_probs &lt;- predict(glm_log, type = \"response\")\np_hat &lt;- cbind(k_0 = 1 - pred_probs,\n               k_1 = pred_probs)\nclass(p_hat)\n# check the first few rows of p\nhead(p)\n\n# check the first few rows of p_hat\nhead(p_hat)\n\n# Compute CE using the matrices directly\nce &lt;- -sum(diag(p %*% t(log(p_hat))))\n\n# Print the value of ce\nce\n\n# Express as average\nce / nrow(mtcars)\n\n# Compute binary CE with MLmetrics implementation ------------------------------\n\n# Obtain vector of true probabilities\np_vec &lt;- mtcars$am\n\n# Obtain vector of predicted probabilities\np_hat_vec &lt;- predict(glm_log, type = \"response\")\n\n# Compute and print binary CE with MLmetrics implementation\nMLmetrics::LogLoss(y_pred = p_hat_vec,\n                   y_true = p_vec)\n\n\n\nOther resources\n\nCross-entropy in RPubs\nA Gentle Introduction to Cross-Entropy for Machine Learning\nUnderstanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names\nML Gloassary\nLoss Functions in Machine Learning"
  },
  {
    "objectID": "posts/series-knowledge/covmatwt.html",
    "href": "posts/series-knowledge/covmatwt.html",
    "title": "Estimating the weighted covariance matrix in R",
    "section": "",
    "text": "In a sample made of groups of different sizes, descriptive statistics like the mean and the covariance between variables can be computed by assigning proper weights to account for the difference in group sizes. Wights are generally normalized (i.e., \\(\\sum_{i = 1}^{n} w_i = 1\\))."
  },
  {
    "objectID": "posts/series-knowledge/covmatwt.html#example",
    "href": "posts/series-knowledge/covmatwt.html#example",
    "title": "Estimating the weighted covariance matrix in R",
    "section": "Example",
    "text": "Example\nNow, let’s consider a very simple example. Say that you have a dataset with two variables and that you have a vector of weights defining how important each observation should be.\n\n# Initial simple example -------------------------------------------------------\n\n  # Get the dataset used in the example of stats::cov.wt()\n  xy &lt;- cbind(x = 1:10, y = c(1:3, 8:5, 8:10))\n\n  # Define non-negative weights (as in example of stats::cov.wt())\n  wi &lt;- c(0,0,0,1,1,1,1,1,0,0)\n\n  # Get the weighted estimate with the default methods\n  covwt_stats &lt;- stats::cov.wt(xy, wt = wi) # i.e. method = \"unbiased\"\n\n  # Compare unweighted and weighted means\n  data.frame(uw = colMeans(xy),\n             select = colMeans(xy[wi == 1, ]),\n             wg = covwt_stats$center)\n\n   uw select  wg\nx 5.5    6.0 6.0\ny 5.9    6.8 6.8\n\n  # Compare unweighted and weighted covariance matrix\n  data.frame(uw = c(cov(xy)),\n             select = c(cov(xy[wi == 1, ])),\n             wg = c(covwt_stats$cov),\n             row.names = c(sapply(colnames(cov(xy)), paste0, rownames(cov(xy))))\n  )\n\n         uw select   wg\nxx 9.166667    2.5  2.5\nxy 8.055556   -0.5 -0.5\nyx 8.055556   -0.5 -0.5\nyy 9.433333    1.7  1.7\n\n\nNote how by weighting with a vector of 0 and 1s we are basically saying that the observations with a 0 will be excluded from the count. They are weighted to have 0 impact on the computation of the descriptive statistics. This is clear when you compare the results of the select and wg columns."
  },
  {
    "objectID": "posts/series-knowledge/covmatwt.html#computing-the-weighted-covariance-matrix-manually",
    "href": "posts/series-knowledge/covmatwt.html#computing-the-weighted-covariance-matrix-manually",
    "title": "Estimating the weighted covariance matrix in R",
    "section": "Computing the weighted covariance matrix manually",
    "text": "Computing the weighted covariance matrix manually\nWe could replicate the results of the weighting simply by selecting a subset of the original data because all observations were either weighted 0 or equally (1). When this is not the case, weighting is slightly more complicated.\n\nExploring the stats::cov.wt() function code\nLet’s look at how the cov.wt() function works more in depth. The internal code of the function is the following:\n\n# Examine the internal code of stats::cov.wt() ---------------------------------\n\n  cov.wt\n\nfunction (x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE, \n    method = c(\"unbiased\", \"ML\")) \n{\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    else if (!is.matrix(x)) \n        stop(\"'x' must be a matrix or a data frame\")\n    if (!all(is.finite(x))) \n        stop(\"'x' must contain finite values only\")\n    n &lt;- nrow(x)\n    if (with.wt &lt;- !missing(wt)) {\n        if (length(wt) != n) \n            stop(\"length of 'wt' must equal the number of rows in 'x'\")\n        if (any(wt &lt; 0) || (s &lt;- sum(wt)) == 0) \n            stop(\"weights must be non-negative and not all zero\")\n        wt &lt;- wt/s\n    }\n    if (is.logical(center)) {\n        center &lt;- if (center) \n            colSums(wt * x)\n        else 0\n    }\n    else {\n        if (length(center) != ncol(x)) \n            stop(\"length of 'center' must equal the number of columns in 'x'\")\n    }\n    x &lt;- sqrt(wt) * sweep(x, 2, center, check.margin = FALSE)\n    cov &lt;- switch(match.arg(method), unbiased = crossprod(x)/(1 - \n        sum(wt^2)), ML = crossprod(x))\n    y &lt;- list(cov = cov, center = center, n.obs = n)\n    if (with.wt) \n        y$wt &lt;- wt\n    if (cor) {\n        Is &lt;- 1/sqrt(diag(cov))\n        R &lt;- cov\n        R[] &lt;- Is * cov * rep(Is, each = nrow(cov))\n        y$cor &lt;- R\n    }\n    y\n}\n&lt;bytecode: 0x7fa3526d4b58&gt;\n&lt;environment: namespace:stats&gt;\n\n\nNote the following:\n\nThe first thing to pay attention to is that the function can compute the weighted covariance matrix in two ways:\n\nunbiased, using corssprod(x) / (1 - sum(wt^2))\nML (or maximum likelihood), using corssprod(x)\n\nNote that the wt object is divided by the sum of the values it is storing, which amounts to normalising the weights. This happens with wt &lt;- wt/s with s being created inside an if statement as s &lt;- sum(wt).\nx is centered on the normalized weigthed means using the sweep function\nx is weighted by multiplying by sqrt(wt)\n\n\n\nReproducing the internal steps\nFirst, we’ll set up a few objects we need to replicate manually what happens inside the stats::cov.wt() function. We need to define a dataset, a vector of weights, a method to compute descriptives, and based on these we will also create an object to store the number of rows (n). As a vector of weights we sample random values between 0 and 1. We can think of this as an attempt to weight each observation for the probability of sampling them from a population.\n\n# Set up manual computation of cov.wt() ----------------------------------------\n\n  # Assign values to the function arguments\n  x      &lt;- xy                     # data\n  set.seed(20220314)\n  wi     &lt;- runif(length(wi), min = 0, max = 1)\n  method &lt;- \"ML\"                   # use Maximum Likelihood for estimation\n\n  # Assign values to some of the internal objects\n  n &lt;- nrow(x)\n\nNext, we want to make sure we normalize the weights. In other words we want to make sure the weights sum to 1.\n\n# Normalize weights ------------------------------------------------------------\n\n  # Normalise weights (to sum to 1)\n  wn &lt;- wi / sum(wi)\n\n  # Check they sum to 1\n  sum(wn) == 1\n\n[1] TRUE\n\n\nThen, we want to compute the vector of weighted means.\n\n# Compute the weighted means ---------------------------------------------------\n\n  # Center on weighted mean if required\n  center &lt;- colSums(wn * x)\n\n  # Center X on the weigthed mean\n  x_cent &lt;- sweep(x, 2, center, check.margin = FALSE)\n\n  # Note that the sweep is subtracting the \"center\" to each value\n  all.equal(\n    sweep(x, 2, center, check.margin = FALSE),\n    t(apply(x, 1, function (i) i - center))\n  )\n\n[1] TRUE\n\n\nNote that the weighted mean is computed as: [ {x} = _{i = 1}^{n} w_i x_i ] and that center &lt;- colSums(wn * x) is doing exactly this.\nFinally, we want to compute the weighted covariance matrix.\n\n# Compute the weighted covariance matrix ---------------------------------------\n\n  # Weight (centered) data\n  x_weighted &lt;- sqrt(wn) * x_cent\n\n  # Compute the ML weigthed covariance matrix manually\n  covwt_man &lt;- crossprod(x_weighted)\n\n  # Print the manual weigthed covariance matrix\n  covwt_man\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Compute the ML weigthed covariance matrix with stats::cov.wt()\n  covwt_stats &lt;- cov.wt(xy, wt = wi, method = \"ML\", center = TRUE)$cov\n\n  # Compare manual and stats weigthed covariance matrices\n  covwt_man - covwt_stats\n\n  x y\nx 0 0\ny 0 0"
  },
  {
    "objectID": "posts/series-knowledge/covmatwt.html#mathematical-formula-and-alternative-r-computations",
    "href": "posts/series-knowledge/covmatwt.html#mathematical-formula-and-alternative-r-computations",
    "title": "Estimating the weighted covariance matrix in R",
    "section": "Mathematical formula and alternative R computations",
    "text": "Mathematical formula and alternative R computations\n\nUnbiased weighted covariance matrix\nFor a given population covariance matrix \\(Q\\), each element \\(q_{ik}\\) of the unbiased estimation of the weighted covariance matrix \\(\\hat{Q}\\) can be computed with the following formula:\n[ q_{ik} = {i = 1}^{n} w_i (x{ij} - {x}j) (x{ij} - {x}_k) ]\nwith \\(\\bar{x}_j\\) being the weighted mean for variable \\(j\\), and \\(w_i\\) being the normalized weight for a given observation (which we store in the vector wn). The following are alternative ways of computing it with mathematical or R shortcuts:\n\n# Alternative computations of the unbiased weighted covariance mat -------------\n\n  # Literal translation of equation\n  1 / (1 - sum(wn^2)) * t(wn * x_cent) %*% (x_cent)\n\n         x        y\nx 8.320767 6.363485\ny 6.363485 7.275922\n\n  # Rearrange denominator\n  t(wn * x_cent) %*% (x_cent) / (1 - sum(wn^2))\n\n         x        y\nx 8.320767 6.363485\ny 6.363485 7.275922\n\n  # Spread wn\n  t(sqrt(wn) * x_cent) %*% (sqrt(wn)*x_cent) / (1 - sum(wn^2))\n\n         x        y\nx 8.320767 6.363485\ny 6.363485 7.275922\n\n  # Replace manual cross-product with R cross-product\n  crossprod(sqrt(wn) * x_cent)/(1 - sum(wn^2))\n\n         x        y\nx 8.320767 6.363485\ny 6.363485 7.275922\n\n  # Compute with stats::cov.wt()\n  cov.wt(xy, wt = wi, method = \"unbiased\", center = TRUE)$cov\n\n         x        y\nx 8.320767 6.363485\ny 6.363485 7.275922\n\n\n\n\nMaximum Likelihood weighted covariance matrix\nEach element \\(q_{ik}\\) of the maximum likelihood weighted covariance matrix estimate \\(\\hat{Q}\\) can be computed manually with the following formula:\n[ q_{ik} = {i = 1}^{n} w_i (x{ij} - {x}j) (x{ij} - {x}_k) ]\nwith \\(\\bar{x}_j\\) being the weighted mean for variable \\(j\\), and \\(w_i\\) being the normalized weight for a given observation. The following are alternative ways of computing it with mathematical or R shortcuts:\n\n# Alternative computations of the ML weighted covariance mat -------------------\n\n  # R manual cross-product using un-normalised weights\n  1 / sum(wi) * t(wi * x_cent) %*% (x_cent)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Using the normalised weights\n  1 / sum(wn) * t(wn * x_cent) %*% (x_cent)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Dropp the term = 1\n  t(wn * x_cent) %*% (x_cent)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Spread wn\n  t(sqrt(wn) * x_cent) %*% (sqrt(wn) * x_cent)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Replace manual cross-product with R cross-product\n  crossprod(sqrt(wn) * x_cent)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # R cross-product matrix\n  crossprod(x_weighted)\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209\n\n  # Compute with stats::cov.wt()\n  cov.wt(xy, wt = wi, method = \"ML\", center = TRUE)$cov\n\n         x        y\nx 7.102015 5.431419\ny 5.431419 6.210209"
  },
  {
    "objectID": "posts/series-knowledge/covmatwt.html#relationship-with-the-matrix-of-sufficient-statistics",
    "href": "posts/series-knowledge/covmatwt.html#relationship-with-the-matrix-of-sufficient-statistics",
    "title": "Estimating the weighted covariance matrix in R",
    "section": "Relationship with the matrix of sufficient statistics",
    "text": "Relationship with the matrix of sufficient statistics\nNote the relationship between the covariance matrix and the matrix of sufficient statistics is the same as in the unweighted case: \\(\\text{cov} = \\frac{T_{\\text{obs}}}{n}\\).\n\n# Obtain the matrix of sufficient statistics Tobs ------------------------------\n\n  # Define a new weigthing object\n  set.seed(20220314)\n  wi &lt;- runif(length(wi), min = 0, max = 1)\n  wn &lt;- wi / sum(wi)\n\n  # Compute the weighted means of X again\n  center &lt;- colSums(wn * x)\n  x_cent &lt;- sweep(x, 2, center, check.margin = FALSE)\n\n  # \"Effective\" sample size\n  n &lt;- sum(wi)\n\n  # Number of columns\n  p &lt;- ncol(x)\n\n  # Obtain matrix of sufficient statistics (Tobs)\n  Tobs_lopp &lt;- matrix(0, p, p)\n  for(i in 1:nrow(x)){\n    Tobs_lopp &lt;- Tobs_lopp + wi[i] * (x_cent[i, ]) %*% t(x_cent[i, ])\n  }\n\n  # Obtain matrix of sufficient statistics (Tobs) w/ cross-product shortcut\n  Tobs_cp &lt;- t(wi * x_cent) %*% x_cent\n\n  # Compare loop version and cross-product shortcut\n  Tobs_lopp - Tobs_cp\n\n     x            y\n[1,] 0 3.552714e-15\n[2,] 0 0.000000e+00\n\n  # Assign simpler name and print Tobs\n  (Tobs &lt;- Tobs_cp)\n\n         x        y\nx 31.08417 23.77229\ny 23.77229 27.18091\n\n  # Convert to a covariance matrix\n  covmat &lt;- Tobs / n\n\n  # Check it's what you were expecting\n  covmat - cov.wt(xy, wt = wi, method = \"ML\", center = TRUE)$cov\n\n  x             y\nx 0 -8.881784e-16\ny 0 -8.881784e-16\n\n\nNote the following:\n\nwe are using the normalized weights wn to center the data, but we are using the un-normalised weights to scale the data contribution to Tobs\nif we had used the normalized weights, \\(n\\) would have been equal to 1 and covmat would be equal to Tobs.\n\n\n# Obtain the matrix of sufficient statistics Tobs (normalised weights) ---------\n\n  # Convert to a covariance matrix\n  covmat - t(wn * x_cent) %*% x_cent\n\n             x y\nx 0.000000e+00 0\ny 8.881784e-16 0\n\n  # Then, covmat relates to Tobs as\n  (t(wn * x_cent) %*% x_cent * n) - Tobs_cp\n\n              x y\nx  0.000000e+00 0\ny -3.552714e-15 0\n\n  # So we could say\n  Tobs &lt;- t(wn * x_cent) %*% x_cent * n"
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html",
    "href": "posts/series-pca-primer/pca-biplots.html",
    "title": "How to obtain PCA biplots",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an \\(n \\times p\\) data matrix \\(\\mathbf{X}\\) with minimal loss of information. In terms of visualization techniques, PCA allows the representation of large datasets in a bi-dimensional plot. In general, biplots give use a simultaneous representation of \\(n\\) observations and \\(p\\) variables on a single bi-dimensional plot. More precisely, biplots represent the scatterplot of the observations on the first two principal components computed by PCA and the relative position of the \\(p\\) variables in a two-dimensional space. For an in-depth discussion, I recommend reading Jolliffe (2002, 90)."
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#introduction",
    "href": "posts/series-pca-primer/pca-biplots.html#introduction",
    "title": "How to obtain PCA biplots",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an \\(n \\times p\\) data matrix \\(\\mathbf{X}\\) with minimal loss of information. In terms of visualization techniques, PCA allows the representation of large datasets in a bi-dimensional plot. In general, biplots give use a simultaneous representation of \\(n\\) observations and \\(p\\) variables on a single bi-dimensional plot. More precisely, biplots represent the scatterplot of the observations on the first two principal components computed by PCA and the relative position of the \\(p\\) variables in a two-dimensional space. For an in-depth discussion, I recommend reading Jolliffe (2002, 90)."
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#learn-by-coding",
    "href": "posts/series-pca-primer/pca-biplots.html#learn-by-coding",
    "title": "How to obtain PCA biplots",
    "section": "Learn by coding",
    "text": "Learn by coding\nLet us start by loading the ggfortify R package, which provides pleasant looking biplots. We will work with the first four columns of the iris data, which report the length and width of petals and sepals in plants for the iris flowering plant.\n\n# Prepare environment ----------------------------------------------------------\n\n# Load packages (install if you don't have it)\nlibrary(ggfortify) # for default biplots\n\n# Keep the numeric variables from the iris dataset.\nX &lt;- iris[1:4]\n\nWe then use the prcomp R function to compute the PCs of X. We specify the prcomp function to not scale and standardize the data because we rather have more control over how the standardization is performed.\n\n# Perform PCA ------------------------------------------------------------------\n\n# Center and standardize the data\nX_sc &lt;- scale(X)\n\n# Compute PCs\npca_res &lt;- prcomp(X_sc, center = FALSE, scale. = FALSE)\n\n# Generate default biplot with ggfortify\nautoplot(pca_res,\n    data = X,\n    loadings.label = TRUE, \n    loadings.colour = \"blue\"\n)\n\n\n\n\nand print the biplot obtained with the ggfortify::autoplot() function. We specify which data should be plotted (data = X), and we require the loadings to be included in the plot as blue arrows (loadings.colour = \"blue\" and loadings.label = TRUE.)\nNow we replicate this plot by doing all the work ourselves. First, let’s start by computing the PC scores ourselves by taking the singular value decomposition of X and computing the PC scores \\(T\\).\n\n# Getting to the biplots -------------------------------------------------------\n\n# SVD of X\nx_svd &lt;- svd(X_sc)\n\n# Extract the parts of the SVD we need to compute the Principal Component scores\nU &lt;- x_svd$u\nD &lt;- diag(x_svd$d)\nV &lt;- x_svd$v\n\n# Compute the PCs\nT &lt;- U %*% D\n\nWe can now make a scatter plot of the observations based on how they score on the first two PCs we computed.\n\n# &gt; Simple scatter plot --------------------------------------------------------\n\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n\n\n\nThe next step is to overlay the arrows plotting the information regarding the loadings. We want to plot a single arrow for each column of the original data, starting at the center of the plot (\\(PC1 = 0\\), and \\(PC2 = 0\\)) and ending at the coordinates described by the first two columns of the component loadings matrix \\(V\\).\n\n# &gt; Adding loading arrows ------------------------------------------------------\n\n# Scatter plot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1], \n    y0 = rep(0, ncol(X)), y1 = V[, 2]\n)\n\n\n\n\nWe can improve the visualization by adding a few teaks. Let’s add a title, make the scatterplot symbols gray solid dots, scale up the arrow size, and add labels indicating which loading we are plotting with any given arrow.\n\n# &gt; Improving the visuals ------------------------------------------------------\n\n# Define a scaling factor for the arrows\nsf &lt;- 2\n\n# Start over with the original scatterplot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\",\n    main = \"PCA biplot\", # plot title\n    col = \"gray\",\n    pch = 19 # solid circle\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1] * sf, \n    y0 = rep(0, ncol(X)), y1 = V[, 2] * sf,\n    col = \"darkgray\"\n)\n\n# Add names of variables per arrow\ntext(x = V[, 1] * sf, y = V[, 2] * sf, labels = colnames(X))\n\n\n\n\nThe scaling we performed is ad-hoc. We looked at the plot and decided to increase the size of the arrows by an arbitrary scaling factor. In the literature on biplots it is more common to scale both component scores and loadings by taking powers of the diagonal matrix \\(D\\) and recomputing the coordinates1."
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#tldr-just-give-me-the-code",
    "href": "posts/series-pca-primer/pca-biplots.html#tldr-just-give-me-the-code",
    "title": "How to obtain PCA biplots",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Prepare environment ----------------------------------------------------------\n\n# Load packages (install if you don't have it)\nlibrary(ggfortify) # for default biplots\n\n# Keep the numeric variables from the iris dataset.\nX &lt;- iris[1:4]\n\n# Perform PCA ------------------------------------------------------------------\n\n# Center and standardize the data\nX_sc &lt;- scale(X)\n\n# Compute PCs\npca_res &lt;- prcomp(X_sc, center = FALSE, scale. = FALSE)\n\n# Generate default biplot with ggfortify\nautoplot(pca_res,\n    data = X,\n    loadings.label = TRUE, \n    loadings.colour = \"blue\"\n)\n\n# Getting to the biplots -------------------------------------------------------\n\n# SVD of X\nx_svd &lt;- svd(X_sc)\n\n# Extract the parts of the SVD we need to compute the Principal Component scores\nU &lt;- x_svd$u\nD &lt;- diag(x_svd$d)\nV &lt;- x_svd$v\n\n# Compute the PCs\nT &lt;- U %*% D\n\n# &gt; Simple scatter plot --------------------------------------------------------\n\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# &gt; Adding loading arrows ------------------------------------------------------\n\n# Scatter plot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1], \n    y0 = rep(0, ncol(X)), y1 = V[, 2]\n)\n\n# &gt; Improving the visuals ------------------------------------------------------\n\n# Define a scaling factor for the arrows\nsf &lt;- 2\n\n# Start over with the original scatterplot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\",\n    main = \"PCA biplot\", # plot title\n    col = \"gray\",\n    pch = 19 # solid circle\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1] * sf, \n    y0 = rep(0, ncol(X)), y1 = V[, 2] * sf,\n    col = \"darkgray\"\n)\n\n# Add names of variables per arrow\ntext(x = V[, 1] * sf, y = V[, 2] * sf, labels = colnames(X))"
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#footnotes",
    "href": "posts/series-pca-primer/pca-biplots.html#footnotes",
    "title": "How to obtain PCA biplots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html↩︎"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html",
    "href": "posts/series-pca-primer/pcovr.html",
    "title": "Principal covariates regression in R",
    "section": "",
    "text": "Principal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables. Principal covariates regression (PCovR) is an alternative approach that modifies the optimization criteria behind PCA to include a supervision aspect . PCovR looks for a low-dimensional representation of \\(X\\) that accounts for the maximum amount of variation in both \\(X\\) and \\(y\\). Compared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of \\(X\\) variables and that correlate well with a set of \\(Y\\) variables. For more information, I recommend reading Vervloet et al. (2015) and De Jong and Kiers (1992). In this post, you can find my R code notes on this method. In these notes, I show the computations used by the PCovR R-package to perform the method.\nTo understand how PCovR differs from classical PCR we need to complicate the notation. Consider the following decomposition of the data:\n\\[\n\\begin{align}\n    \\mathbf{T} &= \\mathbf{X} \\mathbf{W} \\\\\n    \\mathbf{X} &= \\mathbf{T} \\mathbf{P}_X + \\mathbf{E}_X  \\\\\n    y      &= \\mathbf{T} \\mathbf{P}_y + \\mathbf{e}_y\n\\end{align}\n\\]\nwhere \\(\\mathbf{T}\\) is the matrix of PCs defined above, \\(\\mathbf{W}\\) is a \\(p \\times q\\) matrix of component weights defining what linear combination of the columns of \\(\\mathbf{X}\\) is used to compute the components, and \\(\\mathbf{P}_X\\) and \\(\\mathbf{P}_y\\) are the \\(q \\times p\\) and \\(q \\times 1\\) loading matrices relating the variables in \\(\\mathbf{X}\\) and \\(y\\) to the component scores in \\(\\mathbf{T}\\), respectively. \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\) are the reconstruction errors, the information lost by using \\(\\mathbf{T}\\) as summary of \\(\\mathbf{X}\\).\nClassical PCA can be formulated as the task of finding the \\(\\mathbf{W}\\) and \\(\\mathbf{P}_X\\) that minimize the reconstruction error \\(\\mathbf{E}_X\\):\n\\[\\begin{equation}\n    (\\mathbf{W}, \\mathbf{P}) = \\underset{\\mathbf{W}, \\mathbf{P}_X}{\\operatorname{argmin}} \\lVert \\mathbf{X} - \\mathbf{XWP}' \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\). PCovR can be formulated as the task of minimizing a weighted combination of both \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\):\n\\[\n\\begin{equation}\\label{eq:PCovR}\n    (\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y) = \\underset{\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y}{\\operatorname{argmin  }} \\alpha \\lVert (\\mathbf{X} - \\mathbf{XWP}_X') \\rVert^2 + (1 - \\alpha) \\lVert (y - \\mathbf{XWP}_y') \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\).\nThe parameter \\(\\alpha\\) defines which reconstruction error is being prioritized. When \\(\\alpha = 1\\), the emphasis is exclusively placed on reconstructing \\(\\mathbf{X}\\), leading PCovR to PCR. When \\(\\alpha = 0.5\\), the importance of \\(\\mathbf{X}\\) and \\(y\\) is equally weighted, a case that resembles Partial least square, which we discuss below. In practice, its value can be found by cross-validation or according to a sequential procedure based on maximum likelihood principles (Vervloet et al. 2013). In particular, \\[\n\\begin{equation}\\label{eq:aml}\n    \\alpha_{ML} = \\frac{\\lVert \\mathbf{X} \\lVert^2}{\\lVert \\mathbf{X} \\lVert^2  + \\lVert y \\lVert^2 \\frac{\\hat{\\sigma}_{\\mathbf{E}_X}^2}{\\hat{\\sigma}_{e_y}^2}}\n\\end{equation}\n\\]\nwhere \\(\\hat{\\sigma}_{\\mathbf{E}_X}^2\\) can be obtained as the unexplained variance by components computed according to classical PCA and \\(\\hat{\\sigma}_{e_y}^2\\) can be estimated as the unexplained variance by the linear model regressing \\(y\\) on \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "href": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "title": "Principal covariates regression in R",
    "section": "Reverting to PCA",
    "text": "Reverting to PCA\nI mentioned before that when \\(\\alpha = 1\\), PCovR reduces to PCR. Let’s see that in action. First, we set the desired value for \\(\\alpha\\):\n\n# Reverting to PCA -------------------------------------------------------------\n\n# Use alpha 1\nalpha &lt;- 1\n\nthen, we can perform PCovR\n\n# Estimate PCovR\npackage &lt;- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,\n    r = npcs # fixed number of components\n)\n\nand classical PCA according to the Guerra-Urzola et al. (2021)\n\n# Classical PCA\nuSVt &lt;- svd(X)\nU &lt;- uSVt$u\nD &lt;- diag(uSVt$d)\nV &lt;- uSVt$v\nI &lt;- nrow(X)                              # Define the number of rows\nP_hat &lt;- (I - 1)^{-1 / 2} * V %*% D       # Component loadings\nW_hat &lt;- (I - 1)^{1 / 2} * V %*% solve(D) # Component weights\nT_hat &lt;- (I - 1)^{1 / 2} * U              # Component scores\nT_hat &lt;- X %*% W_hat\n\nWe can now compare the results again\n\n# The scores obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$Te, T_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The loadings obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(t(package$Px), P_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The weights obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$W, W_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# P is now proportional to W\nTuckerCoef(package$W, t(package$Px))$tucker_value\n\n[1] 1"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "href": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "title": "Principal covariates regression in R",
    "section": "Maximum Likelihood estimation of \\(\\alpha\\)",
    "text": "Maximum Likelihood estimation of \\(\\alpha\\)\nThe value of \\(\\alpha\\) is not usually chosen arbitrarily. One could cross-validate it or compute it with a closed-form solution that relies on the Maximum likelihood approach (Vervloet et al. 2016). Here, I show how to use this latter approach. First, let’s simply fit PCovR by using the PCovR::pcovr() function and setting the model selection argument set to “seq”. As explained in the help-file, this “implies a sequential procedure in which the weighting value is determined on the basis of maximum likelihood principles”, exactly what we want.\n\n# Maximum likelihood tuning of alpha -------------------------------------------\n\n# Fit PCovR\npackage &lt;- pcovr(\n    X = X_raw,\n    Y = y_raw,\n    rot = \"none\",\n    R = npcs, # fixed number of components\n    modsel = \"seq\" # fastest option\n)\n\nThen, we can compute the maximum likelihood value of \\(\\alpha\\) by first computing the error terms and taking their ratio.\n\n# Compute error ratio components\nlm_mod &lt;- lm(y ~ -1 + X)\nery &lt;- 1 - summary(lm_mod)$r.squared\n\nRmin &lt;- npcs\nRmax &lt;- npcs\nsing &lt;- svd(X)\nvec &lt;- Rmin:Rmax\nvec &lt;- c(vec[1] - 1, vec, vec[length(vec)] + 1)\nVAF &lt;- c(0, cumsum(sing$d^2) / sum(sing$d^2))\nVAF &lt;- VAF[vec + 1]\nscr &lt;- array(NA, c(1, length(vec)))\nfor (u in 2:(length(vec) - 1)) {\n    scr[, u] &lt;- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])\n}\nerx &lt;- 1 - VAF[which.max(scr)]\n\nWe could have computed the error ratio with the PCovR::err() R function:\n\n# Compute error ratio with function\nerr &lt;- ErrorRatio(\n    X = X,\n    Y = y,\n    Rmin = npcs,\n    Rmax = npcs\n)\n\n# Is it the same?\nerr - erx/ery\n\n[1] 1.110223e-15\n\n\nWith this value, it is now easy to compute \\(\\alpha\\):\n\n# Find alpha ML\nalpha_ML &lt;- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)\n\n# Compare to one found by package\npackage$a - alpha_ML\n\n[1] 0"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#footnotes",
    "href": "posts/series-pca-primer/pcovr.html#footnotes",
    "title": "Principal covariates regression in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCheck the helpfile for the alexithymia data in the PCovR package for more information.↩︎"
  },
  {
    "objectID": "posts/series-em/sweep.html",
    "href": "posts/series-em/sweep.html",
    "title": "The sweep operator",
    "section": "",
    "text": "The sweep operator is a matrix transformation commonly used to estimate regression models. It performs elementary row operations on a \\(p \\times p\\) matrix which happens to be particularly useful to estimate multivariate linear models. Little and Rubin (2002, p148) defined it as follows:\n\nThe sweep operator is defined for symmetric matrices as follows. A \\(p \\times p\\) symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric \\(p \\times p\\) matrix H with elements defined as follows: \\[\nh_{kk} = -1/g_{kk}\n\\] \\[\nh_{jk} = h_{kj} = \\frac{g_{jk}}{g_{kk}}, j \\neq k\n\\] \\[\nh_{jl} = g_{jl} - \\frac{g_{jk}g_{kl}}{g_{kk}}, j \\neq k, l \\neq k\n\\]\n\nThe notation indicating this transformation is usually a variation of \\(\\text{SWEEP}[k]G\\), which can be read as sweeping matrix \\(G\\) on column (and row) \\(k\\). It is important to know that:\n\nAny symmetric matrix \\(G\\) can be swept over \\(l\\) multiple positions. The notation \\(\\text{SWEEP}[k_1, k_2, ..., k_l]G\\) indicates successive applications of \\(\\text{SWEEP}[k]G\\) with \\(k = k_1, \\dots, k_l\\).\nThe sweep operator is commutative. Sweeps on multiple positions do not need to be carried out in any particular order:\n\n\\[\n\\text{SWEEP}[k_2]\\text{SWEEP}[k_1]G = \\text{SWEEP}[k_1]\\text{SWEEP}[k_2]G\n\\]\n\nThe \\(l\\) sweeping positions do not need to be consecutive. For example, \\(k_1\\) could indicate the third column and \\(k_2\\) could indicate the sixth column.\n\nIn this post, I want to show how the sweep operator can be used to estimate the parameters of any linear regressions model. If you are interested in the mathematical details, I recommend reading the full sweep operator description in Goodnight (1979, p154), Schafer (1997), or Little and Rubin (2002, p148).\nGoodnight (1979, p150) is a particularly helpful paper as it describes an easy implementation of the sweep operator. Following Goodnight, given an originally symmetric positive definite matrix G, \\(\\text{SWEEP}[k]G\\) modifies a matrix G as follows:\n\nStep 1: Let \\(D = g_{kk}\\)\nStep 2: Divide row \\(k\\) by \\(D\\).\nStep 3: For every other row \\(i \\neq k\\), let \\(B = g_{ik}\\). Subtract \\(B \\times \\text{row } k\\) from row \\(i\\). Set \\(g_{ik} = -B/D\\).\nStep 4: Set \\(g_{kk} = 1/D\\)."
  },
  {
    "objectID": "posts/series-em/sweep.html#coding-a-sweep-function-in-r",
    "href": "posts/series-em/sweep.html#coding-a-sweep-function-in-r",
    "title": "The sweep operator",
    "section": "Coding a sweep function in R",
    "text": "Coding a sweep function in R\nLet’s start by coding a simple function that performs the operations described by Goodnight (1979, p150). We want a function that takes as inputs a symmetric matrix (argument G) and a vector of positions to sweep over (argument K). The function below takes these two inputs and performs the four sweep steps for every element of K.\n\n# Write an R function implementing SWEEP(k)[G] according to Goodnight ----------\n\nsweepGoodnight &lt;- function (G, K){\n\n  for(k in K){\n    # Step 1: Let D = g_kk\n    D &lt;- G[k, k]\n\n    # Step 2: Divide row k by D.\n    G[k, ] &lt;- G[k, ] / D\n\n    # Step 3:\n    # - For every other row i != k, let B = g_ik\n    # - Subtract B \\times row k from row i.\n    # - set g_ik = -B/D.\n    for(i in 1:nrow(G)){\n      if(i != k){\n        B &lt;- G[i, k]\n        G[i, ] &lt;- G[i, ] - B * G[k, ]\n        G[i, k] &lt;- -1 * B / D\n      }\n    }\n    # Step 4: Set g_kk = 1/D\n    G[k, k] = 1/D\n  }\n\n  # Output\n  return(G)\n}\n\nLet’s check that this function returns what we want by comparing it with a function implemented by someone else.\n\n# Compare sweepGoodnight with other implementations ----------------------------\n\n# Install the `fastmatrix` package (run if you don't have it yet)\n# install.packages(\"fastmatrix\")\n\n# Load fastmatrix\nlibrary(fastmatrix)\n\n# Define an example dataset\nX &lt;- matrix(c(1, 1, 1, 1,\n              1, 2, 1, 3,\n              1, 3, 1, 3,\n              1, 1,-1, 2,\n              1, 2,-1, 2,\n              1, 3,-1, 1), ncol = 4, byrow = TRUE)\n\n# Define the G matrix\nG &lt;- crossprod(X)\n\n# Define a vector of positions to sweep over\nK &lt;- 1:3\n\n# Perform SWEEP[K]G with fastmatrix sweep.operator\nH_fm &lt;- sweep.operator(G, k = K)\n\n# Perform SWEEP[K]G with our sweepGoodnight implementation\nH_sg &lt;- sweepGoodnight(G, K = K)\n\n# Compare the two\nall.equal(H_fm, H_sg)\n\n[1] TRUE\n\n\nThe functions fastmatrix::sweep.operator() and sweepGoodnight() return the same H matrix by sweeping matrix G over the positions defined in K."
  },
  {
    "objectID": "posts/series-em/sweep.html#using-the-sweep-operator-to-estimate-regression-models",
    "href": "posts/series-em/sweep.html#using-the-sweep-operator-to-estimate-regression-models",
    "title": "The sweep operator",
    "section": "Using the sweep operator to estimate regression models",
    "text": "Using the sweep operator to estimate regression models\nTo understand how the sweep operator relates to the estimation of multivariate linear models, we will work with a data set used by Little and Rubin (2002, p152).\n\n# Load Little Rubin data -------------------------------------------------------\n\n# Create data\n  X &lt;- as.data.frame(\n          matrix(\n                  data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26,\n                           29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68,\n                           6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,\n                           60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12,\n                           78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7,\n                           72.5, 93.1, 115.9, 83.8, 113.3, 109.4),\n                  ncol = 5\n          )\n  )\n\n# Store useful information\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n\nLet’s take a quick look at the first rows of the data to get an idea of what we are working with.\n\n# Glance at the first 6 rows of the data\n  head(X)\n\n  V1 V2 V3 V4    V5\n1  7 26  6 60  78.5\n2  1 29 15 52  74.3\n3 11 56  8 20 104.3\n4 11 31  8 47  87.6\n5  7 52  6 33  95.9\n6 11 55  9 22 109.2\n\n\n\nCompute the augmented covariance matrix\nTo obtain the estimates of the regression coefficients of a multivariate linear model, we need to sweep the augmented covariance matrix of the data (\\(\\Theta\\)) over the positions of the predictors. This is a \\((p+1) \\times (p+1)\\) matrix storing the covariance matrix and the means of the dataset. It usually looks like this:\n\\[\n\\Theta =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\sigma^2_1 & ... & \\sigma_{1p}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\sigma_{1p} & ... & \\sigma^2_{p}\n\\end{bmatrix}\n\\]\nwith \\(\\mu_1, \\dots, \\mu_p\\), \\(\\sigma^2_1, \\dots, \\sigma^2_p\\), and \\(\\sigma_{jk}\\) being the means, variances, and covariances of the variables in our dataset, respectively.\nIn R, we can obtain this matrix in just a few steps starting from our dataset X:\n\nAugment the original data with a column of 1s on the left.\nWe can use the cbind() function to append a column of 1s to the left of X. Keep in mind that we need to perform matrix operations with the resulting object. Therefore, we need to make sure we are working with an R object of the class matrix instead of data.frame.\n\n# Obtain the augmented covariance matrix ---------------------------------------\n\n# Augment X\n  X_aug &lt;- cbind(int = 1, as.matrix(X))\n\n# Glance at the first 6 rows of X_aug\n  head(X_aug)\n\n     int V1 V2 V3 V4    V5\n[1,]   1  7 26  6 60  78.5\n[2,]   1  1 29 15 52  74.3\n[3,]   1 11 56  8 20 104.3\n[4,]   1 11 31  8 47  87.6\n[5,]   1  7 52  6 33  95.9\n[6,]   1 11 55  9 22 109.2\n\n\nCompute the augmented matrix of sufficient statistics \\(T\\).\n\\(T\\) is the matrix having as elements the sum of the cross-products of the columns of X_aug.\n\\[\nT =\n\\begin{bmatrix}\nn & \\sum{x_1} & ... & \\sum{x_p}\\\\\n\\sum{x_1} & \\sum{x_1^2} & ... & \\sum{x_1 x_p}\\\\\n... & ... & ... & ...\\\\\n\\sum{x_p} & \\sum{x_1 x_p} & ... & \\sum{x_p^2}\n\\end{bmatrix}\n\\]\nSince the first column of X_aug is a column of 1s, the first element of T is the number of rows in the data, the first column and rows store the sum of scores on each variable (sufficient statistics for the mean), and the other elements store the sum of products between the columns of X (sufficient statistics for the covariance matrix of X).\nIn R, we can compute it easily with the cross-product function:\n\n# Compute the matrix of sufficient statistics (T matrix)\n  Tmat &lt;- crossprod(X_aug)\n\nTransform T to G\n\\(G\\) is simply \\(T / n\\)\n\\[\nG =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\frac{\\sum{x_1^2}}{n} & ... & \\frac{\\sum{x_1 x_p}}{n}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\frac{\\sum{x_1 x_p}}{n} & ... & \\frac{\\sum{x_p^2}}{n}\n\\end{bmatrix}\n\\]\n\n# Compute G\n  G &lt;- Tmat / n\n\nCompute \\(\\Theta\\) by sweeping G over the first row and column.\nLet’s use our sweepGoodnight() function to perform SWEEP[1]G and obtain \\(\\Theta\\)\n\\[\n\\Theta =\n\\begin{bmatrix}\n1 & \\mu_1 & ... &\\mu_p\\\\\n\\mu_1 & \\sigma^2_1 & ... & \\sigma_{1p}\\\\\n... & ... & ... & ...\\\\\n\\mu_p & \\sigma_{1p} & ... & \\sigma^2_{p}\n\\end{bmatrix}\n\\]\nIn R:\n\n# Sweep G over the first position\n  Theta &lt;- sweepGoodnight(G, 1)\n\n# Check how it looks\n  Theta\n\n           int         V1         V2         V3          V4         V5\nint   1.000000   7.461538   48.15385  11.769231   30.000000   95.42308\nV1   -7.461538  31.940828   19.31361 -28.662722  -22.307692   59.68935\nV2  -48.153846  19.313609  223.51479 -12.810651 -233.923077  176.38107\nV3  -11.769231 -28.662722  -12.81065  37.869822    2.923077  -47.55621\nV4  -30.000000 -22.307692 -233.92308   2.923077  258.615385 -190.90000\nV5  -95.423077  59.689349  176.38107 -47.556213 -190.900000  208.90485\n\n# Check Theta is storing the means in the first row and column\n  colMeans(X)\n\n       V1        V2        V3        V4        V5 \n 7.461538 48.153846 11.769231 30.000000 95.423077 \n\n# Check Theta is storing the ML covariance matrix everywhere else\n  cov(X) * (n-1) / n\n\n          V1         V2         V3          V4         V5\nV1  31.94083   19.31361 -28.662722  -22.307692   59.68935\nV2  19.31361  223.51479 -12.810651 -233.923077  176.38107\nV3 -28.66272  -12.81065  37.869822    2.923077  -47.55621\nV4 -22.30769 -233.92308   2.923077  258.615385 -190.90000\nV5  59.68935  176.38107 -47.556213 -190.900000  208.90485\n\n\nPay attention to a couple of things:\n\nThe covariance matrix stored in \\(\\Theta\\) is the Maximum Likelihood version (denominator should be n instead of the default n-1)\nWe could have constructed the object Theta just by using colMeans(X) and cov(X) * (n-1) / n directly. However, it is important to note the relationship between Tmat, G, and Theta. In particular, pay attention to the fact that Theta is the result of sweeping G in the first position. When I started looking into this topic I did not understand this, and I kept sweeping Theta over the first position, resulting in a confusing double sweeping of the first column and row. I will get back to this point in a sec.\n\n\n\n\nEstimate multivariate linear models\nNow let’s see how we can use \\(\\Theta\\) to estimate any multivariate linear model involving the variables in our dataset. First, let’s see how we would obtain these linear models in R with standard procedures. Say we want to regress V1 and V3 on V2, V4, and V5 from the X dataset. We will start by creating a formula for an lm function to estimate the model we want.\n\n# Fit some multivariate linear models ------------------------------------------\n\n  # Define the dependent variables (dvs) of the multivairate linear models\n  dvs &lt;- c(\"V1\", \"V3\")\n\n  # Define the predictors (ivs) of the multivairate linear models\n  ivs &lt;- c(\"V2\", \"V4\", \"V5\")\n\n  # Create the formula (complicated but flexible way)\n  formula_mlm &lt;- paste0(\"cbind(\",\n                       paste0(dvs, collapse = \", \"),\n                       \") ~ \",\n                       paste0(ivs, collapse = \" + \"))\n\n  # Check the formula\n  formula_mlm\n\n[1] \"cbind(V1, V3) ~ V2 + V4 + V5\"\n\n\nNext, we will fit the multivariate linear model with the lm() function:\n\n  # Fit the model with the lm function\n  mlm0 &lt;- lm(formula_mlm, data = X)\n  coef(mlm0)\n\n                     V1          V3\n(Intercept) -45.7660931 135.1150663\nV2           -0.2747666  -0.6559719\nV4            0.1455375  -1.0485195\nV5            0.6507081  -0.6319507\n\n\nThese are our intercepts, and regression coefficients for the multivariate linear model. We can sweep \\(\\Theta\\) over the positions of the independent variables to obtain the the same intercept and regression coefficients. First, let’s define a vector of positions to sweep over based on the variable names we stored in ivs.\n\n# Fit some multivariate linear models using sweep ------------------------------\n\n  # Define positions to sweep over\n  sweep_over &lt;- which(colnames(Theta) %in% ivs)\n\nThen, let’s simply sweep our \\(\\Theta\\) over these positions.\n\n  # Sweep theta\n  H &lt;- sweepGoodnight(Theta, K = sweep_over)\n\n  # Check out the result\n  H\n\n            int          V1           V2          V3           V4           V5\nint  612.422481 -45.7660931 -5.874822779 135.1150663 -6.469828251 -1.408803042\nV1    45.766093   1.6538239  0.274766592  -1.6628629 -0.145537538 -0.650708148\nV2    -5.874823  -0.2747666  0.085293622  -0.6559719  0.073716182 -0.004651691\nV3  -135.115066  -1.6628629  0.655971950   2.4781175  1.048519534  0.631950668\nV4    -6.469828   0.1455375  0.073716182  -1.0485195  0.075591156  0.006836668\nV5    -1.408803   0.6507081 -0.004651691  -0.6319507  0.006836668  0.014961788\n\n\nOur regression coefficients are here in this new matrix. We just need to find them. We know that the dependent variables are V1 and V3, and that the independent variables are V2, V4, and V5. Let’s index the rows of H with the names of the ivs (and the name of the intercept row), and the columns of H with the names of the dvs.\n\n  # Extract the regression coefficients from H\n  H[c(\"int\", ivs), dvs]\n\n             V1          V3\nint -45.7660931 135.1150663\nV2   -0.2747666  -0.6559719\nV4    0.1455375  -1.0485195\nV5    0.6507081  -0.6319507\n\n  # Compare with coefficients from lm function\n  coef(mlm0)\n\n                     V1          V3\n(Intercept) -45.7660931 135.1150663\nV2           -0.2747666  -0.6559719\nV4            0.1455375  -1.0485195\nV5            0.6507081  -0.6319507\n\n\nNote that, we are sweeping \\(\\Theta\\) only over the predictors, but we also get the estimate of the intercept. Remember that \\(\\Theta\\) is the result of sweeping G over the first position, which is the position where the intercept estimate appears. You could obtain the same result by directly sweeping G over position 1, and the position of the predictors. In code:\n\n  # Sweep G\n  sweepGoodnight(G, c(1, sweep_over))[c(\"int\", ivs), dvs]\n\n             V1          V3\nint -45.7660931 135.1150663\nV2   -0.2747666  -0.6559719\nV4    0.1455375  -1.0485195\nV5    0.6507081  -0.6319507\n\n\nTherefore, you can think of finding the coefficients of a multivariate linear model using the sweep operator as:\n\nSWEEP(1, \\(k_1, \\dots, k_l\\))[G] or as,\nSWEEP(\\(k_1, \\dots, k_l\\))[SWEEP(1)[G]] or as,\nSWEEP(\\(k_1, \\dots, k_l\\))[\\(\\Theta\\)]\n\nwith \\(k_1, \\dots, k_l\\) being the positions of the \\(K\\) predictors in matrix \\(G\\).\nFinally, just play around with what variables you consider as dvs and ivs. You will discover the magic of the sweep operator.\n\n# Play around with variable roles ------------------------------------------\n\n  # Define different dependent variables (dvs) for the multivairate linear models\n  dvs &lt;- c(\"V1\", \"V2\", \"V5\")\n\n  # Define different predictors (ivs) for the multivairate linear models\n  ivs &lt;- c(\"V3\", \"V4\")\n\n  # Create the formula (complicated but flexible way)\n  formula_mlm &lt;- paste0(\"cbind(\",\n                       paste0(dvs, collapse = \", \"),\n                       \") ~ \",\n                       paste0(ivs, collapse = \" + \"))\n\n  # Fit the model with the MLM\n  mlm1 &lt;- lm(formula_mlm, data = X)\n  coef(mlm1)\n\n                     V1         V2          V5\n(Intercept) 18.63186149 78.3607367 131.2824064\nV3          -0.75087203 -0.2686979  -1.1998512\nV4          -0.07777123 -0.9014841  -0.7246001\n\n  # Define positions to sweep over\n  sweep_over &lt;- which(colnames(Theta) %in% ivs)\n\n  # Sweep Theta over new positions\n  sweepGoodnight(Theta, K = sweep_over)[c(\"int\", ivs), dvs]\n\n             V1         V2          V5\nint 18.63186149 78.3607367 131.2824064\nV3  -0.75087203 -0.2686979  -1.1998512\nV4  -0.07777123 -0.9014841  -0.7246001"
  },
  {
    "objectID": "posts/series-sampling/norta.html",
    "href": "posts/series-sampling/norta.html",
    "title": "Normal to anything (NORTA) sampling",
    "section": "",
    "text": "The Normal to anything (or NORTA) is a sampling approach that allows the generation of multivariate data with a known rank correlation structure and arbitrary marginal distributions. For example, you could generate 2 variables \\(X_1\\) and \\(X_2\\) with a given rank correlation and beta marginal distributions. The only requirement is that the target marginal distributions need to be continuous distributions with a known (inverse) cumulative distribution function."
  },
  {
    "objectID": "posts/series-sampling/norta.html#sample-from-a-multivariate-normal-distribution",
    "href": "posts/series-sampling/norta.html#sample-from-a-multivariate-normal-distribution",
    "title": "Normal to anything (NORTA) sampling",
    "section": "1. Sample from a multivariate normal distribution",
    "text": "1. Sample from a multivariate normal distribution\nFirst, we sample 1000 observations from a multivariate normal distribution with four correlated variables (\\(\\rho = .7\\)).\n\n# 1. Sample from multivariate normal distribution -----------------------------\n\n# Set the seed\nset.seed(20210422)\n\n# Fix parameters\nn &lt;- 1e3 # smaple size\np &lt;- 2  # number of variables\nmu &lt;- rep(0, p) # vector of means\nSigma &lt;- matrix(.7, nrow = p, ncol = p); diag(Sigma) &lt;- 1 # correlation matrix\n\n# Sample Multivariate Normal data\nX &lt;- mvrnorm(n = n, mu = mu, Sigma = Sigma)\n\nWe can then plot a scatterplot of \\(X_1\\) and \\(X_2\\) to study their multivariate distribution and make the marginal plots visible.\n\n# Plot the multivariate distribution (scatterplot)\nX_scatter &lt;- ggplot(data.frame(X), aes(x = X1, y = X2)) +\n      geom_point()\n\n# Add marginals of X\nggMarginal(X_scatter, type = \"histogram\") \n\n\n\n\nWe can now transform the marginal distributions of the \\(x\\)s to any target continuous distribution."
  },
  {
    "objectID": "posts/series-sampling/norta.html#transform-normal-marginals-into-uniform",
    "href": "posts/series-sampling/norta.html#transform-normal-marginals-into-uniform",
    "title": "Normal to anything (NORTA) sampling",
    "section": "2. Transform normal marginals into uniform",
    "text": "2. Transform normal marginals into uniform\nFor example, consider transforming the marginals to a beta distribution.\nFirst, we compute the values of the normal cumulative distribution function (pnorm()).\n\n# 2. Transform marginals to a uniform distribution -----------------------------\n\n# Transform to uniform distribution (apply normal CDF to X)\nU &lt;- pnorm(X) \n\n# Make scatterplot\nU_scatter &lt;- ggplot(data.frame(U), aes(x = X1, y = X2)) +\n      geom_point()\n\n# Add marginals of U\nggMarginal(U_scatter, type = \"histogram\")"
  },
  {
    "objectID": "posts/series-sampling/norta.html#transform-uniform-marginals-into-anything",
    "href": "posts/series-sampling/norta.html#transform-uniform-marginals-into-anything",
    "title": "Normal to anything (NORTA) sampling",
    "section": "3. Transform uniform marginals into anything",
    "text": "3. Transform uniform marginals into anything\nThen we compute the quantiles corresponding to the resulting cumulative probabilities based on the target marginal distribution using the qbeta() function.\n\n# 3. Transform marginals to anything with a (inverse) CDF ----------------------\n\n# &gt; 3.1 Beta -------------------------------------------------------------------\n\n# Transform to a beta distribution\nX_beta &lt;- qbeta(U, shape1 = .5, shape2 = .5)\n\n# And visualize\nX_beta_scatter &lt;- ggplot(data.frame(X_beta), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_beta_scatter, type = \"histogram\") \n\n\n\n\nAs another example, consider transforming the marginal to a highly skewed T distribution.\n\n# &gt; 3.2 Skewed-t distribution --------------------------------------------------\n\n# Define the target skewness and kurtosis\nsk &lt;- -2\nkt &lt;- 10\n\n# Define direct parameterization for the skew-t (ST) distribution\ncpST &lt;- c(0, 1, sk, kt)\ndpST &lt;- cp2dp(cpST, family = \"ST\")\n\n# Transform to Skew-t (apply target inverse-CDF to X)\nX_st &lt;- apply(U, 2, qst, dp = dpST)\n\n# And visualize\nX_st_scatter &lt;- ggplot(data.frame(X_st), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_st_scatter, type = \"histogram\") \n\n\n\n\n\n3.1 Preserved multivariate relationships\nUpon generating the multivariate data \\(\\mathbf{X}\\), the association between the two variables \\(X_1\\) and \\(X_2\\) is determined by the correlation coefficient \\(\\rho\\) that we used in the mvrnorm() call. This coefficient expresses the linear dependence between the two variables which is not preserved when non-linear transformations are applied to the marginals (which we use). However, the NORTA approach does preserve rank correlations between the variables. In the following code, we compute the Pearson correlation (linear dependency), and the Spearman and Kendall’s correlations (rank correlations) for the original data, and all other transformations.\n\n# Collect all sampled datasets\ndats &lt;- list(X = X, U = U, skewt = X_st, beta = X_beta)\n\n# Compute all types of correlation on all datasets\nround(\n      data.frame(\n            Pearson = sapply(dats, function(i) cor(i, method = \"pearson\")[1,2]),\n            Spearman = sapply(dats, function(i) cor(i, method = \"spearman\")[1,2]),\n            Kendall = sapply(dats, function(i) cor(i, method = \"kendall\")[1,2])\n      ), 3\n)\n\n      Pearson Spearman Kendall\nX       0.684    0.666   0.476\nU       0.667    0.666   0.476\nskewt   0.663    0.666   0.476\nbeta    0.644    0.666   0.476\n\n\nAs you can see, the NORTA sampling approach allows us to easily sample dependent multivariate data with arbitrary marginal distributions and known target (rank) correlations.\n\n\n3.2 Discrete distributions\nOne can be tempted to use any marginal distribution, even discrete distributions. However, the rank correlation is not preserved when this marginal transformation is applied. Consider for example transforming the marginals to Poisson distributions:\n\n# &gt; 3.3 Poissan distribution ---------------------------------------------------\n\n# Transform to poissan (apply target inverse-CDF to X)\nX_pois &lt;- qpois(U, lambda = 2)\n\n# And visualize\nX_pois_scatter &lt;- ggplot(data.frame(X_pois), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_pois_scatter, type = \"histogram\") \n\n\n\n\nor to binomial distributions\n\n# &gt; 3.4 Binomial distribution ---------------------------------------------------\n\n# Transform to binomial\nX_binom &lt;- qbinom(U, size = 6, prob = .2)\n\n# And visualize\nX_binom_scatter &lt;- ggplot(data.frame(X_binom), aes(x = X1, y = X2)) +\n      geom_point()\nggMarginal(X_binom_scatter, type = \"histogram\") \n\n\n\n\nif you check again both the linear and rank correlations you will notice that both have been impacted by these transformations:\n\n# Add the other datasets\ndats &lt;- list(\n            X = X, \n            U = U, \n            skewt = X_st, \n            beta = X_beta, \n            poissan = X_pois, \n            binomial = X_binom\n)\n\n# Compute all types of correlation on all datasets\nround(\n      data.frame(\n            Pearson = sapply(dats, function(i) cor(i, method = \"pearson\")[1,2]),\n            Spearman = sapply(dats, function(i) cor(i, method = \"spearman\")[1,2]),\n            Kendall = sapply(dats, function(i) cor(i, method = \"kendall\")[1,2])\n      ), 3\n)\n\n         Pearson Spearman Kendall\nX          0.684    0.666   0.476\nU          0.667    0.666   0.476\nskewt      0.663    0.666   0.476\nbeta       0.644    0.666   0.476\npoissan    0.644    0.635   0.535\nbinomial   0.620    0.605   0.535"
  },
  {
    "objectID": "posts/series-sampling/norta.html#online-resources",
    "href": "posts/series-sampling/norta.html#online-resources",
    "title": "Normal to anything (NORTA) sampling",
    "section": "Online resources",
    "text": "Online resources\nSAS blogs: introduction to copulas Simulating Dependent Random Variables Using Copulas"
  },
  {
    "objectID": "research/mi-pcr.html",
    "href": "research/mi-pcr.html",
    "title": "Solving the many variables problem in MICE with principal component regression.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Klaas Sijtsma, and Tim Reeskens. 2023. “Solving the Many-Variables Problem in MICE with Principal Component Regression.” Behavior Research Methods. https://doi.org/doi.org/10.3758/s13428-023-02117-1."
  },
  {
    "objectID": "research/mi-hd.html",
    "href": "research/mi-hd.html",
    "title": "High-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M. Lang, Tim Reeskens, and Klaas Sijtsma. 2023. “High-Dimensional Imputation for the Social Sciences: A Comparison of State-of-the-Art Methods.” Sociological Methods & Research 0 (0). https://doi.org/https://doi.org/10.1177/00491241231200194."
  },
  {
    "objectID": "research/mi-spcr.html",
    "href": "research/mi-spcr.html",
    "title": "Supervised Dimensionality Reduction for Multiple Imputation by Chained Equations",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M. Lang, and Klaas Sijtsma. 2023. “Supervised Dimensionality Reduction for Multiple Imputation by Chained Equations.” arXiv Preprint arXiv:2309.01608."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2023\n\n\nMultiple Imputation with GSPCR: a case study\n\n\nWork in progress\n\n\n\n\n2023\n\n\nSupervised Dimensionality Reduction for Multiple Imputation by Chained Equations\n\n\nWork in progress\n\n\n\n\n2023\n\n\nSolving the many variables problem in MICE with principal component regression.\n\n\nPublished\n\n\n\n\n2022\n\n\nHigh-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.\n\n\nPublished\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html",
    "href": "posts/series-pca-primer/pls.html",
    "title": "Implementing a PLS alogirthm in R",
    "section": "",
    "text": "Many data analysts face the problem of analyzing data sets with many, often highly correlated, variables. Partial Least Square Regression (PLSR) is a regression method that uses linear combinations of the original predictors to reduce their dimensionality. As principal component regression, PLS uses derived inputs, however, it differs from PCR by how the linear combinations are constructed.\nGiven a set of predictors \\(X_{n \\times p}\\) and a vector of dependent variable scores \\(y_{n \\times 1}\\), the least-square solution for the multiple linear regression\n\\[\ny = X \\beta + \\epsilon\\text{, with } \\epsilon \\sim N(0, \\sigma^2)\n\\]\nis\n\\[\n\\beta = (X'X)^{-1}X'y\n\\]\nwhere \\(X'\\) is the transpose of the data matrix \\(X\\), and \\(()^{-1}\\) is the matrix inverse. When collinearity is present in \\(X\\) or \\(p &gt; n\\), then \\(X'X\\) is singular and its inverse cannot be computed. Derived input regression methods like PCR and PLSR bypass this problem by taking linear combinations of the columns of the original \\(X\\) and regressing \\(Y\\) on just a few of these linear combinations. The peculiarity of PLSR is that it includes information on both \\(X\\) and \\(Y\\) in the definition of the linear combinations. In this post, we look at two algorithms to estimate PLSR to get a better understanding of the method.\n\n\nHere, I describe informally the algorithm steps:\n\nPreprocessing the data - the columns of the input matrix \\(X\\) are standardized to have mean 0 and variance 1.\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed: \\(\\rho_{1j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the first partial-least-square direction \\(z_1\\) - The cross-products \\(\\rho_{1j}\\) are used as weights to obtain a linear combination of the columns: \\(z_1 = \\sum \\rho_{1j} x_{j}\\). This implies that the contribution of each column to \\(z_1\\) is weighted by their univariate relationship with the dependent variable \\(y\\).\nRegression of \\(y\\) on \\(z_1\\) - The outcome variable \\(y\\) is regressed on this first direction \\(z_1\\) to obtain \\(\\hat{\\theta}_1\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_1\\).\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed again: \\(\\rho_{2j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the second partial-least-square direction \\(z_2\\) - The cross-products \\(\\rho_{2j}\\) are used as weights to obtain a linear combination of the columns: \\(z_2 = \\sum \\rho_{2j} x_{j}\\). Notice that the columns \\(x_j\\) we are using now are orthogonal to the previous partial least square direction \\(z_1\\).\nRegression of \\(y\\) on \\(z_2\\) - The outcome variable \\(y\\) is regressed on the second direction \\(z_2\\) to obtain \\(\\hat{\\theta}_2\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_2\\).\n\nThe procedure continues until \\(M\\) partial least square directions have been computed. The result is a set of independent directions that have both high variance and high correlation with the dependent variable, in contrast to PCR which finds a set of independent directions that have high variance.\nNow we report pseudo-code for the implementation of the PLS algorithm (inspired by Report Algorithm 3.3 p.103 as in (Hastie, Tibshirani, and Wainwright 2015)). We will use it to write the R code in the next session.\n\nStandardized each \\(x_j\\) to have mean 0 and variance 1\nSet:\n\n\\(\\hat{y}^{(0)} = \\bar{y}1\\)\n\\(x_{j}^{(0)} = x_{j}\\)\n\nFor \\(m = 1, 2, \\dots, M\\)\n\n\\(z_m = \\sum_{j = 1}^{p} \\rho_{mj}x_{j}^{(m-1)}\\)\n\\(\\hat{\\theta}_m = \\frac{z_m'y}{z_m' z_m}\\)\n\\(\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_m z_m\\)\nfor \\(j = 1, \\dots, p\\) orthogonalize \\(x_{j}\\) with respect to \\(z_m\\): \\(x_{j}^{(m)} = x_{j}^{(m-1)} - \\frac{z_m' x_{j}^{(m)}}{z_m' z_m}z_m\\)\n\nOutput the sequence of fitted vectors \\(\\hat{y}^{m}\\)"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#popular-algorithms-to-implement-pls",
    "href": "posts/series-pca-primer/pls.html#popular-algorithms-to-implement-pls",
    "title": "Implementing a PLS alogirthm in R",
    "section": "",
    "text": "Here, I describe informally the algorithm steps:\n\nPreprocessing the data - the columns of the input matrix \\(X\\) are standardized to have mean 0 and variance 1.\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed: \\(\\rho_{1j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the first partial-least-square direction \\(z_1\\) - The cross-products \\(\\rho_{1j}\\) are used as weights to obtain a linear combination of the columns: \\(z_1 = \\sum \\rho_{1j} x_{j}\\). This implies that the contribution of each column to \\(z_1\\) is weighted by their univariate relationship with the dependent variable \\(y\\).\nRegression of \\(y\\) on \\(z_1\\) - The outcome variable \\(y\\) is regressed on this first direction \\(z_1\\) to obtain \\(\\hat{\\theta}_1\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_1\\).\nThe cross-product of every column of \\(X\\) and \\(y\\) is computed again: \\(\\rho_{2j} = x_{j}' y\\) for \\(j = 1, \\dots, p\\).\nCompute the second partial-least-square direction \\(z_2\\) - The cross-products \\(\\rho_{2j}\\) are used as weights to obtain a linear combination of the columns: \\(z_2 = \\sum \\rho_{2j} x_{j}\\). Notice that the columns \\(x_j\\) we are using now are orthogonal to the previous partial least square direction \\(z_1\\).\nRegression of \\(y\\) on \\(z_2\\) - The outcome variable \\(y\\) is regressed on the second direction \\(z_2\\) to obtain \\(\\hat{\\theta}_2\\).\nOrthogonalization of \\(X\\) - All columns of \\(X\\) are orthogonalized with respect to \\(z_2\\).\n\nThe procedure continues until \\(M\\) partial least square directions have been computed. The result is a set of independent directions that have both high variance and high correlation with the dependent variable, in contrast to PCR which finds a set of independent directions that have high variance.\nNow we report pseudo-code for the implementation of the PLS algorithm (inspired by Report Algorithm 3.3 p.103 as in (Hastie, Tibshirani, and Wainwright 2015)). We will use it to write the R code in the next session.\n\nStandardized each \\(x_j\\) to have mean 0 and variance 1\nSet:\n\n\\(\\hat{y}^{(0)} = \\bar{y}1\\)\n\\(x_{j}^{(0)} = x_{j}\\)\n\nFor \\(m = 1, 2, \\dots, M\\)\n\n\\(z_m = \\sum_{j = 1}^{p} \\rho_{mj}x_{j}^{(m-1)}\\)\n\\(\\hat{\\theta}_m = \\frac{z_m'y}{z_m' z_m}\\)\n\\(\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_m z_m\\)\nfor \\(j = 1, \\dots, p\\) orthogonalize \\(x_{j}\\) with respect to \\(z_m\\): \\(x_{j}^{(m)} = x_{j}^{(m-1)} - \\frac{z_m' x_{j}^{(m)}}{z_m' z_m}z_m\\)\n\nOutput the sequence of fitted vectors \\(\\hat{y}^{m}\\)"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#coding-the-pls-algorithm-manually",
    "href": "posts/series-pca-primer/pls.html#coding-the-pls-algorithm-manually",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Coding the PLS algorithm manually",
    "text": "Coding the PLS algorithm manually\nLet’s go through the steps described in the pseudo code above. First we want to standardize the predictors.\n\n# 1. Standardize the data ------------------------------------------------------\n\n    # Scale the predictors\n    Xstd &lt;- scale(X)\n\n    # Means of X are now 0\n    cbind(\n        X = head(colMeans(X)), \n        Xstd = round(head(colMeans(Xstd)), 5)\n        )\n\n                         X Xstd\nconfused         1.8196721    0\nright words      1.7950820    0\nsensations       0.5983607    0\ndescribe         2.2213115    0\nanalyze problems 2.5081967    0\nupset            1.6065574    0\n\n    # SD of X are now 1\n    cbind(\n        X = head(apply(X, 2, sd)),\n        Xstd = round(head(apply(Xstd, 2, sd)), 5)\n        )\n\n                        X Xstd\nconfused         1.178431    1\nright words      1.278807    1\nsensations       1.073034    1\ndescribe         1.216364    1\nanalyze problems 1.077539    1\nupset            1.295621    1\n\n\nThen we want to set the initial values\n\n# 2. Set initial vlaues --------------------------------------------------------\n\n    # 2a: Set the initial prediction for y_hat to the mean of y\n    # Create an empty data.frame to store the initial value and future predictions\n    yhat_m &lt;- matrix(rep(NA, n * (M + 1)), nrow = n)\n\n    # Replace the initial prediction with the mean of y\n    yhat_m[, 1] &lt;- mean(y)\n\n    # 2b: Set every xj0 to xj\n    # Create an empty list of X values\n    Xm &lt;- lapply(1:(M + 1), matrix, nrow = n, ncol = p)\n\n    # Place X as initial value for Xm\n    Xm[[1]] &lt;- as.matrix(Xstd)\n\nFinally, we can move to the estimation step. First, we create the container objects to store results\n\n# 3. Estimation ----------------------------------------------------------------\n\n    # Preparing objects\n    z &lt;- matrix(NA, nrow = n, ncol = (M + 1)) # container for directions\n    W &lt;- matrix(NA, nrow = p, ncol = (M + 1)) # container for wights\n    theta_hat &lt;- rep(NA, (M + 1)) # container for thetas\n\nThen we move to the proper estimation.\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:(M + 1)) {\n        # 3a: Compute zm\n        W[, m] &lt;- t(Xm[[m - 1]]) %*% y   # inner products / covariances\n        z[, m] &lt;- Xm[[m - 1]] %*% W[, m] # compute the direction zm\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] &lt;- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        yhat_m[, m] &lt;- yhat_m[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            Xm[[m]][, j] &lt;- orthogonalize(Xm[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Fit PLSR model w/ pls package\n    pls_fit_pls &lt;- pls::plsr(\n        y ~ as.matrix(X),\n        ncomp = M,\n        method = \"oscorespls\",\n        validation = \"none\",\n        scale = TRUE\n    )\n\n    # Fit PCR model w/ plsdof package\n    pls_fit_plsdof &lt;- plsdof::pls.model(as.matrix(Xstd), y)\n\n    # Compare predictions using up to a given m\n    m &lt;- 3\n    head(\n        data.frame(\n            pls = round(as.data.frame(fitted(pls_fit_pls)), 3)[, m],\n            plsdof = round(pls_fit_plsdof$Yhat, 3)[, m + 1],\n            man = round(yhat_m, 3)[, m + 1]\n    ))\n\n     pls plsdof    man\n1 36.819 36.819 36.819\n2 29.243 29.243 29.243\n3 31.336 31.336 31.336\n4 30.767 30.767 30.767\n5 34.869 34.869 34.869\n6 24.876 24.876 24.876"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#types-of-dependent-variables",
    "href": "posts/series-pca-primer/pls.html#types-of-dependent-variables",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Types of dependent variables",
    "text": "Types of dependent variables\n\n# Types of DVs -----------------------------------------------------------------\n\n    data(oliveoil)\n    sens.pcr &lt;- pls::pcr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n    sens.pls &lt;- pls::plsr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n\n    oliveoil$sensory\n\n   yellow green brown glossy transp syrup\nG1   21.4  73.4  10.1   79.7   75.2  50.3\nG2   23.4  66.3   9.8   77.8   68.7  51.7\nG3   32.7  53.5   8.7   82.3   83.2  45.4\nG4   30.2  58.3  12.2   81.1   77.1  47.8\nG5   51.8  32.5   8.0   72.4   65.3  46.5\nI1   40.7  42.9  20.1   67.7   63.5  52.2\nI2   53.8  30.4  11.5   77.8   77.3  45.2\nI3   26.4  66.5  14.2   78.7   74.6  51.8\nI4   65.7  12.1  10.3   81.6   79.6  48.3\nI5   45.0  31.9  28.4   75.7   72.9  52.8\nS1   70.9  12.2  10.8   87.7   88.1  44.5\nS2   73.5   9.7   8.3   89.9   89.7  42.3\nS3   68.1  12.0  10.8   78.4   75.1  46.4\nS4   67.6  13.9  11.9   84.6   83.8  48.5\nS5   71.4  10.6  10.8   88.1   88.5  46.7\nS6   71.4  10.0  11.4   89.5   88.5  47.2"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#prediction-of-new-data",
    "href": "posts/series-pca-primer/pls.html#prediction-of-new-data",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Prediction of new data",
    "text": "Prediction of new data\nHow do we predict new data? Let’s start by generating data from scratch\n\n# Prediction -------------------------------------------------------------------\n\n    n &lt;- 50 # number of observations\n    p &lt;- 15 # number of variables\n    X &lt;- matrix(rnorm(n * p), ncol = p)\n    y &lt;- 100 + rnorm(n)\n    M &lt;- 10 # number of \"components\"\n\n    ntest &lt;- 200 #\n    Xtest &lt;- matrix(rnorm(ntest * p), ncol = p) # test data\n    ytest &lt;- rnorm(ntest) # test data\n\n    # Fit alternative PLSs\n    out_pls &lt;- pls::plsr(\n        y ~ X,\n        ncomp = M,\n        scale = TRUE,\n        center = TRUE,\n        method = \"oscorespls\",\n        validation = \"none\",\n        x = TRUE,\n        model = TRUE\n    )\n    out_plsdof &lt;- plsdof::pls.model(X, y, compute.DoF = TRUE, Xtest = Xtest, ytest = NULL)\n\n    # Obtain predictions on new data\n    head(\n        round(\n            cbind(\n                PLS = predict(out_pls, newdata = Xtest)[, , M],\n                PLSdof = out_plsdof$prediction[, M + 1]\n            ), 5\n        )\n    )\n\n           PLS    PLSdof\n[1,]  99.00044  99.00044\n[2,] 101.55959 101.55959\n[3,]  99.71676  99.71676\n[4,]  99.58406  99.58406\n[5,]  99.34923  99.34923\n[6,] 100.52233 100.52233\n\n    # Make sure scale of prediction is correct\n    out_pls_cF &lt;- plsr(\n      y ~ X,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    Xs &lt;- scale(X, center = TRUE, scale = FALSE)\n    ys &lt;- scale(y, center = FALSE, scale = FALSE)\n\n    out_pls_cF &lt;- plsr(\n      ys ~ Xs,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    head(\n        round(\n        cbind(\n            PLS = predict(out_pls, newdata = Xtest)[, , M],\n            PLS_cf = mean(y) + predict(out_pls_cF, newdata = Xtest)[, , M],\n            PLSdof = out_plsdof$prediction[, M]\n        ), 5\n        )\n    )\n\n           PLS    PLS_cf    PLSdof\n[1,]  99.00044  99.07823  99.00042\n[2,] 101.55959 101.63738 101.55947\n[3,]  99.71676  99.79455  99.71674\n[4,]  99.58406  99.66185  99.58385\n[5,]  99.34923  99.42702  99.34921\n[6,] 100.52233 100.60012 100.52225"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#degrees-of-freedom-of-the-residuals",
    "href": "posts/series-pca-primer/pls.html#degrees-of-freedom-of-the-residuals",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Degrees of freedom of the residuals",
    "text": "Degrees of freedom of the residuals\n\n# PLS degrees of freedom -------------------------------------------------------\n\n    library(plsdof)\n    set.seed(1234)\n\n    # Generate data data\n    n &lt;- 100 # number of observations\n    p &lt;- 15 # number of variables\n    m &lt;- 15\n    X &lt;- matrix(rnorm(n * p), ncol = p)\n    y &lt;- rnorm(n)\n\n    # Fit model with package\n    outpls &lt;- pls.model(X, y, compute.DoF = TRUE)\n    outpls$DoF\n    outpls.internal &lt;- linear.pls.fit(X, y, m, DoF.max = min(n - 1, p + 1))\n\n    # Fit model with person PLS function\n    outpls_man &lt;- pls.manual(ivs = X, dv = y, m = m)\n\n    # Fit model with plsr function from pls\n    pls_fit_pls &lt;- plsr(\n        y ~ X,\n        ncomp = m,\n        scale = FALSE,\n        center = FALSE,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n\n    # Y hats\n    round(outpls_man$Yhat - outpls$Yhat, 5)\n\n    # T scores\n    j &lt;- 1\n    cbind(\n        PLSR = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2)))[, j],\n        PLSTT = outpls.internal$TT[, j],\n        manualTs = outpls_man$Ts[, j],\n        manualTsn = outpls_man$Tsn[, j]\n    )\n\n    # Degrees of freedom PLSR\n    predi &lt;- sapply(1:m, function(j) {\n        predict(pls_fit_pls, ncomp = j)\n    })\n    DoF_plsr &lt;- dofPLS(\n        X,\n        y,\n        TT = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2))),\n        Yhat = predi,\n        DoF.max = m + 1\n    )\n\n    # Degrees of freedom manual\n    DoF_manual &lt;- dofPLS(\n        X,\n        y,\n        TT = outpls_man$Tsn,\n        Yhat = outpls_man$Yhat[, 2:(m + 1)],\n        DoF.max = m + 1\n    )\n\n    # Single DoF\n    DoF_single &lt;- c(1, sapply(1:m, function(i) {\n        dofPLS_single(\n            X,\n            y,\n            TT = outpls_man$Tsn,\n            Yhat = outpls_man$Yhat[, (i + 1)],\n            q = i\n        )\n    }))\n\n    # Compare degrees of freedom\n    cbind(\n        PLSR = DoF_plsr,\n        PLSdof = outpls$DoF,\n        PLS_manual = DoF_manual,\n        diff = round(outpls$DoF - DoF_manual, 5),\n        DoF_single = DoF_single\n    )"
  },
  {
    "objectID": "posts/series-pca-primer/pls.html#footnotes",
    "href": "posts/series-pca-primer/pls.html#footnotes",
    "title": "Implementing a PLS alogirthm in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCheck the helpfile for the alexithymia data in the PCovR package for more information.↩︎"
  },
  {
    "objectID": "posts/series-pca-primer/pca-non-graphical-solutions.html",
    "href": "posts/series-pca-primer/pca-non-graphical-solutions.html",
    "title": "Deciding the Number of PCs with Non-Graphical Solutions to the Scree Test",
    "section": "",
    "text": "Introduction\nHere I describe two different solutions to decide which number of components to sue for PCA with non-graphical solutions to teh scree test:\n\nKaiser Rule (aka Optimal Coordinate) \\(n_{oc}\\). In its simplest form, the Kaiser’s rule retains only the PCs with variances exceeding 1. If a PC has less variance than 1, it means that it explains less total variance than a single variable in the data, which makes it useless.\nAcceleration Factor. For every \\(j\\)-th eigenvalue, the acceleration factor \\(a\\) is calculated as the change in the slope between the line connecting the \\(eig_j\\) and \\(eig_{j-1}\\), and the line connecting \\(eig_j\\) and \\(eig_{j+1}\\) \\[\na_{j} = (eig_{j+1} - eig_{j}) - (eig_{j} - eig_{j-1})\n\\] Once the largest \\(a_j\\) is found, the number of components is set to \\(j-1\\).\n\n\n\nLearn by coding\n\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres &lt;- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues &lt;- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n\n\n\n# Non-graphical solutions\nngs &lt;- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man &lt;- sum(eigenvalues &gt; 1)\n\n# Accelration factor\na &lt;- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] &lt;- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man &lt;- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n\n        manual nFactor\nnaf          2       2\nnkaiser      2       2\n\n\n\n\nTL;DR, just give me the code!\n\n# Prepare environment ----------------------------------------------------------\n\nlibrary(nFactors)\nlibrary(psych)\n\n# Perform PCA\nres &lt;- psych::pca(Harman.5)\n\n# Extract eigenvalues\neigenvalues &lt;- res$values\n\n# Graph\nplotuScree(x = eigenvalues)\n\n# Non-graphical solutions\nngs &lt;- nScree(x = eigenvalues)\n\n# Kaiser rule\\\nnkaiser_man &lt;- sum(eigenvalues &gt; 1)\n\n# Accelration factor\na &lt;- NULL\nfor (j in 2:(length(eigenvalues) - 1)){\n  a[j] &lt;- (eigenvalues[j + 1] - eigenvalues[j]) - (eigenvalues[j] - eigenvalues[j - 1])\n}\n\nnaf_man &lt;- which.max(a) - 1\n\n# Compare results\ndata.frame(manual = c(naf = naf_man, nkaiser = nkaiser_man),\n           nFactor = c(naf = ngs$Components[[\"naf\"]],\n                       nkaiser = ngs$Components[[\"nkaiser\"]]))\n\n\n\nOther resources\n\nCross-entropy in RPubs\nA Gentle Introduction to Cross-Entropy for Machine Learning\nUnderstanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names\nML Gloassary\nLoss Functions in Machine Learning"
  },
  {
    "objectID": "posts/series-pca-primer/pca-svd.html",
    "href": "posts/series-pca-primer/pca-svd.html",
    "title": "Principal Component Analysis, SVD, and EVD",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an \\(n \\times p\\) data matrix \\(\\mathbf{X}\\) with minimal loss of information. We refer to this low-dimensional representation as the \\(n \\times r\\) matrix \\(\\mathbf{T}\\), where \\(r &lt; p\\).\nThe columns of \\(\\mathbf{T}\\) are called principal components (PCs) of \\(\\mathbf{X}\\). We follow the common practice of assuming that the columns of \\(\\mathbf{X}\\) are mean-centered and scaled to have a variance of 1. The first PC of \\(\\mathbf{X}\\) is the linear combination of the columns of \\(\\mathbf{X}\\) with the largest variance: \\[\n    \\mathbf{t}_1 = \\lambda_{11} \\mathbf{x}_1 + \\lambda_{12} \\mathbf{x}_2 + \\dots + \\lambda_{1p} \\mathbf{x}_p = \\mathbf{X} \\mathbf{\\lambda}_1\n\\] with \\(\\mathbf{\\lambda}_1\\) being the \\(1 \\times p\\) vector of coefficients \\(\\lambda_{11}, \\dots, \\lambda_{1p}\\). The second principal component (\\(\\mathbf{t}_2\\)) is defined by finding the vector of coefficients \\(\\mathbf{\\lambda}_2\\) giving the linear combination of \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_p\\) with maximal variance out of all the linear combinations that are uncorrelated with \\(\\mathbf{t}_1\\). Every subsequent column of \\(\\mathbf{T}\\) can be understood in the same way. As a result, the PCs are independent by definition and every subsequent PC has less variance than the preceding one. We can write the relationship between all the PCs and \\(\\mathbf{X}\\) in matrix notation: \\[\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{T} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\\] where \\(\\mathbf{\\Lambda}\\) is a \\(p \\times r\\) matrix of weights, with columns \\(\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_q\\). PCA can be thought of as the process of projecting the original data from a \\(p\\)-dimensional space to a lower \\(q\\)-dimensional space. The coefficient vectors \\(\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r\\) define the directions in which we are projecting the \\(n\\) observations of \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_p\\). The projected values are the principal component scores \\(\\mathbf{T}\\).\nThe goal of PCA is to find the values of \\(\\mathbf{\\Lambda}\\) that maximize the variance of the columns of \\(\\mathbf{T}\\). One way to find the PCA solution for \\(\\mathbf{\\Lambda}\\) is by taking the truncated singular value decomposition (SVD) of \\(\\mathbf{X}\\):\n\\[\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\\]\nwhere:\n\n\\(\\mathbf{D}\\) is the \\(r \\times r\\) diagonal matrix with elements equal to the square root of the non-zero eigenvalues of \\(\\mathbf{XX}^T\\) and \\(\\mathbf{X}^T\\mathbf{X}\\);\n\\(\\mathbf{U}\\) is an \\(n \\times p\\) orthogonal matrix such that \\(\\mathbf{U}^T\\mathbf{U=I}\\)\n\\(\\mathbf{V}\\) is the \\(p \\times p\\) orthogonal matrix of eigenvectors of \\(\\mathbf{X}^T\\mathbf{X}\\) such that \\(\\mathbf{V}^T\\mathbf{V=I}\\);\n\nThe scores on the first \\(r\\) PCs are given by the \\(n \\times r\\) matrix \\(\\mathbf{U}_{r}\\mathbf{D}_{r}\\), where \\(\\mathbf{U}_{r}\\) and \\(\\mathbf{D}_{r}\\) stand for the reduced forms of \\(\\mathbf{U}\\) and \\(\\mathbf{D}\\) obtained by taking their first \\(r\\) column and diagonal values. The weights \\(\\mathbf{\\Lambda}\\) are given by the \\(p \\times r\\) matrix \\(\\mathbf{V}_{r}\\), so that: \\[\n    \\mathbf{T} = \\mathbf{U}_{r}\\mathbf{D}_{r} = \\mathbf{X}\\mathbf{V}_{r}\n\\]\nAnother way to achieve the same solution relies on the eigenvalue decomposition (EVD) of the cross-product matrix (or the correlation matrix). In particular, the EVD of \\(\\mathbf{X}^{T} \\mathbf{X}\\) is equal to \\(\\mathbf{V} \\mathbf{D}^{2} \\mathbf{V}^{T}\\), where \\(\\mathbf{V}\\) and \\(\\mathbf{D}\\) are the same eigenvectors and eigenvalues, respectively, defined above."
  },
  {
    "objectID": "posts/series-pca-primer/pca-svd.html#pca-as-svd",
    "href": "posts/series-pca-primer/pca-svd.html#pca-as-svd",
    "title": "Principal Component Analysis, SVD, and EVD",
    "section": "PCA as SVD",
    "text": "PCA as SVD\nAs we discussed, PCA can be obtained as the SVD of \\(X\\).\n\n# PCA as singular value decomposition of X -------------------------------------\n\n# Perform SVD\nuSVt  &lt;- svd(X)\n\n# Extract objects\nU     &lt;- uSVt$u\nSigma &lt;- diag(uSVt$d)\nV     &lt;- uSVt$v\n\n# Compute the PC scores\nT_svd &lt;- U %*% Sigma\n\n# Compute the PC scores (equivalent way)\nT_svd &lt;- X %*% V\n\n# Define eigenvalues\neigenv_svd &lt;- uSVt$d^2\n\n# Compute cumulative proportion of explained variance\nCPVE_svd &lt;- cumsum(prop.table(eigenv_svd))"
  },
  {
    "objectID": "posts/series-pca-primer/pca-svd.html#pca-as-eigenvalue-decomposition",
    "href": "posts/series-pca-primer/pca-svd.html#pca-as-eigenvalue-decomposition",
    "title": "Principal Component Analysis, SVD, and EVD",
    "section": "PCA as eigenvalue decomposition",
    "text": "PCA as eigenvalue decomposition\nPCA can also be obtained as the eigenvalue decomposition of the cross-product matrix of \\(X\\)\n\n# PCA as eigenvalue decomposition of XtX --------------------------------------------\n\n# Compute the cross-product matrix XtX\nXtX &lt;- t(X) %*% X\n\n# Perform eigenvalue decomposition\neigenmat_XtX &lt;- eigen(XtX)\n\n# Extract eigenvalues\neigenvalues_XtX &lt;- eigenmat_XtX$values\n\n# Extract component loadings\neigenvectors_XtX &lt;- eigenmat_XtX$vectors\n\n# Compute the PC scores\nT_eig_XtX &lt;- X %*% eigenvectors_XtX\n\n# Compute cumulative proportion of explained variance\nCPVE_eig_XtX &lt;- cumsum(prop.table(eigenvalues_XtX))\n\nIt can also be computed by the eigenvalue decomposition of the correlation matrix of \\(X\\).\n\n# PCA as eigenvalue decomposition of cor(X) -----------------------------------------\n\n# Compute the correlation matrix\nX_cormat &lt;- cor(X, method = \"pearson\")\n\n# Perform eigenvalue decomposition\neigenmaT_eig_corx &lt;- eigen(X_cormat)\n\n# Extract eigenvalues\neigenvalues_corx &lt;- eigenmaT_eig_corx$values\n\n# Extract component loadings\neigenvectors_corx &lt;- eigenmaT_eig_corx$vectors\n\n# Compute the PC scores\nT_eig_corx &lt;- X %*% eigenvectors_corx\n\n# Compute cumulative proportion of explained variance\nCPVE_corx &lt;- cumsum(prop.table(eigenvalues_corx))"
  },
  {
    "objectID": "posts/series-pca-primer/pca-svd.html#compare",
    "href": "posts/series-pca-primer/pca-svd.html#compare",
    "title": "Principal Component Analysis, SVD, and EVD",
    "section": "Compare",
    "text": "Compare\nTo check our results we can compare the PC scores, CPVEs, and loading matrices with the results of the standard R function to compute PCA (prcomp). Let’s compute the PCs and all the relevant quantities with the prcomp:\n\n# PCA with prcomp --------------------------------------------------------------\n\n# Compute PCA with prcomp\nPCX &lt;- prcomp(X)\n\n# Extract component scores\nT_prcomp &lt;- PCX$x\n\n# Extract eigenvalues\neigenvalues_prcomp &lt;- PCX$sdev^2\n\n# Extract component loadings\nV_prcomp &lt;- as.matrix(PCX$rotation)\n\n# Extract cumulative explained variance\nCPVE_PCX &lt;- cumsum(prop.table(eigenvalues_prcomp))\n\nTo compare the PC score and loading matrices we will use Tucker congruence coefficient. This is a measure of similarity between matrices that ranges between -1 and +1. A congruence of 1 means the two matrices are identical. To compare the vectors of the cumulative proportion of explained variance we will simply print the vectors one next to the other. In the following, you can see that all of the ways to find solutions to the PCA problem are identical.\n\nPC scores\n\n# Compare solutions ------------------------------------------------------------\n\n# Load package for tucker congruence\nlibrary(RegularizedSCA)\n\n# &gt; PC scores ------------------------------------------------------------------\n\n# prcomp results = SVD\nTuckerCoef(T_prcomp, T_svd)$tucker_value\n\n[1] 1\n\n# prcomp results = eigenvalues_XtX\nTuckerCoef(T_prcomp, T_eig_XtX)$tucker_value\n\n[1] 1\n\n# prcomp results = eigenvectors_corx\nTuckerCoef(T_prcomp, T_eig_corx)$tucker_value\n\n[1] 1\n\n\n\n\nComponent loadings\n\n# &gt; Component loadings ---------------------------------------------------------\n\n# prcomp results = SVD\nTuckerCoef(V_prcomp, V)$tucker_value\n\n[1] 1\n\n# prcomp results = eigenvalues_XtX\nTuckerCoef(V_prcomp, eigenvectors_XtX)$tucker_value\n\n[1] 1\n\n# prcomp results = eigenvectors_corx\nTuckerCoef(V_prcomp, eigenvectors_corx)$tucker_value\n\n[1] 1\n\n\n\n\nCumulative proportion of explained variance\n\n# Cumulative proportion of explained variance\ndata.frame(\n    SVD = CPVE_svd,\n    eig_XtX = CPVE_eig_XtX,\n    eig_corx = CPVE_corx,\n    prcomp = CPVE_PCX\n)\n\n        SVD   eig_XtX  eig_corx    prcomp\n1 0.4680609 0.4680609 0.4680609 0.4680609\n2 0.8992696 0.8992696 0.8992696 0.8992696\n3 0.9505615 0.9505615 0.9505615 0.9505615\n4 1.0000000 1.0000000 1.0000000 1.0000000"
  },
  {
    "objectID": "posts/series-knowledge/ridge.html",
    "href": "posts/series-knowledge/ridge.html",
    "title": "Estimating ridge regression in R",
    "section": "",
    "text": "When there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance. This problem can be alleviated by imposing a size constraint (or penalty) on the coefficients. Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients values minimize a penalized residual sum of squares:\n\\[\n\\hat{\\beta}^{\\text{ridge}} = \\text{argmin}_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\right\\}\n\\]\nThe ridge solutions are not equivariant under scaling of the inputs. Therefore, it is recommended to standardize the inputs before solving the minimization problem.\nNotice that the intercept \\(\\beta_0\\) has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for \\(Y\\). Furthermore, by centering the predictors, we can separate the solution to the minimazion problem into two parts:\n\nIntercept \\[\n\\hat{\\beta}_0 = \\bar{y}=\\frac{1}{N}\\sum_{i = 1}^{N} y_i\n\\]\nPenalised regression coefficients \\[\n\\hat{\\beta}^{\\text{ridge}}=(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^Ty\n\\] which is the regular way of estimating regression coefficients with a penalty term (\\(\\lambda\\)) added on the diagonal (\\(\\mathbf{I}\\)) of the cross-product matrix (\\(\\mathbf{X}^T\\mathbf{X}\\)) to make it invertible (\\((...)^{-1}\\))."
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "href": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "title": "Estimating ridge regression in R",
    "section": "Fitting ridge regression manually",
    "text": "Fitting ridge regression manually\nFirst, let’s make sure the predictors are centered on the mean and scaled to have a variance of 1.\n\n# Fitting ridge regression manually --------------------------------------------\n\n# Scale the data (standardize)\nX_scale &lt;- scale(X, center = TRUE, scale = TRUE)\n\nThen, we want to fit the ridge regression manually by separating the intercept and the regression coefficients estimation (two-step approach):\n\nEstimate the intercept (\\(\\hat{\\beta}_0\\))\n::: {.cell}\n# Estimate the intercept\nb0_hat_r &lt;- mean(y)\n:::\nEstimate the ridge regression coefficients (\\(\\hat{\\beta}^{\\text{ridge}}\\)).\n\n\nCompute the cross-product matrix of the predictors.\nThis is the same step we would take if we wanted to compute the OLS estimates.\n::: {.cell}\n# Compute the cross-product matrix of the data\nXtX &lt;- t(X_scale) %*% X_scale\n:::\nDefine a value of \\(\\lambda\\).\nThis value is usually chosen by cross-validation from a grid of possible values. However, here we are only interested in how \\(\\lambda\\) is used in the computation, so we can simply give it a fixed value.\n::: {.cell}\n# Define a lambda value\nlambda &lt;- .1\n:::\nCompute \\(\\hat{\\beta}^{\\text{ridge}}\\).\n::: {.cell}\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r &lt;- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y\n:::\nwhere diag(p) is the identity matrix \\(\\mathbf{I}\\).\n\nFinally, let’s print the results:\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r)),\n  3\n)\n\n    twostep\nb0   20.091\nb1   -0.194\nb2    1.366\nb3   -1.373\nb4    0.438\nb5   -3.389\nb6    1.361\nb7    0.162\nb8    1.243\nb9    0.496\nb10  -0.460\n\n\nIt is important to note the effect of centering and scaling. When fitting ridge regression, many sources recommend centering the data. This allows separating the estimation of the intercept from the estimation of the regression coefficients. As a result, only the regression coefficients are penalised. To understand the effect of centering, consider what happens in regular OLS estimation when predictors are centered:\n\n# Centering in regular OLS -----------------------------------------------------\n\n# Create a version of X that is centered\nX_center &lt;- scale(X, center = TRUE, scale = FALSE)\n\n# Fit an regular linear model\nlm_ols &lt;- lm(y ~ X_center)\n\n# Check that b0 is equal to the mean of y\ncoef(lm_ols)[\"(Intercept)\"] - mean(y)\n\n  (Intercept) \n-3.552714e-15 \n\n\nFurthermore, let’s see what would have happened if we had penalised the intercept as well.\n\n# Consequence of penalising the intercept --------------------------------------\n\n# Add a vector of 1s to penalise the intercept\nX_scale_w1 &lt;- cbind(1, X_scale)\n\n# Compute the cross-product matrix of the data\nXtX &lt;- t(X_scale_w1) %*% X_scale_w1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r_w1 &lt;- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r),\n             onestep = c(b0 = bs_hat_r_w1[1],\n                         b = bs_hat_r_w1[-1])),\n  3\n)\n\n    twostep onestep\nb0   20.091  20.028\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460\n\n\nAs you see, the intercept would be shrunk toward zero, without any benefit. As a result, any prediction would also be offset by the same amount.\n\nAn alternative way to avoid penalising the intercept\nIt can be handy to obtain estimates of the regression coefficients and intercept in one step. We can use matrix algebra and R to simplify the two-step procedure to a single step. In particular, we can avoid the penalisation of the intercept by setting to 0 the first element of the “penalty” matrix lambda * diag(p + 1).\n\n# Alternative to avoid penalization of the intercept ---------------------------\n\n# Compute cross-product matrix\nXtX &lt;- crossprod(X_scale_w1)\n\n# Create penalty matrix\npen &lt;- lambda * diag(p + 1)\n\n# replace first element with 0\npen[1, 1] &lt;- 0\n\n# Obtain standardized estimates\nbs_hat_r2 &lt;- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)\n\n# Compare\nround(\n        data.frame(\n                twostep = c(b0 = b0_hat_r, b = bs_hat_r),\n                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])\n        ),\n        3\n)\n\n    twostep onestep\nb0   20.091  20.091\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460"
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "href": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "title": "Estimating ridge regression in R",
    "section": "Fit ridge regression with glmnet",
    "text": "Fit ridge regression with glmnet\nThe most popular R package to fit regularised regression is glmnet. Let’s see how we can replicate the results we obtained with the manual approach with glmnet. There are three important differences to consider:\n\nglmnet uses the biased sample variance estimate when scaling the predictors;\nglmnet returns the unstandardized regression coefficients;\nglmnet uses a different parametrization for \\(\\lambda\\).\n\nTo obtain the same results with the manual approach and glmnet we need to account for these.\n\nUse the biased estimation of variance\nFirst, let’s use the biased sample variance estimate in computing \\(\\hat{\\beta}^{\\text{ridge}}\\) with the manual approach:\n\n# Fitting ridge manually with biased variance estimation -----------------------\n\n# Standardize X\nX_scale &lt;- sapply(1:p, function (j){\n  muj &lt;- mean(X[, j])                  # mean\n  sj &lt;- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd\n  (X[, j] - muj) / sj                  # center and scale\n})\n\n# Craete the desing matrix\nX_scale_dm &lt;- cbind(1, X_scale)\n\n# Compute cross-product matrix\nXtX &lt;- crossprod(X_scale_dm)\n\n# Create penalty matrix\npen &lt;- lambda * diag(p + 1)\npen[1, 1] &lt;- 0\n\n# Obtain standardized estimates\nbs_hat_r3 &lt;- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)\n\n# Print results\nround(\n      data.frame(\n              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])\n      ),\n      3\n)\n\n    manual\nb0  20.091\nb1  -0.191\nb2   1.353\nb3  -1.354\nb4   0.430\nb5  -3.343\nb6   1.343\nb7   0.159\nb8   1.224\nb9   0.488\nb10 -0.449\n\n\n\n\nReturn the unstandardized coefficients\nNext, we need to revert these regression coefficients to their original scale. Since we are estimating the regression coefficients on the scaled data, they are computed on the standardized scale.\n\n# Return the  unstandardized coefficients --------------------------------------\n\n# Extract the original mean and standard deviations of all X variables\nmean_x &lt;- colMeans(X)\nsd_x &lt;- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version\n\n# Revert to original scale\nbs_hat_r4 &lt;- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),\n               bs_hat_r3[-1] / sd_x)\n\n# Compare manual standardized and unstandardized results\nround(\n      data.frame(\n              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),\n              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])\n      ),\n      3\n)\n\n    standardized unstandardized\nb0        20.091         12.908\nb1        -0.191         -0.109\nb2         1.353          0.011\nb3        -1.354         -0.020\nb4         0.430          0.818\nb5        -3.343         -3.471\nb6         1.343          0.764\nb7         0.159          0.320\nb8         1.224          2.491\nb9         0.488          0.672\nb10       -0.449         -0.282\n\n\n\n\nAdjust the parametrization of \\(\\lambda\\) for glmnet\nNext, we need to understand the relationship between the \\(\\lambda\\) parametrization we used and the one used by glmnet. The following code shows that if we want to use a given value of lambda in glmnet we need to multiply it by the standard deviation of the dependent variable (sd_y) and divide it by the sample size (n).\n\n# Adjust the parametrization of lambda -----------------------------------------\n\n# Extract the original mean and standard deviations of y (for lambda parametrization)\nmean_y &lt;- mean(y)\nsd_y &lt;- sqrt(var(y) * (n - 1) / n)\n\n# Compute the value glmnet wants for your target lambda\nlambda_glmnet &lt;- sd_y * lambda / n\n\n\n\nCompare manual and glmnet ridge regression output\nFinally, we can compare the results:\n\n# Fitting ridge regression with glmnet -----------------------------------------\n\n# Fit glmnet\nfit_glmnet_s &lt;- glmnet(x = X,\n                       y = y,\n                       alpha = 0,\n                       lambda = lambda_glmnet, # correction for how penalty is used\n                       thresh = 1e-20)\n\nbs_glmnet &lt;- coef(fit_glmnet_s)\n\n# Compare estimated coefficients\nround(\n      data.frame(\n        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),\n        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])\n      ),\n      3\n)\n\n       manual glmnet\nb0     12.908 12.908\nb.cyl  -0.109 -0.109\nb.disp  0.011  0.011\nb.hp   -0.020 -0.020\nb.drat  0.818  0.818\nb.wt   -3.471 -3.471\nb.qsec  0.764  0.764\nb.vs    0.320  0.320\nb.am    2.491  2.491\nb.gear  0.672  0.672\nb.carb -0.282 -0.282"
  },
  {
    "objectID": "posts/series-sm-course/boxplots.html",
    "href": "posts/series-sm-course/boxplots.html",
    "title": "Understanding boxplots",
    "section": "",
    "text": "Boxplots are descriptive tools to visualize the distribution of variables with a focus on their measures of spread and center. A boxplots report in the same figure the median, the 1st and 3rd quartiles, and indicate possible outliers.\nImagine wanting to plot the distribution of age in a court of students enrolled in a master program at a university. The age of the students is likely to be normally distributed around a mean of 26.\n\n# Set up ----------------------------------------------------------------------\n\n# Set seed\nset.seed(20220906)\n\n# Generate some age variable for a university master programme\nage &lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)\n\nThen, we can create the boxplot of this age variable in R by using the boxplot() function.\n\n# Look at the boxplot ---------------------------------------------------------\nboxplot(age)\n\n\n\n\n\n\nThe variable age is centered around 26 and 50% of the distribution is located between 25 (1st quartile) and 27 (3rd quartile). There are 6 values that represent possible outliers (the circles outside the whiskers)."
  },
  {
    "objectID": "posts/series-sm-course/boxplots.html#tldr-just-give-me-the-code",
    "href": "posts/series-sm-course/boxplots.html#tldr-just-give-me-the-code",
    "title": "Understanding boxplots",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Set up ----------------------------------------------------------------------\n\n# Set seed\nset.seed(20220906)\n\n# Generate some age variable for a university master programme\nage &lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)\n\n# Look at the boxplot ---------------------------------------------------------\nboxplot(age)\n\n# Boxplot with explanation\nC &lt;- 1.5 # range multiplier\nboxplot(age, range = C)\n\n# Add arrows pointings to statistics\narrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,\n       x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,\n       length = 0.1)\n\n# Add labels of statistics\ntext(x = rep(.66, 5),\n     y = boxplot.stats(age, coef = C)$stats,\n     labels = c(\"lower whisker\",\n                \"1st quartile\",\n                \"median\",\n                \"3rd quartile\",\n                \"upper whisker\"),\n     adj = 1)\n\n# Add y axis labels\naxis(side = 2, at = boxplot.stats(age, coef = C)$stats[c(1, 3, 4)], labels = TRUE)\n\n# Compute boxplot statistics manually ------------------------------------------\n\n# Compute the median\nmed &lt;- median(age)\n\n# Compute 1st and 3rd quartiles\nqnt &lt;- quantile(age, probs = c(.25, .75))\n\n# Compute interquartile range\nIQR &lt;- diff(qnt)[[1]]\n\n# Compute fences/whisker bounds\nC &lt;- 1.5 # range multiplier\nfences &lt;- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)\n\n# Put together the boxplot stats\nbxstats &lt;- sort(c(med = med, qnt, f = fences))\n\n# Compute boxplot statistics with R function\nbxstats_auto &lt;- boxplot.stats(age, coef = C)$stats\n\n# Compare results obtain manually and with the R function\ndata.frame(manual = bxstats, R.function = bxstats_auto)\n\n# Visualize the effect of different C -----------------------------------------\n\n# Allow two plots one next to the other\npar(mfrow = c(1, 2))\n\n# Plot C = 1.5 and 3\nlapply(c(1.5, 3.0), FUN = function (x){\n  C &lt;- x\n  boxplot(age, range = C, main = paste0(\"C = \", C))\n\n  # Add arrows pointings to statistics\n  arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,\n         x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,\n         length = 0.1)\n  # Add labels of statistics\n  text(x = rep(.66, 5),\n       y = boxplot.stats(age, coef = C)$stats,\n       labels = c(paste(ifelse(C == 1.5, \"inner\", \"outer\"), \"fence \\n lower bound\"),\n                  \"1st quartile\",\n                  \"median\",\n                  \"3rd quartile\",\n                  paste(ifelse(C == 1.5, \"inner\", \"outer\"), \"fence \\n upper bound\")),\n       adj = 1)\n})"
  },
  {
    "objectID": "posts/series-sm-course/qunatiles.html",
    "href": "posts/series-sm-course/qunatiles.html",
    "title": "Understanding quantiles",
    "section": "",
    "text": "Quantiles, percentiles, and quartiles\nDescriptive statistics are a fundamental tools for a data analysis. Their purpose is to describe and summarize data. Quantitative variables such as weight, age, and income can be described by distributions (e.g., normal distribution) with measures of center (the typical value of a variable) and variability (spread around the center). The mean is a measure of center. The standard deviation is a measure of variability.\nThere are special descriptive statistics that help us describe simultaneously center and spread of a variable’s distribution. These measures are often known as measures of positions. In general, they tell us the point of the distribution of a variable at which a given percentage of the data falls below or above that point. The minimum and maximum values of a variable are measures of positions defining the point at which no data, or all data, fall below it, respectively. The median is a measure of position that defines the point at which half of the data falls below it (and above it). The median is a special case of the measure of position called percentile. The p-th percentile is the point such that p% of the observations fall below or at that point and (100 - p)% fall above it. The 50-th percentile is the median.\nQuartiles are commonly used percentiles. The first quartile is the 25-th percentile, the value of the variable leaving to its left 25% of the distribution. The third quartile is the 75-th percentile, the value of the variable leaving to its left 75% of the distribution. You can think of quartiles as diving the probability distribution in 4 intervals with equal probabilities (e.g., area under the probability density function). Similarly, you can think of percentiles as diving the probability distribution in 100 intervals with equal probabilities.\n\n\n\n\n\nWe use the word quantile to describe the general measure of position that divides the probability distribution in intervals with equal probabilities. Percentiles divide the probability distribution of a variable in 100 intervals. Deciles divide it in 10 intervals, and quartiles in four. As such, percentiles, deciles, quartiles, and the median are all special cases of quantiles.\n\n\nTL;DR, just give me the code!\n\n# Set a seed\n\nset.seed(20220906)\n\n# Sample from a normal distribution\n\nage &lt;- rnorm(1e5, mean = 27, sd = 2)\n\n# Define the 1st and 2nd quartile\n\nquartiles &lt;- quantile(age, probs = c(.25, .75))\n\n# Plot density distribution\n\nplot(density(age),\n     main = \"Quartiles for the probability distribution of age\",\n     xlab = NA)\n\n# Costum x ticks\naxis(side = 1, at = c(27, round(quartiles, 1)), labels = TRUE)\n\n# Add points for quantiles\npoints(c(quartiles, median(age)),\n        y = rep(0, length(quartiles)+1))\npoints(c(quartiles, median(age)),\n       y = dnorm(c(quartiles, median(age)), mean = mean(age), sd = sd(age)))\n\n# Add segments to devide plot\nsegments(x0 = quartiles[1], y0 = 0,\n         x1 = quartiles[1], y1 = dnorm(quartiles[1],\n                                       mean = mean(age), sd = sd(age)))\nsegments(x0 = median(age), y0 = 0,\n         x1 = median(age), y1 = max(dnorm(age,\n                                       mean = mean(age), sd = sd(age))))\nsegments(x0 = quartiles[2], y0 = 0,\n         x1 = quartiles[2], y1 = dnorm(quartiles[2],\n                                       mean = mean(age), sd = sd(age)))\n\n# Add quartile labels\ntext(x = quartiles[1],\n     y = -.005,\n     \"1st quartile\")\ntext(x = quartiles[2],\n     y = -.005,\n     \"3rd quartile\")\n\n# Add percentage under the curve labels\ntext(x = c(24, 30, 26.3, 27.7),\n     y = c(.03, .03, .06, .06),\n     \"25 %\")"
  },
  {
    "objectID": "series-sm-course.html",
    "href": "series-sm-course.html",
    "title": "Course: Statistics and Methodology",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-sampling.html",
    "href": "series-sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\n\n\nNo matching items"
  }
]