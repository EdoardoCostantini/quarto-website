[
  {
    "objectID": "series-pca.html",
    "href": "series-pca.html",
    "title": "A primer on PCA",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series-sm-course.html",
    "href": "series-sm-course.html",
    "title": "Course: Statistics and Methodology",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a Ph.D. candidate at Tilburg University working on multiple imputation algorithms for survey data."
  },
  {
    "objectID": "series-sampling.html",
    "href": "series-sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 13, 2022\n\n\nPrincipal covariates regression in R\n\n\nSupervised learning\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\nApr 22, 2022\n\n\nCross-entropy as a measure of predictive performance\n\n\nMachine Learning\n\n\n\n\nMar 14, 2022\n\n\nEstimating the weighted covariance matrix in R\n\n\nStatistics\n\n\n\n\nFeb 28, 2022\n\n\nEstimating ridge regression in R\n\n\nPenalty\n\n\n\n\nNov 17, 2021\n\n\nThe sweep operator\n\n\nThe EM algorithm,Matrix algebra,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html",
    "href": "posts/series-pca-primer/pcovr.html",
    "title": "Principal covariates regression in R",
    "section": "",
    "text": "Principal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables. Principal covariates regression (PCovR) is an alternative approach that modifies the optimization criteria behind PCA to include a supervision aspect . PCovR looks for a low-dimensional representation of \\(X\\) that accounts for the maximum amount of variation in both \\(X\\) and \\(y\\). Compared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of \\(X\\) variables and that correlate well with a set of \\(Y\\) variables. For more information, I recommend reading Vervloet et al. (2015) and De Jong and Kiers (1992). In this post, you can find my R code notes on this method. In these notes, I show the computations used by the PCovR R-package to perform the method.\nTo understand how PCovR differs from classical PCR we need to complicate the notation. Consider the following decomposition of the data:\n\\[\n\\begin{align}\n    \\mathbf{T} &= \\mathbf{X} \\mathbf{W} \\\\\n    \\mathbf{X} &= \\mathbf{T} \\mathbf{P}_X + \\mathbf{E}_X  \\\\\n    y      &= \\mathbf{T} \\mathbf{P}_y + \\mathbf{e}_y\n\\end{align}\n\\]\nwhere \\(\\mathbf{T}\\) is the matrix of PCs defined above, \\(\\mathbf{W}\\) is a \\(p \\times q\\) matrix of component weights defining what linear combination of the columns of \\(\\mathbf{X}\\) is used to compute the components, and \\(\\mathbf{P}_X\\) and \\(\\mathbf{P}_y\\) are the \\(q \\times p\\) and \\(q \\times 1\\) loading matrices relating the variables in \\(\\mathbf{X}\\) and \\(y\\) to the component scores in \\(\\mathbf{T}\\), respectively. \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\) are the reconstruction errors, the information lost by using \\(\\mathbf{T}\\) as summary of \\(\\mathbf{X}\\).\nClassical PCA can be formulated as the task of finding the \\(\\mathbf{W}\\) and \\(\\mathbf{P}_X\\) that minimize the reconstruction error \\(\\mathbf{E}_X\\):\n\\[\\begin{equation}\n    (\\mathbf{W}, \\mathbf{P}) = \\underset{\\mathbf{W}, \\mathbf{P}_X}{\\operatorname{argmin}} \\lVert \\mathbf{X} - \\mathbf{XWP}' \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\). PCovR can be formulated as the task of minimizing a weighted combination of both \\(\\mathbf{E}_X\\) and \\(\\mathbf{e}_y\\):\n\\[\n\\begin{equation}\\label{eq:PCovR}\n    (\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y) = \\underset{\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y}{\\operatorname{argmin  }} \\alpha \\lVert (\\mathbf{X} - \\mathbf{XWP}_X') \\rVert^2 + (1 - \\alpha) \\lVert (y - \\mathbf{XWP}_y') \\rVert^2\n\\end{equation}\n\\]\nsubject to the constraint \\(\\mathbf{P}' \\mathbf{P} = \\mathbf{I}\\).\nThe parameter \\(\\alpha\\) defines which reconstruction error is being prioritized. When \\(\\alpha = 1\\), the emphasis is exclusively placed on reconstructing \\(\\mathbf{X}\\), leading PCovR to PCR. When \\(\\alpha = 0.5\\), the importance of \\(\\mathbf{X}\\) and \\(y\\) is equally weighted, a case that resembles Partial least square, which we discuss below. In practice, its value can be found by cross-validation or according to a sequential procedure based on maximum likelihood principles (Vervloet et al. 2013). In particular, \\[\n\\begin{equation}\\label{eq:aml}\n    \\alpha_{ML} = \\frac{\\lVert \\mathbf{X} \\lVert^2}{\\lVert \\mathbf{X} \\lVert^2  + \\lVert y \\lVert^2 \\frac{\\hat{\\sigma}_{\\mathbf{E}_X}^2}{\\hat{\\sigma}_{e_y}^2}}\n\\end{equation}\n\\]\nwhere \\(\\hat{\\sigma}_{\\mathbf{E}_X}^2\\) can be obtained as the unexplained variance by components computed according to classical PCA and \\(\\hat{\\sigma}_{e_y}^2\\) can be estimated as the unexplained variance by the linear model regressing \\(y\\) on \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "href": "posts/series-pca-primer/pcovr.html#reverting-to-pca",
    "title": "Principal covariates regression in R",
    "section": "Reverting to PCA",
    "text": "Reverting to PCA\nI mentioned before that when \\(\\alpha = 1\\), PCovR reduces to PCR. Let’s see that in action. First, we set the desired value for \\(\\alpha\\):\n\n# Reverting to PCA -------------------------------------------------------------\n\n# Use alpha 1\nalpha <- 1\n\nthen, we can perform PCovR\n\n# Estimate PCovR\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,\n    r = npcs # fixed number of components\n)\n\nand classical PCA according to the Guerra-Urzola et al. (2021)\n\n# Classical PCA\nuSVt <- svd(X)\nU <- uSVt$u\nD <- diag(uSVt$d)\nV <- uSVt$v\nI <- nrow(X)                              # Define the number of rows\nP_hat <- (I - 1)^{-1 / 2} * V %*% D       # Component loadings\nW_hat <- (I - 1)^{1 / 2} * V %*% solve(D) # Component weights\nT_hat <- (I - 1)^{1 / 2} * U              # Component scores\nT_hat <- X %*% W_hat\n\nWe can now compare the results again\n\n# The scores obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$Te, T_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The loadings obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(t(package$Px), P_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# The weights obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$W, W_hat[, 1:npcs])$tucker_value\n\n[1] 1\n\n# P is now proportional to W\nTuckerCoef(package$W, t(package$Px))$tucker_value\n\n[1] 1"
  },
  {
    "objectID": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "href": "posts/series-pca-primer/pcovr.html#maximum-likelihood-estimation-of-alpha",
    "title": "Principal covariates regression in R",
    "section": "Maximum Likelihood estimation of \\(\\alpha\\)",
    "text": "Maximum Likelihood estimation of \\(\\alpha\\)\nThe value of \\(\\alpha\\) is not usually chosen arbitrarily. One could cross-validate it or compute it with a closed-form solution that relies on the Maximum likelihood approach (Vervloet et al. 2016). Here, I show how to use this latter approach. First, let’s simply fit PCovR by using the PCovR::pcovr() function and setting the model selection argument set to “seq”. As explained in the help-file, this “implies a sequential procedure in which the weighting value is determined on the basis of maximum likelihood principles”, exactly what we want.\n\n# Maximum likelihood tuning of alpha -------------------------------------------\n\n# Fit PCovR\npackage <- pcovr(\n    X = X_raw,\n    Y = y_raw,\n    rot = \"none\",\n    R = npcs, # fixed number of components\n    modsel = \"seq\" # fastest option\n)\n\nThen, we can compute the maximum likelihood value of \\(\\alpha\\) by first computing the error terms and taking their ratio.\n\n# Compute error ratio components\nlm_mod <- lm(y ~ -1 + X)\nery <- 1 - summary(lm_mod)$r.squared\n\nRmin <- npcs\nRmax <- npcs\nsing <- svd(X)\nvec <- Rmin:Rmax\nvec <- c(vec[1] - 1, vec, vec[length(vec)] + 1)\nVAF <- c(0, cumsum(sing$d^2) / sum(sing$d^2))\nVAF <- VAF[vec + 1]\nscr <- array(NA, c(1, length(vec)))\nfor (u in 2:(length(vec) - 1)) {\n    scr[, u] <- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])\n}\nerx <- 1 - VAF[which.max(scr)]\n\nWe could have computed the error ratio with the PCovR::err() R function:\n\n# Compute error ratio with function\nerr <- ErrorRatio(\n    X = X,\n    Y = y,\n    Rmin = npcs,\n    Rmax = npcs\n)\n\n# Is it the same?\nerr - erx/ery\n\n[1] 1.110223e-15\n\n\nWith this value, it is now easy to compute \\(\\alpha\\):\n\n# Find alpha ML\nalpha_ML <- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)\n\n# Compare to one found by package\npackage$a - alpha_ML\n\n[1] 0"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2023\n\n\nSupervised principal component regression imputation in MICE.\n\n\nWork in progress\n\n\n\n\n2023\n\n\nSolving the many variables problem in MICE with principal component regression.\n\n\nCompleted\n\n\n\n\n2022\n\n\nHigh-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.\n\n\nWork in progress\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/mi-pcr.html",
    "href": "publications/mi-pcr.html",
    "title": "Solving the many variables problem in MICE with principal component regression.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Klaas Sijtsma, and Tim Reeskens. 2022. “Solving the \"Many Variables\" Problem in MICE with Principal Component Regression.” arXiv Preprint arXiv:2206.15107."
  },
  {
    "objectID": "posts/series-knowledge/ridge.html",
    "href": "posts/series-knowledge/ridge.html",
    "title": "Estimating ridge regression in R",
    "section": "",
    "text": "When there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance. This problem can be alleviated by imposing a size constraint (or penalty) on the coefficients. Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients values minimize a penalized residual sum of squares:\n\\[\n\\hat{\\beta}^{\\text{ridge}} = \\text{argmin}_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\right\\}\n\\]\nThe ridge solutions are not equivariant under scaling of the inputs. Therefore, it is recommended to standardize the inputs before solving the minimization problem.\nNotice that the intercept \\(\\beta_0\\) has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for \\(Y\\). Furthermore, by centering the predictors, we can separate the solution to the minimazion problem into two parts:\n\nIntercept \\[\n\\hat{\\beta}_0 = \\bar{y}=\\frac{1}{N}\\sum_{i = 1}^{N} y_i\n\\]\nPenalised regression coefficients \\[\n\\hat{\\beta}^{\\text{ridge}}=(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^Ty\n\\] which is the regular way of estimating regression coefficients with a penalty term (\\(\\lambda\\)) added on the diagonal (\\(\\mathbf{I}\\)) of the cross-product matrix (\\(\\mathbf{X}^T\\mathbf{X}\\)) to make it invertible (\\((...)^{-1}\\))."
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "href": "posts/series-knowledge/ridge.html#fitting-ridge-regression-manually",
    "title": "Estimating ridge regression in R",
    "section": "Fitting ridge regression manually",
    "text": "Fitting ridge regression manually\nFirst, let’s make sure the predictors are centered on the mean and scaled to have a variance of 1.\n\n# Fitting ridge regression manually --------------------------------------------\n\n# Scale the data (standardize)\nX_scale <- scale(X, center = TRUE, scale = TRUE)\n\nThen, we want to fit the ridge regression manually by separating the intercept and the regression coefficients estimation (two-step approach):\n\nEstimate the intercept (\\(\\hat{\\beta}_0\\))\n::: {.cell}\n# Estimate the intercept\nb0_hat_r <- mean(y)\n:::\nEstimate the ridge regression coefficients (\\(\\hat{\\beta}^{\\text{ridge}}\\)).\n\n\nCompute the cross-product matrix of the predictors.\nThis is the same step we would take if we wanted to compute the OLS estimates.\n::: {.cell}\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale) %*% X_scale\n:::\nDefine a value of \\(\\lambda\\).\nThis value is usually chosen by cross-validation from a grid of possible values. However, here we are only interested in how \\(\\lambda\\) is used in the computation, so we can simply give it a fixed value.\n::: {.cell}\n# Define a lambda value\nlambda <- .1\n:::\nCompute \\(\\hat{\\beta}^{\\text{ridge}}\\).\n::: {.cell}\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r <- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y\n:::\nwhere diag(p) is the identity matrix \\(\\mathbf{I}\\).\n\nFinally, let’s print the results:\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r)),\n  3\n)\n\n    twostep\nb0   20.091\nb1   -0.194\nb2    1.366\nb3   -1.373\nb4    0.438\nb5   -3.389\nb6    1.361\nb7    0.162\nb8    1.243\nb9    0.496\nb10  -0.460\n\n\nIt is important to note the effect of centering and scaling. When fitting ridge regression, many sources recommend centering the data. This allows separating the estimation of the intercept from the estimation of the regression coefficients. As a result, only the regression coefficients are penalised. To understand the effect of centering, consider what happens in regular OLS estimation when predictors are centered:\n\n# Centering in regular OLS -----------------------------------------------------\n\n# Create a version of X that is centered\nX_center <- scale(X, center = TRUE, scale = FALSE)\n\n# Fit an regular linear model\nlm_ols <- lm(y ~ X_center)\n\n# Check that b0 is equal to the mean of y\ncoef(lm_ols)[\"(Intercept)\"] - mean(y)\n\n  (Intercept) \n-3.552714e-15 \n\n\nFurthermore, let’s see what would have happened if we had penalised the intercept as well.\n\n# Consequence of penalising the intercept --------------------------------------\n\n# Add a vector of 1s to penalise the intercept\nX_scale_w1 <- cbind(1, X_scale)\n\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale_w1) %*% X_scale_w1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r_w1 <- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r),\n             onestep = c(b0 = bs_hat_r_w1[1],\n                         b = bs_hat_r_w1[-1])),\n  3\n)\n\n    twostep onestep\nb0   20.091  20.028\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460\n\n\nAs you see, the intercept would be shrunk toward zero, without any benefit. As a result, any prediction would also be offset by the same amount.\n\nAn alternative way to avoid penalising the intercept\nIt can be handy to obtain estimates of the regression coefficients and intercept in one step. We can use matrix algebra and R to simplify the two-step procedure to a single step. In particular, we can avoid the penalisation of the intercept by setting to 0 the first element of the “penalty” matrix lambda * diag(p + 1).\n\n# Alternative to avoid penalization of the intercept ---------------------------\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_w1)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\n\n# replace first element with 0\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r2 <- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)\n\n# Compare\nround(\n        data.frame(\n                twostep = c(b0 = b0_hat_r, b = bs_hat_r),\n                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])\n        ),\n        3\n)\n\n    twostep onestep\nb0   20.091  20.091\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460"
  },
  {
    "objectID": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "href": "posts/series-knowledge/ridge.html#fit-ridge-regression-with-glmnet",
    "title": "Estimating ridge regression in R",
    "section": "Fit ridge regression with glmnet",
    "text": "Fit ridge regression with glmnet\nThe most popular R package to fit regularised regression is glmnet. Let’s see how we can replicate the results we obtained with the manual approach with glmnet. There are three important differences to consider:\n\nglmnet uses the biased sample variance estimate when scaling the predictors;\nglmnet returns the unstandardized regression coefficients;\nglmnet uses a different parametrization for \\(\\lambda\\).\n\nTo obtain the same results with the manual approach and glmnet we need to account for these.\n\nUse the biased estimation of variance\nFirst, let’s use the biased sample variance estimate in computing \\(\\hat{\\beta}^{\\text{ridge}}\\) with the manual approach:\n\n# Fitting ridge manually with biased variance estimation -----------------------\n\n# Standardize X\nX_scale <- sapply(1:p, function (j){\n  muj <- mean(X[, j])                  # mean\n  sj <- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd\n  (X[, j] - muj) / sj                  # center and scale\n})\n\n# Craete the desing matrix\nX_scale_dm <- cbind(1, X_scale)\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_dm)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r3 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)\n\n# Print results\nround(\n      data.frame(\n              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])\n      ),\n      3\n)\n\n    manual\nb0  20.091\nb1  -0.191\nb2   1.353\nb3  -1.354\nb4   0.430\nb5  -3.343\nb6   1.343\nb7   0.159\nb8   1.224\nb9   0.488\nb10 -0.449\n\n\n\n\nReturn the unstandardized coefficients\nNext, we need to revert these regression coefficients to their original scale. Since we are estimating the regression coefficients on the scaled data, they are computed on the standardized scale.\n\n# Return the  unstandardized coefficients --------------------------------------\n\n# Extract the original mean and standard deviations of all X variables\nmean_x <- colMeans(X)\nsd_x <- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version\n\n# Revert to original scale\nbs_hat_r4 <- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),\n               bs_hat_r3[-1] / sd_x)\n\n# Compare manual standardized and unstandardized results\nround(\n      data.frame(\n              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),\n              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])\n      ),\n      3\n)\n\n    standardized unstandardized\nb0        20.091         12.908\nb1        -0.191         -0.109\nb2         1.353          0.011\nb3        -1.354         -0.020\nb4         0.430          0.818\nb5        -3.343         -3.471\nb6         1.343          0.764\nb7         0.159          0.320\nb8         1.224          2.491\nb9         0.488          0.672\nb10       -0.449         -0.282\n\n\n\n\nAdjust the parametrization of \\(\\lambda\\) for glmnet\nNext, we need to understand the relationship between the \\(\\lambda\\) parametrization we used and the one used by glmnet. The following code shows that if we want to use a given value of lambda in glmnet we need to multiply it by the standard deviation of the dependent variable (sd_y) and divide it by the sample size (n).\n\n# Adjust the parametrization of lambda -----------------------------------------\n\n# Extract the original mean and standard deviations of y (for lambda parametrization)\nmean_y <- mean(y)\nsd_y <- sqrt(var(y) * (n - 1) / n)\n\n# Compute the value glmnet wants for your target lambda\nlambda_glmnet <- sd_y * lambda / n\n\n\n\nCompare manual and glmnet ridge regression output\nFinally, we can compare the results:\n\n# Fitting ridge regression with glmnet -----------------------------------------\n\n# Fit glmnet\nfit_glmnet_s <- glmnet(x = X,\n                       y = y,\n                       alpha = 0,\n                       lambda = lambda_glmnet, # correction for how penalty is used\n                       thresh = 1e-20)\n\nbs_glmnet <- coef(fit_glmnet_s)\n\n# Compare estimated coefficients\nround(\n      data.frame(\n        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),\n        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])\n      ),\n      3\n)\n\n       manual glmnet\nb0     12.908 12.908\nb.cyl  -0.109 -0.109\nb.disp  0.011  0.011\nb.hp   -0.020 -0.020\nb.drat  0.818  0.818\nb.wt   -3.471 -3.471\nb.qsec  0.764  0.764\nb.vs    0.320  0.320\nb.am    2.491  2.491\nb.gear  0.672  0.672\nb.carb -0.282 -0.282"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2023\n\n\nSupervised principal component regression imputation in MICE.\n\n\nWork in progress\n\n\n\n\n2023\n\n\nSolving the many variables problem in MICE with principal component regression.\n\n\nCompleted\n\n\n\n\n2022\n\n\nHigh-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.\n\n\nWork in progress\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/mi-spcr.html",
    "href": "publications/mi-spcr.html",
    "title": "Supervised principal component regression imputation in MICE.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Tim Reeskens, and Klaas Sijtsma. 2022. “Supervised Principal Component Regression for Multiple Imputation by Chained Equations.” Work in Progress."
  },
  {
    "objectID": "publications/mi-hd.html",
    "href": "publications/mi-hd.html",
    "title": "High-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Tim Reeskens, and Klaas Sijtsma. 2022. “High-Dimensional Imputation for the Social Sciences: A Comparison of State-of-the-Art Methods.” arXiv Preprint arXiv:2208.13656."
  },
  {
    "objectID": "research/mi-spcr.html",
    "href": "research/mi-spcr.html",
    "title": "Supervised principal component regression imputation in MICE.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Tim Reeskens, and Klaas Sijtsma. 2022. “Supervised Principal Component Regression for Multiple Imputation by Chained Equations.” Work in Progress."
  },
  {
    "objectID": "research/mi-hd.html",
    "href": "research/mi-hd.html",
    "title": "High-dimensional imputation for the social sciences: a comparison of state-of-the-art methods.",
    "section": "",
    "text": "Citation\n\nCostantini, Edoardo, Kyle M Lang, Tim Reeskens, and Klaas Sijtsma. 2022. “High-Dimensional Imputation for the Social Sciences: A Comparison of State-of-the-Art Methods.” arXiv Preprint arXiv:2208.13656."
  }
]