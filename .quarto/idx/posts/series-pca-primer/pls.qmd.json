{"title":"Implementing a PLS alogirthm in R","markdown":{"yaml":{"draft":false,"title":"Implementing a PLS alogirthm in R","author":"Edoardo Costantini","date":"2022-06-13","slug":"pls","categories":["Supervised learning"],"bibliography":"../../resources/bibshelf.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nMany data analysts face the problem of analyzing data sets with many, often highly correlated, variables. \nPartial Least Square Regression (PLSR) is a regression method that uses linear combinations of the original predictors to reduce their dimensionality.\nAs principal component regression, PLS uses derived inputs, however, it differs from PCR by how the linear combinations are constructed.\n\nGiven a set of predictors $X_{n \\times p}$ and a vector of dependent variable scores $y_{n \\times 1}$, the least-square solution for the multiple linear regression\n\n$$\ny = X \\beta + \\epsilon\\text{, with } \\epsilon \\sim N(0, \\sigma^2)\n$$\n\nis \n\n$$\n\\beta = (X'X)^{-1}X'y\n$$\n\nwhere $X'$ is the transpose of the data matrix $X$, and $()^{-1}$ is the matrix inverse.\nWhen collinearity is present in $X$ or $p > n$, then $X'X$ is singular and its inverse cannot be computed.\nDerived input regression methods like PCR and PLSR bypass this problem by taking linear combinations of the columns of the original $X$ and regressing $Y$ on just a few of these linear combinations.\nThe peculiarity of PLSR is that it includes information on both $X$ and $Y$ in the definition of the linear combinations.\nIn this post, we look at two algorithms to estimate PLSR to get a better understanding of the method.\n\n## Popular algorithms to implement PLS\n\nHere, I describe informally the algorithm steps:\n\n1. Preprocessing the data - the columns of the input matrix $X$ are standardized to have mean 0 and variance 1.\n2. The cross-product of every column of $X$ and $y$ is computed: $\\rho_{1j} = x_{j}' y$ for $j = 1, \\dots, p$. \n3. Compute the first partial-least-square direction $z_1$ - The cross-products $\\rho_{1j}$ are used as weights to obtain a linear combination of the columns: $z_1 = \\sum \\rho_{1j} x_{j}$. This implies that the contribution of each column to $z_1$ is weighted by their univariate relationship with the dependent variable $y$.\n4. Regression of $y$ on $z_1$ - The outcome variable $y$ is regressed on this first direction $z_1$ to obtain $\\hat{\\theta}_1$.\n5. Orthogonalization of $X$ - All columns of $X$ are orthogonalized with respect to $z_1$.\n6. The cross-product of every column of $X$ and $y$ is computed again: $\\rho_{2j} = x_{j}' y$ for $j = 1, \\dots, p$.\n7. Compute the second partial-least-square direction $z_2$ - The cross-products $\\rho_{2j}$ are used as weights to obtain a linear combination of the columns: $z_2 = \\sum \\rho_{2j} x_{j}$. Notice that the columns $x_j$ we are using now are orthogonal to the previous partial least square direction $z_1$.\n8. Regression of $y$ on $z_2$ - The outcome variable $y$ is regressed on the second direction $z_2$ to obtain $\\hat{\\theta}_2$.\n9. Orthogonalization of $X$ - All columns of $X$ are orthogonalized with respect to $z_2$.\n\nThe procedure continues until $M$ partial least square directions have been computed.\nThe result is a set of independent directions that have both high variance and high correlation with the dependent variable, in contrast to PCR which finds a set of independent directions that have high variance.\n\nNow we report pseudo-code for the implementation of the PLS algorithm (inspired by Report Algorithm 3.3 p.103 as in [@hastieEtAl:2015]).\nWe will use it to write the R code in the next session.\n\n1. Standardized each $x_j$ to have mean 0 and variance 1\n2. Set:\n   - $\\hat{y}^{(0)} = \\bar{y}1$\n   - $x_{j}^{(0)} = x_{j}$\n3. For $m = 1, 2, \\dots, M$\n    a. $z_m = \\sum_{j = 1}^{p} \\rho_{mj}x_{j}^{(m-1)}$\n    b. $\\hat{\\theta}_m = \\frac{z_m'y}{z_m' z_m}$\n    c. $\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_m z_m$\n    d. for $j = 1, \\dots, p$ orthogonalize $x_{j}$ with respect to $z_m$: $x_{j}^{(m)} = x_{j}^{(m-1)} - \\frac{z_m' x_{j}^{(m)}}{z_m' z_m}z_m$\n4. Output the sequence of fitted vectors $\\hat{y}^{m}$\n\n# Learn by coding\n\n```{r functions, echo = FALSE}\n# Functions needed ----------------------------------------------------------------\n\n# Degrees of freedom for supervised derived input models\n\ndofPLS <- function(X, y, TT, Yhat, m = ncol(X), DoF.max = ncol(X) + 1){\n    # Example inputs\n    # X = scale(mtcars[, -1])\n    # y = mtcars[, 1]\n    # m = ncol(X)\n    # DoF.max = m + 1\n    # TT <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$TT # normalizezs PC scores\n    # Yhat <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$Yhat[, 2:(m + 1)]\n\n    # Body\n    n <- nrow(X)\n\n    # Scale data\n    mean.X <- apply(X, 2, mean)\n    sd.X <- apply(X, 2, sd)\n    sd.X[sd.X == 0] <- 1\n    X <- X - rep(1, nrow(X)) %*% t(mean.X)\n    X <- X / (rep(1, nrow(X)) %*% t(sd.X))\n    K <- X %*% t(X)\n\n    # pls.dof\n    DoF.max <- DoF.max - 1\n    TK <- matrix(, m, m)\n    KY <- krylov(K, K %*% y, m)\n    lambda <- eigen(K)$values\n    tr.K <- vector(length = m)\n    for (i in 1:m) {\n        tr.K[i] <- sum(lambda^i)\n    }\n    BB <- t(TT) %*% KY\n    BB[row(BB) > col(BB)] <- 0\n    b <- t(TT) %*% y\n    DoF <- vector(length = m)\n    Binv <- backsolve(BB, diag(m))\n    tkt <- rep(0, m)\n    ykv <- rep(0, m)\n    KjT <- array(dim = c(m, n, m))\n    dummy <- TT\n    for (i in 1:m) {\n        dummy <- K %*% dummy\n        KjT[i, , ] <- dummy\n    }\n    trace.term <- rep(0, m)\n\n    for (i in 1:m) {\n        Binvi <- Binv[1:i, 1:i, drop = FALSE]\n        ci <- Binvi %*% b[1:i]\n        Vi <- TT[, 1:i, drop = FALSE] %*% t(Binvi)\n        trace.term[i] <- sum(ci * tr.K[1:i])\n        ri <- y - Yhat[, i]\n        for (j in 1:i) {\n            KjTj <- KjT[j, , ]\n            tkt[i] <- tkt[i] + ci[j] * tr(t(TT[, 1:i, drop = FALSE]) %*%\n                KjTj[, 1:i, drop = FALSE])\n            ri <- K %*% ri\n            ykv[i] <- ykv[i] + sum(ri * Vi[, j])\n        }\n    }\n\n    DoF <- trace.term + 1:m - tkt + ykv\n\n    DoF[DoF > DoF.max] <- DoF.max\n    DoF <- c(0, DoF) + 1\n    # TODO: check that it is correct to add the 1 after checking the DoF max condition\n    DoF\n\n}\n\n# Extract single factor --------------------------------------------------------\n\ndofPLS_single <- function(X, y, q = 1, TT, Yhat){\n    # Example inputs\n    # X = scale(mtcars[, -1])\n    # y = mtcars[, 1]\n    # m = ncol(X)\n    # DoF.max = m + 1\n    # TT <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$TT # normalizezs PC scores\n    # q <- 3 # desired component / latent variable\n    # Yhat <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$Yhat[, (q + 1)]\n\n    # Body\n    n <- nrow(X)\n    m <- ncol(X)\n    DoF.max <- ncol(X) + 1\n\n    # Scale data\n    mean.X <- apply(X, 2, mean)\n    sd.X <- apply(X, 2, sd)\n    sd.X[sd.X == 0] <- 1\n    X <- X - rep(1, nrow(X)) %*% t(mean.X)\n    X <- X / (rep(1, nrow(X)) %*% t(sd.X))\n    K <- X %*% t(X)\n\n    # pls.dof\n    DoF.max <- DoF.max - 1\n    TK <- matrix(, m, m)\n    KY <- krylov(K, K %*% y, m)\n    lambda <- eigen(K)$values\n    tr.K <- vector(length = m)\n    for (i in 1:m) {\n        tr.K[i] <- sum(lambda^i)\n    }\n    BB <- t(TT) %*% KY\n    BB[row(BB) > col(BB)] <- 0\n    b <- t(TT) %*% y\n    DoF <- vector(length = m)\n    Binv <- backsolve(BB, diag(m))\n    tkt <- 0\n    ykv <- 0\n    KjT <- array(dim = c(q, n, m))\n    dummy <- TT\n    for (i in 1:q) {\n        dummy <- K %*% dummy\n        KjT[i, , ] <- dummy\n    }\n    trace.term <- 0\n\n    Binvi <- Binv[1:q, 1:q, drop = FALSE]\n    ci <- Binvi %*% b[1:q]\n    Vi <- TT[, 1:q, drop = FALSE] %*% t(Binvi)\n    trace.term <- sum(ci * tr.K[1:q])\n    ri <- y - Yhat\n    for (j in 1:q) {\n        KjTj <- KjT[j, , ]\n        tkt <- tkt + ci[j] * tr(t(TT[, 1:q, drop = FALSE]) %*%\n            KjTj[, 1:q, drop = FALSE])\n        ri <- K %*% ri\n        ykv <- ykv + sum(ri * Vi[, j])\n    }\n\n    DoF <- trace.term + q - tkt + ykv\n    DoF <- ifelse(DoF > DoF.max, DoF.max, DoF)\n    DoF <- DoF + 1\n    DoF\n}\n\n# Pls function manual\n\npls.manual <- function(ivs, dv, m = 1L){\n\n    # Parms\n    # M   <- 10 # number of \"components\"\n    # ivs <- yarn[[1]]\n    # dv <- yarn[[2]]\n\n    # Scale data\n    M <- m + 1\n    p <- ncol(ivs)\n    n <- nrow(ivs)\n    X      <- lapply(1:M, matrix, nrow = n, ncol = p)\n    X[[1]] <- scale(ivs)\n    y      <- dv\n    y_hat <- cbind(\n        mean(y),\n        matrix(rep(NA, n * (M - 1)), nrow = n)\n    )\n    z         <- matrix(NA, nrow = n, ncol = M)\n    theta_hat <- rep(NA, M)\n    W <- matrix(nrow = ncol(ivs), ncol = M)\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:M) {\n        # 3a: Compute zm\n        store_2a <- matrix(NA, nrow = n, ncol = p)\n        for (j in 1:p) {\n            rho_hat_mj <- t(X[[m - 1]][, j]) %*% y\n            store_2a[, j] <- rho_hat_mj %*% X[[m - 1]][, j]\n        }        \n        z[, m] <- rowSums(store_2a)\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] <- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        y_hat[, m] <- y_hat[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            X[[m]][, j] <- orthogonalize(X[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Normalize the component scores\n    Tsn <- apply(z[, -1], 2, function(j) j / sqrt(sum(j^2)))\n\n    # Return\n    return(\n        list(\n            Ts = z[, -1],\n            Tsn = Tsn,\n            Yhat = y_hat,\n            W = W[, -1]\n        )\n    )\n\n}\n\n# Orthogonalize two vectors\northogonalize <- function(vec1, vec2) {\n    v <- vec1\n    u <- vec2\n\n    newv <- v - drop(t(u) %*% v / (t(u) %*% u)) * u\n\n    return(newv)\n}\n\n```\n\nWe start by loading a package already implementing the PLS algorithm and some data to test our code.\nWe load the `PCovR` package to use the `alexithymia` data which reports the scores of 122 Belgian psychology students on three scales:\n\n- 20-item Toronto Alexithymia Scale (TAS-20) measuring the inability to recognize emotions.\n- Center for Epidemiological Studies Depression Scale (CES-D) measuring depression.\n- Rosenberg Self-Esteem Scale (RSE).\n\nThe main objective of the data collection was to assess how well TAS-20 can predict depression and self-esteem[^1].\n\n[^1]: Check the helpfile for the `alexithymia` data in the `PCovR` package for more information.\n\n```{r prep, warning = FALSE, message = FALSE}\n# Load packages ----------------------------------------------------------------\n\n# load packages\nlibrary(PCovR)  # for data\nlibrary(pls)    # for pls algorithm\nlibrary(plsdof) # for pls algorithm\n\n# Load data\ndata(alexithymia)\n\n# Devide in X and y\nX <- alexithymia$X\ny <- alexithymia$Y$RSE\n\n# Count the number of variables and observations\np <- ncol(X)\nn <- nrow(X)\n\n# Define the number of directions we will use\nM <- 10\n\n```\n\n## Coding the PLS algorithm manually\n\nLet's go through the steps described in the pseudo code above.\nFirst we want to standardize the predictors.\n\n```{r standardize}\n# 1. Standardize the data ------------------------------------------------------\n\n    # Scale the predictors\n    Xstd <- scale(X)\n\n    # Means of X are now 0\n    cbind(\n        X = head(colMeans(X)), \n        Xstd = round(head(colMeans(Xstd)), 5)\n        )\n\n    # SD of X are now 1\n    cbind(\n        X = head(apply(X, 2, sd)),\n        Xstd = round(head(apply(Xstd, 2, sd)), 5)\n        )\n\n```\n\nThen we want to set the initial values\n\n```{r initial}\n# 2. Set initial vlaues --------------------------------------------------------\n\n    # 2a: Set the initial prediction for y_hat to the mean of y\n    # Create an empty data.frame to store the initial value and future predictions\n    yhat_m <- matrix(rep(NA, n * (M + 1)), nrow = n)\n\n    # Replace the initial prediction with the mean of y\n    yhat_m[, 1] <- mean(y)\n\n    # 2b: Set every xj0 to xj\n    # Create an empty list of X values\n    Xm <- lapply(1:(M + 1), matrix, nrow = n, ncol = p)\n\n    # Place X as initial value for Xm\n    Xm[[1]] <- as.matrix(Xstd)\n\n```\n\nFinally, we can move to the estimation step.\nFirst, we create the container objects to store results\n\n```{r}\n# 3. Estimation ----------------------------------------------------------------\n\n    # Preparing objects\n    z <- matrix(NA, nrow = n, ncol = (M + 1)) # container for directions\n    W <- matrix(NA, nrow = p, ncol = (M + 1)) # container for wights\n    theta_hat <- rep(NA, (M + 1)) # container for thetas\n\n```\n\nThen we move to the proper estimation.\n\n```{r pls-manual}\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:(M + 1)) {\n        # 3a: Compute zm\n        W[, m] <- t(Xm[[m - 1]]) %*% y   # inner products / covariances\n        z[, m] <- Xm[[m - 1]] %*% W[, m] # compute the direction zm\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] <- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        yhat_m[, m] <- yhat_m[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            Xm[[m]][, j] <- orthogonalize(Xm[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Fit PLSR model w/ pls package\n    pls_fit_pls <- pls::plsr(\n        y ~ as.matrix(X),\n        ncomp = M,\n        method = \"oscorespls\",\n        validation = \"none\",\n        scale = TRUE\n    )\n\n    # Fit PCR model w/ plsdof package\n    pls_fit_plsdof <- plsdof::pls.model(as.matrix(Xstd), y)\n\n    # Compare predictions using up to a given m\n    m <- 3\n    head(\n        data.frame(\n            pls = round(as.data.frame(fitted(pls_fit_pls)), 3)[, m],\n            plsdof = round(pls_fit_plsdof$Yhat, 3)[, m + 1],\n            man = round(yhat_m, 3)[, m + 1]\n    ))\n\n```\n\n## Types of dependent variables\n\n```{r dv-types}\n# Types of DVs -----------------------------------------------------------------\n\n    data(oliveoil)\n    sens.pcr <- pls::pcr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n    sens.pls <- pls::plsr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n\n    oliveoil$sensory\n\n```\n\n## Prediction of new data\n\nHow do we predict new data? Let's start by generating data from scratch\n\n```{r prediction}\n# Prediction -------------------------------------------------------------------\n\n    n <- 50 # number of observations\n    p <- 15 # number of variables\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- 100 + rnorm(n)\n    M <- 10 # number of \"components\"\n\n    ntest <- 200 #\n    Xtest <- matrix(rnorm(ntest * p), ncol = p) # test data\n    ytest <- rnorm(ntest) # test data\n\n    # Fit alternative PLSs\n    out_pls <- pls::plsr(\n        y ~ X,\n        ncomp = M,\n        scale = TRUE,\n        center = TRUE,\n        method = \"oscorespls\",\n        validation = \"none\",\n        x = TRUE,\n        model = TRUE\n    )\n    out_plsdof <- plsdof::pls.model(X, y, compute.DoF = TRUE, Xtest = Xtest, ytest = NULL)\n\n    # Obtain predictions on new data\n    head(\n        round(\n            cbind(\n                PLS = predict(out_pls, newdata = Xtest)[, , M],\n                PLSdof = out_plsdof$prediction[, M + 1]\n            ), 5\n        )\n    )\n\n    # Make sure scale of prediction is correct\n    out_pls_cF <- plsr(\n      y ~ X,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    Xs <- scale(X, center = TRUE, scale = FALSE)\n    ys <- scale(y, center = FALSE, scale = FALSE)\n\n    out_pls_cF <- plsr(\n      ys ~ Xs,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    head(\n        round(\n        cbind(\n            PLS = predict(out_pls, newdata = Xtest)[, , M],\n            PLS_cf = mean(y) + predict(out_pls_cF, newdata = Xtest)[, , M],\n            PLSdof = out_plsdof$prediction[, M]\n        ), 5\n        )\n    )\n    \n```\n\n## Degrees of freedom of the residuals\n\n```{r pls-dfs, eval = FALSE}\n# PLS degrees of freedom -------------------------------------------------------\n\n    library(plsdof)\n    set.seed(1234)\n\n    # Generate data data\n    n <- 100 # number of observations\n    p <- 15 # number of variables\n    m <- 15\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- rnorm(n)\n\n    # Fit model with package\n    outpls <- pls.model(X, y, compute.DoF = TRUE)\n    outpls$DoF\n    outpls.internal <- linear.pls.fit(X, y, m, DoF.max = min(n - 1, p + 1))\n\n    # Fit model with person PLS function\n    outpls_man <- pls.manual(ivs = X, dv = y, m = m)\n\n    # Fit model with plsr function from pls\n    pls_fit_pls <- plsr(\n        y ~ X,\n        ncomp = m,\n        scale = FALSE,\n        center = FALSE,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n\n    # Y hats\n    round(outpls_man$Yhat - outpls$Yhat, 5)\n\n    # T scores\n    j <- 1\n    cbind(\n        PLSR = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2)))[, j],\n        PLSTT = outpls.internal$TT[, j],\n        manualTs = outpls_man$Ts[, j],\n        manualTsn = outpls_man$Tsn[, j]\n    )\n\n    # Degrees of freedom PLSR\n    predi <- sapply(1:m, function(j) {\n        predict(pls_fit_pls, ncomp = j)\n    })\n    DoF_plsr <- dofPLS(\n        X,\n        y,\n        TT = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2))),\n        Yhat = predi,\n        DoF.max = m + 1\n    )\n\n    # Degrees of freedom manual\n    DoF_manual <- dofPLS(\n        X,\n        y,\n        TT = outpls_man$Tsn,\n        Yhat = outpls_man$Yhat[, 2:(m + 1)],\n        DoF.max = m + 1\n    )\n\n    # Single DoF\n    DoF_single <- c(1, sapply(1:m, function(i) {\n        dofPLS_single(\n            X,\n            y,\n            TT = outpls_man$Tsn,\n            Yhat = outpls_man$Yhat[, (i + 1)],\n            q = i\n        )\n    }))\n\n    # Compare degrees of freedom\n    cbind(\n        PLSR = DoF_plsr,\n        PLSdof = outpls$DoF,\n        PLS_manual = DoF_manual,\n        diff = round(outpls$DoF - DoF_manual, 5),\n        DoF_single = DoF_single\n    )\n\n```\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n\n# References","srcMarkdownNoYaml":"\n\n# Introduction\n\nMany data analysts face the problem of analyzing data sets with many, often highly correlated, variables. \nPartial Least Square Regression (PLSR) is a regression method that uses linear combinations of the original predictors to reduce their dimensionality.\nAs principal component regression, PLS uses derived inputs, however, it differs from PCR by how the linear combinations are constructed.\n\nGiven a set of predictors $X_{n \\times p}$ and a vector of dependent variable scores $y_{n \\times 1}$, the least-square solution for the multiple linear regression\n\n$$\ny = X \\beta + \\epsilon\\text{, with } \\epsilon \\sim N(0, \\sigma^2)\n$$\n\nis \n\n$$\n\\beta = (X'X)^{-1}X'y\n$$\n\nwhere $X'$ is the transpose of the data matrix $X$, and $()^{-1}$ is the matrix inverse.\nWhen collinearity is present in $X$ or $p > n$, then $X'X$ is singular and its inverse cannot be computed.\nDerived input regression methods like PCR and PLSR bypass this problem by taking linear combinations of the columns of the original $X$ and regressing $Y$ on just a few of these linear combinations.\nThe peculiarity of PLSR is that it includes information on both $X$ and $Y$ in the definition of the linear combinations.\nIn this post, we look at two algorithms to estimate PLSR to get a better understanding of the method.\n\n## Popular algorithms to implement PLS\n\nHere, I describe informally the algorithm steps:\n\n1. Preprocessing the data - the columns of the input matrix $X$ are standardized to have mean 0 and variance 1.\n2. The cross-product of every column of $X$ and $y$ is computed: $\\rho_{1j} = x_{j}' y$ for $j = 1, \\dots, p$. \n3. Compute the first partial-least-square direction $z_1$ - The cross-products $\\rho_{1j}$ are used as weights to obtain a linear combination of the columns: $z_1 = \\sum \\rho_{1j} x_{j}$. This implies that the contribution of each column to $z_1$ is weighted by their univariate relationship with the dependent variable $y$.\n4. Regression of $y$ on $z_1$ - The outcome variable $y$ is regressed on this first direction $z_1$ to obtain $\\hat{\\theta}_1$.\n5. Orthogonalization of $X$ - All columns of $X$ are orthogonalized with respect to $z_1$.\n6. The cross-product of every column of $X$ and $y$ is computed again: $\\rho_{2j} = x_{j}' y$ for $j = 1, \\dots, p$.\n7. Compute the second partial-least-square direction $z_2$ - The cross-products $\\rho_{2j}$ are used as weights to obtain a linear combination of the columns: $z_2 = \\sum \\rho_{2j} x_{j}$. Notice that the columns $x_j$ we are using now are orthogonal to the previous partial least square direction $z_1$.\n8. Regression of $y$ on $z_2$ - The outcome variable $y$ is regressed on the second direction $z_2$ to obtain $\\hat{\\theta}_2$.\n9. Orthogonalization of $X$ - All columns of $X$ are orthogonalized with respect to $z_2$.\n\nThe procedure continues until $M$ partial least square directions have been computed.\nThe result is a set of independent directions that have both high variance and high correlation with the dependent variable, in contrast to PCR which finds a set of independent directions that have high variance.\n\nNow we report pseudo-code for the implementation of the PLS algorithm (inspired by Report Algorithm 3.3 p.103 as in [@hastieEtAl:2015]).\nWe will use it to write the R code in the next session.\n\n1. Standardized each $x_j$ to have mean 0 and variance 1\n2. Set:\n   - $\\hat{y}^{(0)} = \\bar{y}1$\n   - $x_{j}^{(0)} = x_{j}$\n3. For $m = 1, 2, \\dots, M$\n    a. $z_m = \\sum_{j = 1}^{p} \\rho_{mj}x_{j}^{(m-1)}$\n    b. $\\hat{\\theta}_m = \\frac{z_m'y}{z_m' z_m}$\n    c. $\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\hat{\\theta}_m z_m$\n    d. for $j = 1, \\dots, p$ orthogonalize $x_{j}$ with respect to $z_m$: $x_{j}^{(m)} = x_{j}^{(m-1)} - \\frac{z_m' x_{j}^{(m)}}{z_m' z_m}z_m$\n4. Output the sequence of fitted vectors $\\hat{y}^{m}$\n\n# Learn by coding\n\n```{r functions, echo = FALSE}\n# Functions needed ----------------------------------------------------------------\n\n# Degrees of freedom for supervised derived input models\n\ndofPLS <- function(X, y, TT, Yhat, m = ncol(X), DoF.max = ncol(X) + 1){\n    # Example inputs\n    # X = scale(mtcars[, -1])\n    # y = mtcars[, 1]\n    # m = ncol(X)\n    # DoF.max = m + 1\n    # TT <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$TT # normalizezs PC scores\n    # Yhat <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$Yhat[, 2:(m + 1)]\n\n    # Body\n    n <- nrow(X)\n\n    # Scale data\n    mean.X <- apply(X, 2, mean)\n    sd.X <- apply(X, 2, sd)\n    sd.X[sd.X == 0] <- 1\n    X <- X - rep(1, nrow(X)) %*% t(mean.X)\n    X <- X / (rep(1, nrow(X)) %*% t(sd.X))\n    K <- X %*% t(X)\n\n    # pls.dof\n    DoF.max <- DoF.max - 1\n    TK <- matrix(, m, m)\n    KY <- krylov(K, K %*% y, m)\n    lambda <- eigen(K)$values\n    tr.K <- vector(length = m)\n    for (i in 1:m) {\n        tr.K[i] <- sum(lambda^i)\n    }\n    BB <- t(TT) %*% KY\n    BB[row(BB) > col(BB)] <- 0\n    b <- t(TT) %*% y\n    DoF <- vector(length = m)\n    Binv <- backsolve(BB, diag(m))\n    tkt <- rep(0, m)\n    ykv <- rep(0, m)\n    KjT <- array(dim = c(m, n, m))\n    dummy <- TT\n    for (i in 1:m) {\n        dummy <- K %*% dummy\n        KjT[i, , ] <- dummy\n    }\n    trace.term <- rep(0, m)\n\n    for (i in 1:m) {\n        Binvi <- Binv[1:i, 1:i, drop = FALSE]\n        ci <- Binvi %*% b[1:i]\n        Vi <- TT[, 1:i, drop = FALSE] %*% t(Binvi)\n        trace.term[i] <- sum(ci * tr.K[1:i])\n        ri <- y - Yhat[, i]\n        for (j in 1:i) {\n            KjTj <- KjT[j, , ]\n            tkt[i] <- tkt[i] + ci[j] * tr(t(TT[, 1:i, drop = FALSE]) %*%\n                KjTj[, 1:i, drop = FALSE])\n            ri <- K %*% ri\n            ykv[i] <- ykv[i] + sum(ri * Vi[, j])\n        }\n    }\n\n    DoF <- trace.term + 1:m - tkt + ykv\n\n    DoF[DoF > DoF.max] <- DoF.max\n    DoF <- c(0, DoF) + 1\n    # TODO: check that it is correct to add the 1 after checking the DoF max condition\n    DoF\n\n}\n\n# Extract single factor --------------------------------------------------------\n\ndofPLS_single <- function(X, y, q = 1, TT, Yhat){\n    # Example inputs\n    # X = scale(mtcars[, -1])\n    # y = mtcars[, 1]\n    # m = ncol(X)\n    # DoF.max = m + 1\n    # TT <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$TT # normalizezs PC scores\n    # q <- 3 # desired component / latent variable\n    # Yhat <- linear.pls.fit(X, y, m, DoF.max = DoF.max)$Yhat[, (q + 1)]\n\n    # Body\n    n <- nrow(X)\n    m <- ncol(X)\n    DoF.max <- ncol(X) + 1\n\n    # Scale data\n    mean.X <- apply(X, 2, mean)\n    sd.X <- apply(X, 2, sd)\n    sd.X[sd.X == 0] <- 1\n    X <- X - rep(1, nrow(X)) %*% t(mean.X)\n    X <- X / (rep(1, nrow(X)) %*% t(sd.X))\n    K <- X %*% t(X)\n\n    # pls.dof\n    DoF.max <- DoF.max - 1\n    TK <- matrix(, m, m)\n    KY <- krylov(K, K %*% y, m)\n    lambda <- eigen(K)$values\n    tr.K <- vector(length = m)\n    for (i in 1:m) {\n        tr.K[i] <- sum(lambda^i)\n    }\n    BB <- t(TT) %*% KY\n    BB[row(BB) > col(BB)] <- 0\n    b <- t(TT) %*% y\n    DoF <- vector(length = m)\n    Binv <- backsolve(BB, diag(m))\n    tkt <- 0\n    ykv <- 0\n    KjT <- array(dim = c(q, n, m))\n    dummy <- TT\n    for (i in 1:q) {\n        dummy <- K %*% dummy\n        KjT[i, , ] <- dummy\n    }\n    trace.term <- 0\n\n    Binvi <- Binv[1:q, 1:q, drop = FALSE]\n    ci <- Binvi %*% b[1:q]\n    Vi <- TT[, 1:q, drop = FALSE] %*% t(Binvi)\n    trace.term <- sum(ci * tr.K[1:q])\n    ri <- y - Yhat\n    for (j in 1:q) {\n        KjTj <- KjT[j, , ]\n        tkt <- tkt + ci[j] * tr(t(TT[, 1:q, drop = FALSE]) %*%\n            KjTj[, 1:q, drop = FALSE])\n        ri <- K %*% ri\n        ykv <- ykv + sum(ri * Vi[, j])\n    }\n\n    DoF <- trace.term + q - tkt + ykv\n    DoF <- ifelse(DoF > DoF.max, DoF.max, DoF)\n    DoF <- DoF + 1\n    DoF\n}\n\n# Pls function manual\n\npls.manual <- function(ivs, dv, m = 1L){\n\n    # Parms\n    # M   <- 10 # number of \"components\"\n    # ivs <- yarn[[1]]\n    # dv <- yarn[[2]]\n\n    # Scale data\n    M <- m + 1\n    p <- ncol(ivs)\n    n <- nrow(ivs)\n    X      <- lapply(1:M, matrix, nrow = n, ncol = p)\n    X[[1]] <- scale(ivs)\n    y      <- dv\n    y_hat <- cbind(\n        mean(y),\n        matrix(rep(NA, n * (M - 1)), nrow = n)\n    )\n    z         <- matrix(NA, nrow = n, ncol = M)\n    theta_hat <- rep(NA, M)\n    W <- matrix(nrow = ncol(ivs), ncol = M)\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:M) {\n        # 3a: Compute zm\n        store_2a <- matrix(NA, nrow = n, ncol = p)\n        for (j in 1:p) {\n            rho_hat_mj <- t(X[[m - 1]][, j]) %*% y\n            store_2a[, j] <- rho_hat_mj %*% X[[m - 1]][, j]\n        }        \n        z[, m] <- rowSums(store_2a)\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] <- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        y_hat[, m] <- y_hat[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            X[[m]][, j] <- orthogonalize(X[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Normalize the component scores\n    Tsn <- apply(z[, -1], 2, function(j) j / sqrt(sum(j^2)))\n\n    # Return\n    return(\n        list(\n            Ts = z[, -1],\n            Tsn = Tsn,\n            Yhat = y_hat,\n            W = W[, -1]\n        )\n    )\n\n}\n\n# Orthogonalize two vectors\northogonalize <- function(vec1, vec2) {\n    v <- vec1\n    u <- vec2\n\n    newv <- v - drop(t(u) %*% v / (t(u) %*% u)) * u\n\n    return(newv)\n}\n\n```\n\nWe start by loading a package already implementing the PLS algorithm and some data to test our code.\nWe load the `PCovR` package to use the `alexithymia` data which reports the scores of 122 Belgian psychology students on three scales:\n\n- 20-item Toronto Alexithymia Scale (TAS-20) measuring the inability to recognize emotions.\n- Center for Epidemiological Studies Depression Scale (CES-D) measuring depression.\n- Rosenberg Self-Esteem Scale (RSE).\n\nThe main objective of the data collection was to assess how well TAS-20 can predict depression and self-esteem[^1].\n\n[^1]: Check the helpfile for the `alexithymia` data in the `PCovR` package for more information.\n\n```{r prep, warning = FALSE, message = FALSE}\n# Load packages ----------------------------------------------------------------\n\n# load packages\nlibrary(PCovR)  # for data\nlibrary(pls)    # for pls algorithm\nlibrary(plsdof) # for pls algorithm\n\n# Load data\ndata(alexithymia)\n\n# Devide in X and y\nX <- alexithymia$X\ny <- alexithymia$Y$RSE\n\n# Count the number of variables and observations\np <- ncol(X)\nn <- nrow(X)\n\n# Define the number of directions we will use\nM <- 10\n\n```\n\n## Coding the PLS algorithm manually\n\nLet's go through the steps described in the pseudo code above.\nFirst we want to standardize the predictors.\n\n```{r standardize}\n# 1. Standardize the data ------------------------------------------------------\n\n    # Scale the predictors\n    Xstd <- scale(X)\n\n    # Means of X are now 0\n    cbind(\n        X = head(colMeans(X)), \n        Xstd = round(head(colMeans(Xstd)), 5)\n        )\n\n    # SD of X are now 1\n    cbind(\n        X = head(apply(X, 2, sd)),\n        Xstd = round(head(apply(Xstd, 2, sd)), 5)\n        )\n\n```\n\nThen we want to set the initial values\n\n```{r initial}\n# 2. Set initial vlaues --------------------------------------------------------\n\n    # 2a: Set the initial prediction for y_hat to the mean of y\n    # Create an empty data.frame to store the initial value and future predictions\n    yhat_m <- matrix(rep(NA, n * (M + 1)), nrow = n)\n\n    # Replace the initial prediction with the mean of y\n    yhat_m[, 1] <- mean(y)\n\n    # 2b: Set every xj0 to xj\n    # Create an empty list of X values\n    Xm <- lapply(1:(M + 1), matrix, nrow = n, ncol = p)\n\n    # Place X as initial value for Xm\n    Xm[[1]] <- as.matrix(Xstd)\n\n```\n\nFinally, we can move to the estimation step.\nFirst, we create the container objects to store results\n\n```{r}\n# 3. Estimation ----------------------------------------------------------------\n\n    # Preparing objects\n    z <- matrix(NA, nrow = n, ncol = (M + 1)) # container for directions\n    W <- matrix(NA, nrow = p, ncol = (M + 1)) # container for wights\n    theta_hat <- rep(NA, (M + 1)) # container for thetas\n\n```\n\nThen we move to the proper estimation.\n\n```{r pls-manual}\n\n    # PLS Algorithm following HastieEtAl2017 p 81 (Algorithm 3.3)\n    for (m in 2:(M + 1)) {\n        # 3a: Compute zm\n        W[, m] <- t(Xm[[m - 1]]) %*% y   # inner products / covariances\n        z[, m] <- Xm[[m - 1]] %*% W[, m] # compute the direction zm\n\n        # 3b: Compute regression coefficient for y ~ Zm\n        theta_hat[m] <- drop(t(z[, m]) %*% y / t(z[, m]) %*% z[, m])\n\n        # 3c: Compute predicted y with the current m directions\n        yhat_m[, m] <- yhat_m[, m - 1] + theta_hat[m] * z[, m]\n\n        # 3d: Orthogonalize all columns of X with respect to zm\n        for (j in 1:p) {\n            Xm[[m]][, j] <- orthogonalize(Xm[[m-1]][, j], z[, m])\n        }\n    }\n\n    # Fit PLSR model w/ pls package\n    pls_fit_pls <- pls::plsr(\n        y ~ as.matrix(X),\n        ncomp = M,\n        method = \"oscorespls\",\n        validation = \"none\",\n        scale = TRUE\n    )\n\n    # Fit PCR model w/ plsdof package\n    pls_fit_plsdof <- plsdof::pls.model(as.matrix(Xstd), y)\n\n    # Compare predictions using up to a given m\n    m <- 3\n    head(\n        data.frame(\n            pls = round(as.data.frame(fitted(pls_fit_pls)), 3)[, m],\n            plsdof = round(pls_fit_plsdof$Yhat, 3)[, m + 1],\n            man = round(yhat_m, 3)[, m + 1]\n    ))\n\n```\n\n## Types of dependent variables\n\n```{r dv-types}\n# Types of DVs -----------------------------------------------------------------\n\n    data(oliveoil)\n    sens.pcr <- pls::pcr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n    sens.pls <- pls::plsr(sensory ~ chemical, ncomp = 4, scale = TRUE, data = oliveoil)\n\n    oliveoil$sensory\n\n```\n\n## Prediction of new data\n\nHow do we predict new data? Let's start by generating data from scratch\n\n```{r prediction}\n# Prediction -------------------------------------------------------------------\n\n    n <- 50 # number of observations\n    p <- 15 # number of variables\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- 100 + rnorm(n)\n    M <- 10 # number of \"components\"\n\n    ntest <- 200 #\n    Xtest <- matrix(rnorm(ntest * p), ncol = p) # test data\n    ytest <- rnorm(ntest) # test data\n\n    # Fit alternative PLSs\n    out_pls <- pls::plsr(\n        y ~ X,\n        ncomp = M,\n        scale = TRUE,\n        center = TRUE,\n        method = \"oscorespls\",\n        validation = \"none\",\n        x = TRUE,\n        model = TRUE\n    )\n    out_plsdof <- plsdof::pls.model(X, y, compute.DoF = TRUE, Xtest = Xtest, ytest = NULL)\n\n    # Obtain predictions on new data\n    head(\n        round(\n            cbind(\n                PLS = predict(out_pls, newdata = Xtest)[, , M],\n                PLSdof = out_plsdof$prediction[, M + 1]\n            ), 5\n        )\n    )\n\n    # Make sure scale of prediction is correct\n    out_pls_cF <- plsr(\n      y ~ X,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    Xs <- scale(X, center = TRUE, scale = FALSE)\n    ys <- scale(y, center = FALSE, scale = FALSE)\n\n    out_pls_cF <- plsr(\n      ys ~ Xs,\n      ncomp = M,\n      scale = TRUE,\n      center = FALSE,\n      method = \"oscorespls\",\n      validation = \"none\"\n    )\n\n    head(\n        round(\n        cbind(\n            PLS = predict(out_pls, newdata = Xtest)[, , M],\n            PLS_cf = mean(y) + predict(out_pls_cF, newdata = Xtest)[, , M],\n            PLSdof = out_plsdof$prediction[, M]\n        ), 5\n        )\n    )\n    \n```\n\n## Degrees of freedom of the residuals\n\n```{r pls-dfs, eval = FALSE}\n# PLS degrees of freedom -------------------------------------------------------\n\n    library(plsdof)\n    set.seed(1234)\n\n    # Generate data data\n    n <- 100 # number of observations\n    p <- 15 # number of variables\n    m <- 15\n    X <- matrix(rnorm(n * p), ncol = p)\n    y <- rnorm(n)\n\n    # Fit model with package\n    outpls <- pls.model(X, y, compute.DoF = TRUE)\n    outpls$DoF\n    outpls.internal <- linear.pls.fit(X, y, m, DoF.max = min(n - 1, p + 1))\n\n    # Fit model with person PLS function\n    outpls_man <- pls.manual(ivs = X, dv = y, m = m)\n\n    # Fit model with plsr function from pls\n    pls_fit_pls <- plsr(\n        y ~ X,\n        ncomp = m,\n        scale = FALSE,\n        center = FALSE,\n        method = \"oscorespls\",\n        validation = \"none\"\n    )\n\n    # Y hats\n    round(outpls_man$Yhat - outpls$Yhat, 5)\n\n    # T scores\n    j <- 1\n    cbind(\n        PLSR = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2)))[, j],\n        PLSTT = outpls.internal$TT[, j],\n        manualTs = outpls_man$Ts[, j],\n        manualTsn = outpls_man$Tsn[, j]\n    )\n\n    # Degrees of freedom PLSR\n    predi <- sapply(1:m, function(j) {\n        predict(pls_fit_pls, ncomp = j)\n    })\n    DoF_plsr <- dofPLS(\n        X,\n        y,\n        TT = apply(scores(pls_fit_pls), 2, function(j) j / sqrt(sum(j^2))),\n        Yhat = predi,\n        DoF.max = m + 1\n    )\n\n    # Degrees of freedom manual\n    DoF_manual <- dofPLS(\n        X,\n        y,\n        TT = outpls_man$Tsn,\n        Yhat = outpls_man$Yhat[, 2:(m + 1)],\n        DoF.max = m + 1\n    )\n\n    # Single DoF\n    DoF_single <- c(1, sapply(1:m, function(i) {\n        dofPLS_single(\n            X,\n            y,\n            TT = outpls_man$Tsn,\n            Yhat = outpls_man$Yhat[, (i + 1)],\n            q = i\n        )\n    }))\n\n    # Compare degrees of freedom\n    cbind(\n        PLSR = DoF_plsr,\n        PLSdof = outpls$DoF,\n        PLS_manual = DoF_manual,\n        diff = round(outpls$DoF - DoF_manual, 5),\n        DoF_single = DoF_single\n    )\n\n```\n\n# TL;DR, just give me the code!\n```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}\n```\n\n# References"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"pls.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.340","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"title-block-banner":true,"draft":false,"title":"Implementing a PLS alogirthm in R","author":"Edoardo Costantini","date":"2022-06-13","slug":"pls","categories":["Supervised learning"],"bibliography":["../../resources/bibshelf.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}