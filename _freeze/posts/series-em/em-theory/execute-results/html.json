{
  "hash": "02eb9f16234abc9a095ca9c3bb23b972",
  "result": {
    "markdown": "---\ndraft: true\ntitle: The theory of the EM algorithm\nauthor: Edoardo Costantini\ndate: '2021-11-15'\nslug: em-theory\ncategories: [\"The EM algorithm\"]\nbibliography: ../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nThe Expectation Maximization (EM) algorithm is one possible way to implement the full information maximum likelihood missing data technique [@enders:2010 p86].\nIt is an iterative optimization procedure that allows to estimate the parameters an analysis model in the presence of missing values without requiring the computation of first and second derivatives, which makes it possible to obtain Maximum Likelihood estimates of the parameters of interest even when they cannot be obtained in closed form.\n\n## Likelihood review\n\nFollowing [@schafer:1997 ch 2.3.2], consider an $n \\times p$ data matrix $Y$, with some columns having missing values.\nThe **complete data probability density function** of $Y$ is the function of $Y$ for fixed $\\theta$ that can be written as:\n\n$$\nf(Y|\\theta) = \\prod_{i = 1}^{n} f(y_i|\\theta) (\\#eq:pdfy)\n$$\n\nwhere\n\n- $f$ is a probability density function (pdf);\n- $\\theta$ is a vector of unknown parameters defining the $f$ distribution;\n- $f(y_i|\\theta)$ is the density value for a single $i$-th row of the data matrix $Y$.\n\nThe **complete-data likelihood function** $L(\\theta|Y)$ is any function of $\\theta$ for fixed $Y$ that is proportional to $f(Y|\\theta)$:\n\n$$\nL(\\theta|Y) \\propto f(Y|\\theta)\n$$\n\nand the complete-data *log*likelihood function $l(\\theta|Y)$ is simply the natural logarithm of $L(\\theta|Y)$\n\n$$\nl(\\theta|Y) = log(L(\\theta|Y))\n$$\n\nThe **maximum likelihood estimate** of $\\theta$ is the value of $\\theta$ that leads to the highest value of of the log-likelihood function, for a fixed $Y$.\nThis estimate can be found by setting the first derivative of the log-likelihood function equal to 0, and solving for $\\theta$:\n\n$$\n\\frac{\\partial l(\\theta|Y)}{\\partial \\theta} = 0 (\\#eq:le)\n$$\n\n\\@ref(eq:le) is called the **likelihood equation** or **score function**.\nIf there are $d$ components to $\\theta$, then the likelihood equation is a set of $d$ simultaneous equations defined by differentiating $l(\\theta|Y)$ with respect to each component.\n\n### Example\n\nIf we assume that $Y$ comes from a multivariate normal distribution with parameters $\\theta = (\\mu, \\Sigma)$:\n\n- its complete data density is:\n\n  \\begin{aligned}\n  f(Y|\\mu, \\Sigma) &= \\prod^{n}_{i = 1} |(2 \\pi)^{p} \\Sigma|^{-\\frac{1}{2}} \\text{exp}\\left(-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu) \\right) \\\\\n  &= (2 \\pi)^{-np/2} |\\Sigma|^{-\\frac{n}{2}} \\text{exp}\\left(-\\frac{1}{2} \\sum^{n}_{i=1}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu) \\right)\n  \\end{aligned}\n\n- the complete data likelihood is:\n\n  \\begin{aligned}\n  L(\\mu, \\Sigma | Y) &\\propto |\\Sigma|^{-\\frac{n}{2}} \\text{exp}\\left(-\\frac{1}{2} \\sum^{n}_{i=1}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu) \\right)\n  \\end{aligned}\n\n- its log version is:\n\n  \\begin{aligned}\n  l(\\mu, \\Sigma | Y) &= log \\left( |\\Sigma|^{-\\frac{n}{2}} \\text{exp}\\left(-\\frac{1}{2} \\sum^{n}_{i=1}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu) \\right) \\right) \\\\\n  &= -\\frac{n}{2}log(\\Sigma) - \\frac{1}{2} \\sum^{n}_{i=1}(y_i - \\mu)^T \\Sigma^{-1} (y_i - \\mu)\n  \\end{aligned}\n\n- the likelihood equation is made of the following parts:\n\n  \\begin{aligned}\n  \\frac{\\partial l(\\theta|Y)}{\\partial \\mu} = 0 \\\\\n  \\frac{\\partial l(\\theta|Y)}{\\partial \\sigma_{ij}} = 0\n  \\end{aligned}\n\n  for derivation of these you can check out [@rao:1973 p 529], and you can use [Matrix Calculus](http://www.matrixcalculus.org) to perform the computations yourself.\n\n- the ML estiamtes of $\\mu$ and $\\Sigma$ derived by solving the likelihood equation are:\n\n  \\begin{aligned}\n  \\hat{\\mu} &= \\bar{y} \\\\\n  \\hat{\\Sigma} &= \\frac{S}{n}\n  \\end{aligned}\n\n  where $\\bar{y}$ is the vector of sample means, and $S$ is the $p \\times p$ cross-product matrix with the $(i, j)$th element $s_{ij} = \\sum^{n}_{i = 1}(y_{ij} - \\bar{y}_{j})(y_{ij} - \\bar{y}_{j})$\n\n\n\n## Incomplete data\n\nIn any incomplete-data problem, we can factor the distribution of the complete data $Y$ in \\@ref(eq:pdfy) as:\n\n$$\nf(Y|\\theta) = f(Y_{obs}|\\theta) f(Y_{mis}|Y_{obs}, \\theta) (\\#eq:idp)\n$$\n\nwhere $f(Y_{obs}|\\theta)$ is the density of the observed data $Y$; and $f(Y_{mis}|Y_{obs}, \\theta)$ is the density of the missing data given the observed data.\nWe can rewrite \\@ref(eq:idp) as a function of $\\theta$ and obtain the partitioning of the complete data likelihood function:\n\n$$\nL(\\theta|Y) = L(\\theta|Y_{obs}) f(Y_{mis}|Y_{obs}, \\theta) c (\\#eq:cdlp)\n$$\n\nand to make \\@ref(eq:cdlp) easier to work with, it is useful to take its $log$:\n\n$$\nl(\\theta|Y) = l(\\theta|Y_{obs}) + log f(Y_{mis}|Y_{obs}, \\theta) + c (\\#eq:lcdlp)\n$$\n\nwhere:\n\n- $l(\\theta|Y)$ is the complete-data loglikelihood\n- $l(\\theta|Y_{obs})$ is the observed-data loglikelihood\n- $c$ is an arbitrary proportionality constant\n- $f(Y_{mis}|Y_{obs}, \\theta)$ is the predictive distribution of the missing data given $\\theta$\n\nThe task is to find the estimate of $\\theta$ that maximizes the log likelihood $l(\\theta|Y_{obs})$.\nWhen this likelihood is differentiable the ML estimates of $\\theta$ can be found by setting the its first derivative equal to 0 and solving for $\\theta$:\n\n$$\n\\frac{\\partial l(\\theta|Y_{obs})}{\\partial \\theta} = 0 (\\#eq:floglike)\n$$\n\nHowever, it is not always easy (or possible) to find a closed form solution to \\@ref(eq:floglike).\n\n\n# The EM algorithm\n\nSince $Y_{mis}$ is unknown, we cannot calculate the terms of \\@ref(eq:lcdlp) that include it, so instead we take its average over the predictive distribution $f(Y_{mis}|Y_{obs}, \\theta^{(t)})$, where $\\theta^{(t)}$ is the current estimate of the unknown parameter.\nThis means we multiply both sides of \\@ref(eq:lcdlp) by $f(Y_{mis}|Y_{obs}, \\theta^{(t)})$ and integrate over the unknown $Y_{mis}$ :\n\n\\begin{aligned}\n\\int & l(\\theta|Y) f\\left(Y_{mis}|Y_{obs}, \\theta^{(t)}\\right) dY_{mis} = l(\\theta|Y_{obs}) + \\\\\n & \\int log f(Y_{mis}|Y_{obs}, \\theta) f(Y_{mis}|Y_{obs}, \\theta^{(t)}) dY_{mis} + c\n\\end{aligned}\n\nwhich can be shortened to:\n\n$$\nQ\\left(\\theta|\\theta^{(t)}\\right) = l(\\theta|Y_{obs}) + H(\\theta|\\theta^{(t)}) + c (\\#eq:avglcdlp)\n$$\n\nIt can be shown that if we define $\\theta^{t+1}$ as the value of $\\theta$ that maximizes $Q(\\theta|\\theta^{(t)})$, then $\\theta^{t+1}$ is a better estimate than $\\theta^{t+1}$ in the sense that its observed-data loglikelihood is at least as high as that of $\\theta^{t}$:\n\n$$\nl \\left( \\theta^{(t+1)}|Y_{obs} \\right) \\geq l \\left( \\theta^{(t)}|Y_{obs} \\right)\n$$\n\nAfter defining a starting value $\\theta^{(0)}$, the EM algorithm consists of alternatively performing the following two steps:\n\n1. E-step (or expectation step) which finds the expected complete-data loglikelihood if $\\theta$ were $\\theta^{(t)}$:\n\n$$\nQ\\left(\\theta|\\theta^{(t)}\\right) = \\int l(\\theta|Y) f\\left(Y_{mis}|Y_{obs}, \\theta^{(t)}\\right) dY_{mis}\n$$\n\n2. M-step (or maximization step) consisting of maximizing $Q\\left(\\theta|\\theta^{(t)}\\right)$ to find $Q^{t+1}$\n\n## EM for regular exponential families\n\nConsider the case when the complete-data probability model belongs to the [exponential family](https://en.wikipedia.org/wiki/Exponential_family) defined by:\n\n$$\nf(Y|\\theta) = b(Y) \\text{exp}\\left(\\frac{s(Y)\\theta}{a(\\theta)}\\right)\n$$\n\nwhere\n- $\\theta$ a parameter vector\n- $s(Y)$ denotes a vector of complete-data sufficient statistics\n- $a$ and $b$ are functions of $\\theta$ and $Y$, respectively.\n\n# Other notes\n\nWe refer to the observed part of $Y$ as $Y_{obs}$ and to the missing part as $Y_{mis}$.\nUnder the missing at random assumption, the observed-data likelihood is proportional to the probability of $Y_{obs}$ given the unknown parameters $\\theta$:\n\n$$\nL(\\theta|Y_{obs}) \\propto f(Y_{obs}|\\theta) (\\#eq:odl)\n$$\n\nIf we assume that $Y$ comes from a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) with parameter $\\theta = (\\mu, \\Sigma)$ and an arbitrary number ($S$) of missing data patterns, then, the observed-data likelihood can be written as:\n\n$$\nL(\\theta|Y_{obs}) = \\prod^{S}_{s = 1} \\prod^{}_{i \\in I(s)} |\\Sigma_s^*|^{1/2} \\text{exp}\\left( - \\frac{1}{2} (y_i^* - \\mu_s^*)^T\\Sigma_s^{*-1}(y_i^* - \\mu_s^*) \\right)\n$$\n\nwhere $I(s)$ is an indicator vector describing which rows are observed in the $s$-th missing data pattern, $y_i^*$ represents the observed part of the $i$-th row of $Y$, and $\\mu_s^*$ and $\\Sigma_s$ are the mean vector and covariance matrices for the variables that are observed in pattern $s$.\nExpect for special cases, it is not possible to express this likelihood as a product of complete-data likelihoods with distinct parameters, and computing its first derivative with respect to the individual parameters can be very complicated.\n\nThe EM algorithm is a convenient alternative to maximize this likelihood (i.e., finding the ML estimates of $\\theta$).\nEM is based on the fact that $Y_{mis}$ contains information on $\\theta$ and that $\\theta$ can be used to find plausible values for $Y_{mis}$.\nIn fact, you can think of EM as iterating between filling in missing values based on a current estimate of $\\theta$, and re-estimating $\\theta$ based on the filled-in missing values until convergence.\n\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n:::\n\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}