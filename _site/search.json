[
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html",
    "href": "posts/series-pca-primer/pca-biplots.html",
    "title": "How to obtain PCA biplots",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an \\(n \\times p\\) data matrix \\(\\mathbf{X}\\) with minimal loss of information. In terms of visualization techniques, PCA allows the representation of large datasets in a bi-dimensional plot. In general, biplots give use a simultaneous representation of \\(n\\) observations and \\(p\\) variables on a single bi-dimensional plot. More precisely, biplots represent the scatterplot of the observations on the first two principal components computed by PCA and the relative position of the \\(p\\) variables in a two-dimensional space. For an in-depth discussion, I recommend reading Jolliffe (2002, 90)."
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#learn-by-coding",
    "href": "posts/series-pca-primer/pca-biplots.html#learn-by-coding",
    "title": "How to obtain PCA biplots",
    "section": "Learn by coding",
    "text": "Learn by coding\nLet us start by loading the ggfortify R package, which provides pleasant looking biplots. We will work with the first four columns of the iris data, which report the length and width of petals and sepals in plants for the iris flowering plant.\n\n# Prepare environment ----------------------------------------------------------\n\n# Load packages (install if you don't have it)\nlibrary(ggfortify) # for default biplots\n\n# Keep the numeric variables from the iris dataset.\nX <- iris[1:4]\n\nWe then use the prcomp R function to compute the PCs of X. We specify the prcomp function to not scale and standardize the data because we rather have more control over how the standardization is performed.\n\n# Perform PCA ------------------------------------------------------------------\n\n# Center and standardize the data\nX_sc <- scale(X)\n\n# Compute PCs\npca_res <- prcomp(X_sc, center = FALSE, scale. = FALSE)\n\n# Generate default biplot with ggfortify\nautoplot(pca_res,\n    data = X,\n    loadings.label = TRUE, \n    loadings.colour = \"blue\"\n)\n\n\n\n\nand print the biplot obtained with the ggfortify::autoplot() function. We specify which data should be plotted (data = X), and we require the loadings to be included in the plot as blue arrows (loadings.colour = \"blue\" and loadings.label = TRUE.)\nNow we replicate this plot by doing all the work ourselves. First, let’s start by computing the PC scores ourselves by taking the singular value decomposition of X and computing the PC scores \\(T\\).\n\n# Getting to the biplots -------------------------------------------------------\n\n# SVD of X\nx_svd <- svd(X_sc)\n\n# Extract the parts of the SVD we need to compute the Principal Component scores\nU <- x_svd$u\nD <- diag(x_svd$d)\nV <- x_svd$v\n\n# Compute the PCs\nT <- U %*% D\n\nWe can now make a scatter plot of the observations based on how they score on the first two PCs we computed.\n\n# > Simple scatter plot --------------------------------------------------------\n\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n\n\n\nThe next step is to overlay the arrows plotting the information regarding the loadings. We want to plot a single arrow for each column of the original data, starting at the center of the plot (\\(PC1 = 0\\), and \\(PC2 = 0\\)) and ending at the coordinates described by the first two columns of the component loadings matrix \\(V\\).\n\n# > Adding loading arrows ------------------------------------------------------\n\n# Scatter plot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1], \n    y0 = rep(0, ncol(X)), y1 = V[, 2]\n)\n\n\n\n\nWe can improve the visualization by adding a few teaks. Let’s add a title, make the scatterplot symbols gray solid dots, scale up the arrow size, and add labels indicating which loading we are plotting with any given arrow.\n\n# > Improving the visuals ------------------------------------------------------\n\n# Define a scaling factor for the arrows\nsf <- 2\n\n# Start over with the original scatterplot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\",\n    main = \"PCA biplot\", # plot title\n    col = \"gray\",\n    pch = 19 # solid circle\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1] * sf, \n    y0 = rep(0, ncol(X)), y1 = V[, 2] * sf,\n    col = \"darkgray\"\n)\n\n# Add names of variables per arrow\ntext(x = V[, 1] * sf, y = V[, 2] * sf, labels = colnames(X))\n\n\n\n\nThe scaling we performed is ad-hoc. We looked at the plot and decided to increase the size of the arrows by an arbitrary scaling factor. In the literature on biplots it is more common to scale both component scores and loadings by taking powers of the diagonal matrix \\(D\\) and recomputing the coordinates1."
  },
  {
    "objectID": "posts/series-pca-primer/pca-biplots.html#tldr-just-give-me-the-code",
    "href": "posts/series-pca-primer/pca-biplots.html#tldr-just-give-me-the-code",
    "title": "How to obtain PCA biplots",
    "section": "TL;DR, just give me the code!",
    "text": "TL;DR, just give me the code!\n\n# Prepare environment ----------------------------------------------------------\n\n# Load packages (install if you don't have it)\nlibrary(ggfortify) # for default biplots\n\n# Keep the numeric variables from the iris dataset.\nX <- iris[1:4]\n\n# Perform PCA ------------------------------------------------------------------\n\n# Center and standardize the data\nX_sc <- scale(X)\n\n# Compute PCs\npca_res <- prcomp(X_sc, center = FALSE, scale. = FALSE)\n\n# Generate default biplot with ggfortify\nautoplot(pca_res,\n    data = X,\n    loadings.label = TRUE, \n    loadings.colour = \"blue\"\n)\n\n# Getting to the biplots -------------------------------------------------------\n\n# SVD of X\nx_svd <- svd(X_sc)\n\n# Extract the parts of the SVD we need to compute the Principal Component scores\nU <- x_svd$u\nD <- diag(x_svd$d)\nV <- x_svd$v\n\n# Compute the PCs\nT <- U %*% D\n\n# > Simple scatter plot --------------------------------------------------------\n\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# > Adding loading arrows ------------------------------------------------------\n\n# Scatter plot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\"\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1], \n    y0 = rep(0, ncol(X)), y1 = V[, 2]\n)\n\n# > Improving the visuals ------------------------------------------------------\n\n# Define a scaling factor for the arrows\nsf <- 2\n\n# Start over with the original scatterplot\nplot(\n    x = T[, 1], xlab = \"PC1\",\n    y = T[, 2], ylab = \"PC2\",\n    main = \"PCA biplot\", # plot title\n    col = \"gray\",\n    pch = 19 # solid circle\n)\n\n# Add arrows from 0 to loading on the selected PCs (scaled up)\narrows(\n    x0 = rep(0, ncol(X)), x1 = V[, 1] * sf, \n    y0 = rep(0, ncol(X)), y1 = V[, 2] * sf,\n    col = \"darkgray\"\n)\n\n# Add names of variables per arrow\ntext(x = V[, 1] * sf, y = V[, 2] * sf, labels = colnames(X))"
  },
  {
    "objectID": "series-pca.html",
    "href": "series-pca.html",
    "title": "A primer on PCA",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nDec 8, 2022\n\n\nHow to obtain PCA biplots\n\n\nPCA,Interpretation\n\n\n\n\nOct 26, 2022\n\n\nNormal to anything (NORTA) sampling\n\n\ndistributions\n\n\n\n\nSep 6, 2022\n\n\nUnderstanding quantiles\n\n\nStatistics,Distributions\n\n\n\n\nSep 5, 2022\n\n\nUnderstanding boxplots\n\n\nPlotting\n\n\n\n\nJun 22, 2022\n\n\nUnderstanding the residual standard error\n\n\nKnowledge snippet,linear models,ols\n\n\n\n\nJun 13, 2022\n\n\nImplementing a PLS alogirthm in R\n\n\nSupervised learning\n\n\n\n\nMay 16, 2022\n\n\nDeciding the Number of PCs with Non-Graphical Solutions to the Scree Test\n\n\nPCA,Tutorials\n\n\n\n\nMay 13, 2022\n\n\nPrincipal Component Analysis, SVD, and EVD\n\n\nPCA\n\n\n\n\nApr 22, 2022\n\n\nCross-entropy as a measure of predictive performance\n\n\nMachine Learning\n\n\n\n\nMar 14, 2022\n\n\nEstimating the weighted covariance matrix in R\n\n\nStatistics\n\n\n\n\nFeb 28, 2022\n\n\nEstimating ridge regression in R\n\n\nPenalty\n\n\n\n\nNov 17, 2021\n\n\nThe sweep operator\n\n\nThe EM algorithm,Matrix algebra,Statistics\n\n\n\n\n\n\nNo matching items"
  }
]