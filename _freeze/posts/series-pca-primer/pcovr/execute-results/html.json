{
  "hash": "0b75aaed92fb0e831de213ea3696f521",
  "result": {
    "markdown": "---\ndraft: false\ntitle: Principal covariates regression in R\nauthor: Edoardo Costantini\ndate: '2022-12-13'\nslug: pcovr\ncategories: [\"Supervised learning\"]\nbibliography: ../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nPrincipal covariates regression is a method to analyze the relationship between sets of multivariate data in the presence of highly-collinear variables.\nPrincipal covariates regression (PCovR) is an alternative approach that modifies the optimization criteria behind PCA to include a supervision aspect \\citep{deJongKiers:1992}.\nPCovR looks for a low-dimensional representation of $X$ that accounts for the maximum amount of variation in both $X$ and $y$.\nCompared to regular principal component regression, principal covariates regression PCovR extracts components that account for much of the variability in a set of $X$ variables and that correlate well with a set of $Y$ variables.\nFor more information, I recommend reading @vervloetEtAl:2015 and @deJongKiers:1992.\nIn this post, you can find my R code notes on this method.\nIn these notes, I show the computations used by the `PCovR` R-package to perform the method.\n\nTo understand how PCovR differs from classical PCR we need to complicate the notation.\nConsider the following decomposition of the data:\n\n$$\n\\begin{align}\n    \\mathbf{T} &= \\mathbf{X} \\mathbf{W} \\\\\n    \\mathbf{X} &= \\mathbf{T} \\mathbf{P}_X + \\mathbf{E}_X  \\\\\n    y      &= \\mathbf{T} \\mathbf{P}_y + \\mathbf{e}_y\n\\end{align}\n$$\n\nwhere $\\mathbf{T}$ is the matrix of PCs defined above, $\\mathbf{W}$ is a $p \\times q$ matrix of component weights defining what linear combination of the columns of $\\mathbf{X}$ is used to compute the components, and $\\mathbf{P}_X$ and $\\mathbf{P}_y$ are the $q \\times p$ and $q \\times 1$ loading matrices relating the variables in $\\mathbf{X}$ and $y$ to the component scores in $\\mathbf{T}$, respectively.\n$\\mathbf{E}_X$ and $\\mathbf{e}_y$ are the reconstruction errors, the information lost by using  $\\mathbf{T}$ as summary of $\\mathbf{X}$.\n\nClassical PCA can be formulated as the task of finding the $\\mathbf{W}$ and $\\mathbf{P}_X$ that minimize the reconstruction error $\\mathbf{E}_X$:\n\n$$\\begin{equation}\n    (\\mathbf{W}, \\mathbf{P}) = \\underset{\\mathbf{W}, \\mathbf{P}_X}{\\operatorname{argmin}} \\lVert \\mathbf{X} - \\mathbf{XWP}' \\rVert^2\n\\end{equation}\n$$\n\nsubject to the constraint $\\mathbf{P}' \\mathbf{P} = \\mathbf{I}$.\nPCovR can be formulated as the task of minimizing a weighted combination of both $\\mathbf{E}_X$ and $\\mathbf{e}_y$:\n\n$$\n\\begin{equation}\\label{eq:PCovR}\n    (\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y) = \\underset{\\mathbf{W}, \\mathbf{P}_X, \\mathbf{P}_y}{\\operatorname{argmin  }} \\alpha \\lVert (\\mathbf{X} - \\mathbf{XWP}_X') \\rVert^2 + (1 - \\alpha) \\lVert (y - \\mathbf{XWP}_y') \\rVert^2\n\\end{equation}\n$$\n\nsubject to the constraint $\\mathbf{P}' \\mathbf{P} = \\mathbf{I}$.\n\nThe parameter $\\alpha$ defines which reconstruction error is being prioritized.\nWhen $\\alpha = 1$, the emphasis is exclusively placed on reconstructing $\\mathbf{X}$, leading PCovR to PCR.\nWhen $\\alpha = 0.5$, the importance of $\\mathbf{X}$ and $y$ is equally weighted, a case that resembles Partial least square, which we discuss below.\nIn practice, its value can be found by cross-validation or according to a sequential procedure based on maximum likelihood principles [@vervloetEtAl:2013].\nIn particular,\n$$\n\\begin{equation}\\label{eq:aml}\n    \\alpha_{ML} = \\frac{\\lVert \\mathbf{X} \\lVert^2}{\\lVert \\mathbf{X} \\lVert^2  + \\lVert y \\lVert^2 \\frac{\\hat{\\sigma}_{\\mathbf{E}_X}^2}{\\hat{\\sigma}_{e_y}^2}}\n\\end{equation}\n$$\n\nwhere $\\hat{\\sigma}_{\\mathbf{E}_X}^2$ can be obtained as the unexplained variance by components computed according to classical PCA and $\\hat{\\sigma}_{e_y}^2$ can be estimated as the unexplained variance by the linear model regressing $y$ on $\\mathbf{X}$.\n\n# Learn by coding\n\nTo understand PCovR and its relationship with PCR we will use the R package `PCovR`, the data `alexithymia`, and the package `RegularizedSCA` to compute a measure of similarity between matrices (Tucker's congruence).\nThe `alexithymia` data reports the scores of 122 Belgian psychology students on three scales:\n\n- 20-item Toronto Alexithymia Scale (TAS-20) measuring the inability to recognize emotions.\n- Center for Epidemiological Studies Depression Scale (CES-D) measuring depression.\n- Rosenberg Self-Esteem Scale (RSE).\n\nThe main objective of the data collection was to assess how well TAS-20 can predict depression and self-esteem[^1]. We collect all of the predictors in the `X_raw` object and the measure of depressive symptomatology CES-D in `y_raw`.\n\n[^1]: Check the helpfile for the `alexithymia` data in the `PCovR` package for more information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up environment -----------------------------------------------------------\n\n# Load package that implements this method\nlibrary(\"PCovR\")\n\n# Load package for a measure of similarity (TuckerCoef)\nlibrary(\"RegularizedSCA\")\n\n# Load package for Tucker congruence between vectors\nlibrary(\"psych\")\n\n# Load example data from PCovR package\ndata(alexithymia)\n\n# Subset data\nX_raw <- alexithymia$X\ny_raw <- alexithymia$Y[, 1, drop = FALSE]\n```\n:::\n\n\nThe first thing we'll do is explore the data measures of center and spread.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check variable means and variances\ndata.frame(\n    mean = c(\n        X = colMeans(X_raw),\n        y = colMeans(y_raw)\n        ),\n    variance = c(\n        X = apply(X_raw, 2, var), \n        y = apply(y_raw, 2, var)\n        )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          mean   variance\nX.confused           1.8196721   1.388701\nX.right words        1.7950820   1.635348\nX.sensations         0.5983607   1.151402\nX.describe           2.2213115   1.479542\nX.analyze problems   2.5081967   1.161089\nX.upset              1.6065574   1.678634\nX.puzzled            1.1393443   1.294472\nX.let happen         1.2622951   1.302534\nX.identify           1.6393443   1.620919\nX.essential          2.7131148   1.231066\nX.feel about people  1.7213115   1.475410\nX.describe more      1.0081967   1.363569\nX.going on           0.9836066   1.470803\nX.why angry          1.2540984   1.496884\nX.daily activities   1.6639344   1.514226\nX.entertainment      1.6967213   1.601477\nX.reveal feelings    1.6475410   2.097886\nX.close              2.8442623   1.240008\nX.useful             2.4672131   1.193131\nX.hidden meanings    1.2131148   1.045116\ny.CES-D             16.4672131 118.350156\n```\n:::\n:::\n\n\nWe will need to scale the data before performing any form of dimensionality reduction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scale data\nX <- scale(X_raw)# * (n - 1) / n\ny <- scale(y_raw)# * (n - 1) / n\n```\n:::\n\n\nOnce the data has been mean-centered and standardized we can proceed to perform PCovR of the data.\nFirst, let's assume that both $\\alpha$ and the number of PCs we want to compute are given.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimation -------------------------------------------------------------------\n\n# Define fixed parameters\nalpha <- .5\nnpcs <- 5\n```\n:::\n\n\nThen we can use the `PCovR::pcovr_est()` function to estimate the loadings, and weights and compute the component scores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# > Estimation with package ----------------------------------------------------\n\n# Estimate with PCovR function\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,  # fixed alpha\n    r = npcs    # fixed number of components\n)\n```\n:::\n\n\nInside the object `package` we can find all of the desired matrices. For example, we can extract the component scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract PC scores\nT_p <- package$Te\n```\n:::\n\n\nLet's also compute the component scores manually based on the procedure described by @vervloetEtAl:2016:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# > Estimation with Vervolet approach (manual) ---------------------------------\n\n# Compute the hat matrix\nHx <- X %*% solve(t(X) %*% X) %*% t(X)\n\n# Compute the G matrix\nG <- alpha * X %*% t(X) / sum(X^2) + (1 - alpha) * Hx %*% y %*% t(y) %*% Hx / sum(y^2)\n\n# Take the eigendecomposition of the G matrix\nEG <- eigen(G)\n\n# Take the first npcs eigenvectors of G as the PC scores \nT_m <- EG$vectors[, 1:npcs]\n```\n:::\n\n\nWe can now compare the results obtained with the two approaches. To keep the section slim and easy to read we will use the [Tucker congruence](https://en.wikipedia.org/wiki/Congruence_coefficient) coefficient to compare matrices.\nThis is a measure of similarity between matrices that ranges between -1 and +1.\nA congruence greater than 0.95 means the two matrices (or vectors) are virtually identical.\nFirst, notice that the PC scores obtained are the same:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare results --------------------------------------------------------------\n\n# T scores\nTuckerCoef(T_p, T_m)$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# Also interesting\nTuckerCoef(T_p, X %*% package$W)$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nGiven the component scores, we can compute all of the other matrices of interest and compare them with the results obtained with the R package `PCovR`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# W (weights)\nW_p <- package$W\nW_m <- solve(t(X) %*% X) %*% t(X) %*% T_m\nTuckerCoef(W_p, W_m)$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# Px\nPx_p <- package$Px\nPx_m <- t(W_m) %*% t(X) %*% X\nTuckerCoef(t(Px_p), t(Px_m))$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# Py\nPy_p <- drop(package$Py)\nPy_m <- drop(t(W_m) %*% t(X) %*% y) # WtXtY\nPy_m <- t(T_m) %*% y #TtY\nfactor.congruence(x = Py_p, y = Py_m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     CES-D\n[1,]  0.98\n```\n:::\n\n```{.r .cell-code}\n# B\nB_p <- drop(package$B)\nB_m <- drop(W_m %*% Py_m) # WPY\nB_m <- drop(W_m %*% t(W_m) %*% t(X) %*% y) # WWtXtY\nfactor.congruence(B_p, B_m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    1\n```\n:::\n:::\n\n\n## Reverting to PCA\n\nI mentioned before that when $\\alpha = 1$, PCovR reduces to PCR. Let's see that in action. First, we set the desired value for $\\alpha$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reverting to PCA -------------------------------------------------------------\n\n# Use alpha 1\nalpha <- 1\n```\n:::\n\n\nthen, we can perform PCovR\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate PCovR\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,\n    r = npcs # fixed number of components\n)\n```\n:::\n\n\nand classical PCA according to the @guerraEtAl:2021\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classical PCA\nuSVt <- svd(X)\nU <- uSVt$u\nD <- diag(uSVt$d)\nV <- uSVt$v\nI <- nrow(X)                              # Define the number of rows\nP_hat <- (I - 1)^{-1 / 2} * V %*% D       # Component loadings\nW_hat <- (I - 1)^{1 / 2} * V %*% solve(D) # Component weights\nT_hat <- (I - 1)^{1 / 2} * U              # Component scores\nT_hat <- X %*% W_hat\n```\n:::\n\n\nWe can now compare the results again\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The scores obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$Te, T_hat[, 1:npcs])$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# The loadings obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(t(package$Px), P_hat[, 1:npcs])$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# The weights obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$W, W_hat[, 1:npcs])$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# P is now proportional to W\nTuckerCoef(package$W, t(package$Px))$tucker_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n## Maximum Likelihood estimation of $\\alpha$\n\nThe value of $\\alpha$ is not usually chosen arbitrarily. One could cross-validate it or compute it with a closed-form solution that relies on the Maximum likelihood approach [@vervloetEtAl:2016]. Here, I show how to use this latter approach.\nFirst, let's simply fit PCovR by using the `PCovR::pcovr()` function and setting the model selection argument set to \"seq\". As explained in the help-file, this \"implies a sequential procedure in which the weighting value is determined on the basis of maximum likelihood principles\", exactly what we want.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Maximum likelihood tuning of alpha -------------------------------------------\n\n# Fit PCovR\npackage <- pcovr(\n    X = X_raw,\n    Y = y_raw,\n    rot = \"none\",\n    R = npcs, # fixed number of components\n    modsel = \"seq\" # fastest option\n)\n```\n:::\n\n\nThen, we can compute the maximum likelihood value of $\\alpha$ by first computing the error terms and taking their ratio.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute error ratio components\nlm_mod <- lm(y ~ -1 + X)\nery <- 1 - summary(lm_mod)$r.squared\n\nRmin <- npcs\nRmax <- npcs\nsing <- svd(X)\nvec <- Rmin:Rmax\nvec <- c(vec[1] - 1, vec, vec[length(vec)] + 1)\nVAF <- c(0, cumsum(sing$d^2) / sum(sing$d^2))\nVAF <- VAF[vec + 1]\nscr <- array(NA, c(1, length(vec)))\nfor (u in 2:(length(vec) - 1)) {\n    scr[, u] <- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])\n}\nerx <- 1 - VAF[which.max(scr)]\n```\n:::\n\n\nWe could have computed the error ratio with the `PCovR::err()` R function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute error ratio with function\nerr <- ErrorRatio(\n    X = X,\n    Y = y,\n    Rmin = npcs,\n    Rmax = npcs\n)\n\n# Is it the same?\nerr - erx/ery\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.110223e-15\n```\n:::\n:::\n\n\nWith this value, it is now easy to compute $\\alpha$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find alpha ML\nalpha_ML <- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)\n\n# Compare to one found by package\npackage$a - alpha_ML\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up environment -----------------------------------------------------------\n\n# Load package that implements this method\nlibrary(\"PCovR\")\n\n# Load package for a measure of similarity (TuckerCoef)\nlibrary(\"RegularizedSCA\")\n\n# Load package for Tucker congruence between vectors\nlibrary(\"psych\")\n\n# Load example data from PCovR package\ndata(alexithymia)\n\n# Subset data\nX_raw <- alexithymia$X\ny_raw <- alexithymia$Y[, 1, drop = FALSE]\n\n# Check variable means and variances\ndata.frame(\n    mean = c(\n        X = colMeans(X_raw),\n        y = colMeans(y_raw)\n        ),\n    variance = c(\n        X = apply(X_raw, 2, var), \n        y = apply(y_raw, 2, var)\n        )\n)\n\n# Scale data\nX <- scale(X_raw)# * (n - 1) / n\ny <- scale(y_raw)# * (n - 1) / n\n\n# Estimation -------------------------------------------------------------------\n\n# Define fixed parameters\nalpha <- .5\nnpcs <- 5\n\n# > Estimation with package ----------------------------------------------------\n\n# Estimate with PCovR function\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,  # fixed alpha\n    r = npcs    # fixed number of components\n)\n\n# Extract PC scores\nT_p <- package$Te\n\n# > Estimation with Vervolet approach (manual) ---------------------------------\n\n# Compute the hat matrix\nHx <- X %*% solve(t(X) %*% X) %*% t(X)\n\n# Compute the G matrix\nG <- alpha * X %*% t(X) / sum(X^2) + (1 - alpha) * Hx %*% y %*% t(y) %*% Hx / sum(y^2)\n\n# Take the eigendecomposition of the G matrix\nEG <- eigen(G)\n\n# Take the first npcs eigenvectors of G as the PC scores \nT_m <- EG$vectors[, 1:npcs]\n\n# Compare results --------------------------------------------------------------\n\n# T scores\nTuckerCoef(T_p, T_m)$tucker_value\n\n# Also interesting\nTuckerCoef(T_p, X %*% package$W)$tucker_value\n\n# W (weights)\nW_p <- package$W\nW_m <- solve(t(X) %*% X) %*% t(X) %*% T_m\nTuckerCoef(W_p, W_m)$tucker_value\n\n# Px\nPx_p <- package$Px\nPx_m <- t(W_m) %*% t(X) %*% X\nTuckerCoef(t(Px_p), t(Px_m))$tucker_value\n\n# Py\nPy_p <- drop(package$Py)\nPy_m <- drop(t(W_m) %*% t(X) %*% y) # WtXtY\nPy_m <- t(T_m) %*% y #TtY\nfactor.congruence(x = Py_p, y = Py_m)\n\n# B\nB_p <- drop(package$B)\nB_m <- drop(W_m %*% Py_m) # WPY\nB_m <- drop(W_m %*% t(W_m) %*% t(X) %*% y) # WWtXtY\nfactor.congruence(B_p, B_m)\n\n# Reverting to PCA -------------------------------------------------------------\n\n# Use alpha 1\nalpha <- 1\n\n# Estimate PCovR\npackage <- PCovR::pcovr_est(\n    X = X,\n    Y = y,\n    a = alpha,\n    r = npcs # fixed number of components\n)\n\n# Classical PCA\nuSVt <- svd(X)\nU <- uSVt$u\nD <- diag(uSVt$d)\nV <- uSVt$v\nI <- nrow(X)                              # Define the number of rows\nP_hat <- (I - 1)^{-1 / 2} * V %*% D       # Component loadings\nW_hat <- (I - 1)^{1 / 2} * V %*% solve(D) # Component weights\nT_hat <- (I - 1)^{1 / 2} * U              # Component scores\nT_hat <- X %*% W_hat\n\n# The scores obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$Te, T_hat[, 1:npcs])$tucker_value\n\n# The loadings obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(t(package$Px), P_hat[, 1:npcs])$tucker_value\n\n# The weights obtained with PCovR are the same as the ones obtained with PCA\nTuckerCoef(package$W, W_hat[, 1:npcs])$tucker_value\n\n# P is now proportional to W\nTuckerCoef(package$W, t(package$Px))$tucker_value\n\n# Maximum likelihood tuning of alpha -------------------------------------------\n\n# Fit PCovR\npackage <- pcovr(\n    X = X_raw,\n    Y = y_raw,\n    rot = \"none\",\n    R = npcs, # fixed number of components\n    modsel = \"seq\" # fastest option\n)\n\n# Compute error ratio components\nlm_mod <- lm(y ~ -1 + X)\nery <- 1 - summary(lm_mod)$r.squared\n\nRmin <- npcs\nRmax <- npcs\nsing <- svd(X)\nvec <- Rmin:Rmax\nvec <- c(vec[1] - 1, vec, vec[length(vec)] + 1)\nVAF <- c(0, cumsum(sing$d^2) / sum(sing$d^2))\nVAF <- VAF[vec + 1]\nscr <- array(NA, c(1, length(vec)))\nfor (u in 2:(length(vec) - 1)) {\n    scr[, u] <- (VAF[u] - VAF[u - 1]) / (VAF[u + 1] - VAF[u])\n}\nerx <- 1 - VAF[which.max(scr)]\n\n\n# Compute error ratio with function\nerr <- ErrorRatio(\n    X = X,\n    Y = y,\n    Rmin = npcs,\n    Rmax = npcs\n)\n\n# Is it the same?\nerr - erx/ery\n\n# Find alpha ML\nalpha_ML <- sum(X^2) / (sum(X^2) + sum(y^2) * erx / ery)\n\n# Compare to one found by package\npackage$a - alpha_ML\n```\n:::\n\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}