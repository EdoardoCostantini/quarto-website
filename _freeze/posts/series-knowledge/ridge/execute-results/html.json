{
  "hash": "d4af51fb285d956b6207eeda0a25d64c",
  "result": {
    "markdown": "---\ndraft: false\ntitle: Estimating ridge regression in R\nauthor: Edoardo Costantini\ndate: '2022-02-28'\nslug: ridge\ncategories: [\"Penalty\"]\nbibliography: ../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nWhen there are many correlated predictors in a linear regression model, their regression coefficients can become poorly determined and exhibit high variance.\nThis problem can be alleviated by imposing a size constraint (or penalty) on the coefficients.\nRidge regression shrinks the regression coefficients by imposing a penalty on their size.\nThe ridge coefficients values minimize a penalized residual sum of squares:\n\n$$\n\\hat{\\beta}^{\\text{ridge}} = \\text{argmin}_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2 \\right\\}\n$$\n\nThe ridge solutions are not equivariant under scaling of the inputs.\nTherefore, it is recommended to standardize the inputs before solving the minimization problem.\n\nNotice that the intercept $\\beta_0$ has been left out of the penalty term.\nPenalization of the intercept would make the procedure depend on the origin chosen for $Y$.\nFurthermore, by centering the predictors, we can separate the solution to the [minimazion problem](https://www.notion.so/Ridge-regression-8134d8babda5413ab182df645c6196a8) into two parts:\n\n1. Intercept\n$$\n\\hat{\\beta}_0 = \\bar{y}=\\frac{1}{N}\\sum_{i = 1}^{N} y_i\n$$\n\n2. Penalised regression coefficients\n$$\n\\hat{\\beta}^{\\text{ridge}}=(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^Ty\n$$\nwhich is the regular way of estimating regression coefficients with a penalty term ($\\lambda$) added on the diagonal ($\\mathbf{I}$) of the cross-product matrix ($\\mathbf{X}^T\\mathbf{X}$) to make it invertible ($(...)^{-1}$).\n\n# Learn by coding\n\nThe `glmnet` package can be used to obtain the ridge regression estimates of the regression coefficients.\nIn this section, we will first see how to obtain these estimates \"manually\", that is coding every step on our own, and then we will see how to obtain the same results using the `glmnet` package.\n\nLet's start by setting up the R environment.\nIn this post, we will work with the `mtcars` data.\nIf you are not familiar with it, just look up the R help file on it.\nWe will use the first column of the dataset (variable named `mpg`) as a dependent variable and the remaining ones as predictors in a linear regression.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up -----------------------------------------------------------------------\n\n# Load packages\nlibrary(glmnet)\n\n# Take the mtcars data\ny <- mtcars[, \"mpg\"]\nX <- mtcars[, -1]\n\n# Create a few shorthands we will use\nn <- nrow(X)\np <- ncol(X)\n```\n:::\n\n\n## Fitting ridge regression manually\nFirst, let's make sure the predictors are centered on the mean and scaled to have a variance of 1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitting ridge regression manually --------------------------------------------\n\n# Scale the data (standardize)\nX_scale <- scale(X, center = TRUE, scale = TRUE)\n```\n:::\n\n\nThen, we want to **fit the ridge regression** manually by separating the intercept and the regression coefficients estimation (two-step approach):\n\n1. Estimate the intercept ($\\hat{\\beta}_0$)\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Estimate the intercept\n    b0_hat_r <- mean(y)\n    ```\n    :::\n\n\n2. Estimate the ridge regression coefficients ($\\hat{\\beta}^{\\text{ridge}}$).\n\n  a. Compute the cross-product matrix of the predictors.\n\n        This is the same step we would take if we wanted to compute the OLS estimates.\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        # Compute the cross-product matrix of the data\n        XtX <- t(X_scale) %*% X_scale\n        ```\n        :::\n\n\n  b. Define a value of $\\lambda$.\n\n        This value is usually chosen by cross-validation from a grid of possible values.\n        However, here we are only interested in how $\\lambda$ is used in the computation, so we can simply give it a fixed value.\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        # Define a lambda value\n        lambda <- .1\n        ```\n        :::\n\n\n  c. Compute $\\hat{\\beta}^{\\text{ridge}}$.\n\n\n        ::: {.cell}\n        \n        ```{.r .cell-code}\n        # Estimate the regression coefficients with the ridge penalty\n        bs_hat_r <- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y\n        ```\n        :::\n\n        where `diag(p)` is the identity matrix $\\mathbf{I}$.\n\nFinally, let's print the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r)),\n  3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    twostep\nb0   20.091\nb1   -0.194\nb2    1.366\nb3   -1.373\nb4    0.438\nb5   -3.389\nb6    1.361\nb7    0.162\nb8    1.243\nb9    0.496\nb10  -0.460\n```\n:::\n:::\n\n\n\nIt is important to note the effect of centering and scaling.\nWhen fitting ridge regression, many sources recommend centering the data.\nThis allows separating the estimation of the intercept from the estimation of the regression coefficients.\nAs a result, only the regression coefficients are penalised.\nTo understand the effect of centering, consider what happens in regular OLS estimation when **predictors are centered**:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Centering in regular OLS -----------------------------------------------------\n\n# Create a version of X that is centered\nX_center <- scale(X, center = TRUE, scale = FALSE)\n\n# Fit an regular linear model\nlm_ols <- lm(y ~ X_center)\n\n# Check that b0 is equal to the mean of y\ncoef(lm_ols)[\"(Intercept)\"] - mean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) \n-3.552714e-15 \n```\n:::\n:::\n\n\nFurthermore, let's see what would have happened if we had penalised the intercept as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Consequence of penalising the intercept --------------------------------------\n\n# Add a vector of 1s to penalise the intercept\nX_scale_w1 <- cbind(1, X_scale)\n\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale_w1) %*% X_scale_w1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r_w1 <- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r),\n             onestep = c(b0 = bs_hat_r_w1[1],\n                         b = bs_hat_r_w1[-1])),\n  3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    twostep onestep\nb0   20.091  20.028\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460\n```\n:::\n:::\n\n\nAs you see, the intercept would be shrunk toward zero, without any benefit.\nAs a result, any prediction would also be offset by the same amount.\n\n### An alternative way to avoid penalising the intercept\nIt can be handy to obtain estimates of the regression coefficients and intercept in one step.\nWe can use matrix algebra and R to simplify the two-step procedure to a single step.\nIn particular, we can avoid the penalisation of the intercept by setting to 0 the first element of the \"penalty\" matrix `lambda * diag(p + 1)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Alternative to avoid penalization of the intercept ---------------------------\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_w1)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\n\n# replace first element with 0\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r2 <- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)\n\n# Compare\nround(\n        data.frame(\n                twostep = c(b0 = b0_hat_r, b = bs_hat_r),\n                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])\n        ),\n        3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    twostep onestep\nb0   20.091  20.091\nb1   -0.194  -0.194\nb2    1.366   1.366\nb3   -1.373  -1.373\nb4    0.438   0.438\nb5   -3.389  -3.389\nb6    1.361   1.361\nb7    0.162   0.162\nb8    1.243   1.243\nb9    0.496   0.496\nb10  -0.460  -0.460\n```\n:::\n:::\n\n## Fit ridge regression with glmnet\nThe most popular R package to fit regularised regression is `glmnet`.\nLet's see how we can replicate the results we obtained with the manual approach with glmnet.\nThere are three important differences to consider:\n\n- `glmnet` uses the [biased sample variance estimate](https://en.wikipedia.org/wiki/Variance#Biased_sample_variance) when scaling the predictors;\n- `glmnet` returns the unstandardized regression coefficients;\n- `glmnet` uses a different parametrization for $\\lambda$.\n\nTo obtain the same results with the manual approach and `glmnet` we need to account for these.\n\n### Use the biased estimation of variance\nFirst, let's use the biased sample variance estimate in computing $\\hat{\\beta}^{\\text{ridge}}$ with the manual approach:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitting ridge manually with biased variance estimation -----------------------\n\n# Standardize X\nX_scale <- sapply(1:p, function (j){\n  muj <- mean(X[, j])                  # mean\n  sj <- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd\n  (X[, j] - muj) / sj                  # center and scale\n})\n\n# Craete the desing matrix\nX_scale_dm <- cbind(1, X_scale)\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_dm)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r3 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)\n\n# Print results\nround(\n      data.frame(\n              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])\n      ),\n      3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    manual\nb0  20.091\nb1  -0.191\nb2   1.353\nb3  -1.354\nb4   0.430\nb5  -3.343\nb6   1.343\nb7   0.159\nb8   1.224\nb9   0.488\nb10 -0.449\n```\n:::\n:::\n\n\n### Return the unstandardized coefficients\nNext, we need to revert these regression coefficients to their original scale.\nSince we are estimating the regression coefficients on the scaled data, they are computed on the standardized scale.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Return the  unstandardized coefficients --------------------------------------\n\n# Extract the original mean and standard deviations of all X variables\nmean_x <- colMeans(X)\nsd_x <- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version\n\n# Revert to original scale\nbs_hat_r4 <- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),\n               bs_hat_r3[-1] / sd_x)\n\n# Compare manual standardized and unstandardized results\nround(\n      data.frame(\n              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),\n              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])\n      ),\n      3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    standardized unstandardized\nb0        20.091         12.908\nb1        -0.191         -0.109\nb2         1.353          0.011\nb3        -1.354         -0.020\nb4         0.430          0.818\nb5        -3.343         -3.471\nb6         1.343          0.764\nb7         0.159          0.320\nb8         1.224          2.491\nb9         0.488          0.672\nb10       -0.449         -0.282\n```\n:::\n:::\n\n\n### Adjust the parametrization of $\\lambda$ for `glmnet`\nNext, we need to understand the relationship between the $\\lambda$ parametrization we used and the one used by `glmnet`.\nThe following code shows that if we want to use a given value of `lambda` in `glmnet` we need to multiply it by the standard deviation of the dependent variable (`sd_y`) and divide it by the sample size (`n`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adjust the parametrization of lambda -----------------------------------------\n\n# Extract the original mean and standard deviations of y (for lambda parametrization)\nmean_y <- mean(y)\nsd_y <- sqrt(var(y) * (n - 1) / n)\n\n# Compute the value glmnet wants for your target lambda\nlambda_glmnet <- sd_y * lambda / n\n```\n:::\n\n\n### Compare manual and `glmnet` ridge regression output\n\nFinally, we can compare the results:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitting ridge regression with glmnet -----------------------------------------\n\n# Fit glmnet\nfit_glmnet_s <- glmnet(x = X,\n                       y = y,\n                       alpha = 0,\n                       lambda = lambda_glmnet, # correction for how penalty is used\n                       thresh = 1e-20)\n\nbs_glmnet <- coef(fit_glmnet_s)\n\n# Compare estimated coefficients\nround(\n      data.frame(\n        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),\n        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])\n      ),\n      3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       manual glmnet\nb0     12.908 12.908\nb.cyl  -0.109 -0.109\nb.disp  0.011  0.011\nb.hp   -0.020 -0.020\nb.drat  0.818  0.818\nb.wt   -3.471 -3.471\nb.qsec  0.764  0.764\nb.vs    0.320  0.320\nb.am    2.491  2.491\nb.gear  0.672  0.672\nb.carb -0.282 -0.282\n```\n:::\n:::\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up -----------------------------------------------------------------------\n\n# Load packages\nlibrary(glmnet)\n\n# Take the mtcars data\ny <- mtcars[, \"mpg\"]\nX <- mtcars[, -1]\n\n# Create a few shorthands we will use\nn <- nrow(X)\np <- ncol(X)\n\n# Fitting ridge regression manually --------------------------------------------\n\n# Scale the data (standardize)\nX_scale <- scale(X, center = TRUE, scale = TRUE)\n\n# Estimate the intercept\nb0_hat_r <- mean(y)\n\n\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale) %*% X_scale\n\n# Define a lambda value\nlambda <- .1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r <- solve(XtX + lambda * diag(p)) %*% t(X_scale) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r)),\n  3\n)\n\n# Centering in regular OLS -----------------------------------------------------\n\n# Create a version of X that is centered\nX_center <- scale(X, center = TRUE, scale = FALSE)\n\n# Fit an regular linear model\nlm_ols <- lm(y ~ X_center)\n\n# Check that b0 is equal to the mean of y\ncoef(lm_ols)[\"(Intercept)\"] - mean(y)\n\n# Consequence of penalising the intercept --------------------------------------\n\n# Add a vector of 1s to penalise the intercept\nX_scale_w1 <- cbind(1, X_scale)\n\n# Compute the cross-product matrix of the data\nXtX <- t(X_scale_w1) %*% X_scale_w1\n\n# Estimate the regression coefficients with the ridge penalty\nbs_hat_r_w1 <- solve(XtX + lambda * diag(p+1)) %*% t(X_scale_w1) %*% y\n\n# Print the results\nround(\n  data.frame(twostep = c(b0 = b0_hat_r,\n                         b = bs_hat_r),\n             onestep = c(b0 = bs_hat_r_w1[1],\n                         b = bs_hat_r_w1[-1])),\n  3\n)\n\n# Alternative to avoid penalization of the intercept ---------------------------\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_w1)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\n\n# replace first element with 0\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r2 <- solve(XtX + pen) %*% t(X_scale_w1) %*% (y)\n\n# Compare\nround(\n        data.frame(\n                twostep = c(b0 = b0_hat_r, b = bs_hat_r),\n                onestep = c(b0 = bs_hat_r2[1], b = bs_hat_r2[-1])\n        ),\n        3\n)\n\n# Fitting ridge manually with biased variance estimation -----------------------\n\n# Standardize X\nX_scale <- sapply(1:p, function (j){\n  muj <- mean(X[, j])                  # mean\n  sj <- sqrt( var(X[, j]) * (n-1) / n) # (biased) sd\n  (X[, j] - muj) / sj                  # center and scale\n})\n\n# Craete the desing matrix\nX_scale_dm <- cbind(1, X_scale)\n\n# Compute cross-product matrix\nXtX <- crossprod(X_scale_dm)\n\n# Create penalty matrix\npen <- lambda * diag(p + 1)\npen[1, 1] <- 0\n\n# Obtain standardized estimates\nbs_hat_r3 <- solve(XtX + pen) %*% t(X_scale_dm) %*% (y)\n\n# Print results\nround(\n      data.frame(\n              manual = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1])\n      ),\n      3\n)\n\n# Return the  unstandardized coefficients --------------------------------------\n\n# Extract the original mean and standard deviations of all X variables\nmean_x <- colMeans(X)\nsd_x <- sqrt(apply(X, 2, var) * (n - 1) / n) # biased version\n\n# Revert to original scale\nbs_hat_r4 <- c(bs_hat_r3[1] - crossprod(mean_x, bs_hat_r3[-1] / sd_x),\n               bs_hat_r3[-1] / sd_x)\n\n# Compare manual standardized and unstandardized results\nround(\n      data.frame(\n              standardized = c(b0 = bs_hat_r3[1], b = bs_hat_r3[-1]),\n              unstandardized = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1])\n      ),\n      3\n)\n\n# Adjust the parametrization of lambda -----------------------------------------\n\n# Extract the original mean and standard deviations of y (for lambda parametrization)\nmean_y <- mean(y)\nsd_y <- sqrt(var(y) * (n - 1) / n)\n\n# Compute the value glmnet wants for your target lambda\nlambda_glmnet <- sd_y * lambda / n\n\n# Fitting ridge regression with glmnet -----------------------------------------\n\n# Fit glmnet\nfit_glmnet_s <- glmnet(x = X,\n                       y = y,\n                       alpha = 0,\n                       lambda = lambda_glmnet, # correction for how penalty is used\n                       thresh = 1e-20)\n\nbs_glmnet <- coef(fit_glmnet_s)\n\n# Compare estimated coefficients\nround(\n      data.frame(\n        manual = c(b0 = bs_hat_r4[1], b = bs_hat_r4[-1]),\n        glmnet = c(b0 = bs_glmnet[1], b = bs_glmnet[-1])\n      ),\n      3\n)\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}